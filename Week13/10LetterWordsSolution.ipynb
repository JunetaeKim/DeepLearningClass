{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c788ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Input, Dense, TimeDistributed, RepeatVector, SimpleRNN\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98df7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d5c7ba1",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afef7ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/JunetaeKim/DeepLearningClass/main/Dataset/10LetterWords.txt'\n",
    "Dataset = pd.read_csv(url, header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12de1196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abbreviate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abdication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aberration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abjuration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abnegation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>wrongdoing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>xenophobia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>xerography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>yesteryear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>zoological</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "0     abbreviate\n",
       "1     abdication\n",
       "2     aberration\n",
       "3     abjuration\n",
       "4     abnegation\n",
       "...          ...\n",
       "2561  wrongdoing\n",
       "2562  xenophobia\n",
       "2563  xerography\n",
       "2564  yesteryear\n",
       "2565  zoological\n",
       "\n",
       "[2566 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79237a80",
   "metadata": {},
   "source": [
    "### Alphabet table generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58f69527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# one-hot encoding for alphabets\n",
    "AlphTab = pd.DataFrame(char_to_int,index = [0]).T\n",
    "AlphOneHot =  pd.get_dummies(AlphTab[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac585a37",
   "metadata": {},
   "source": [
    "### Data encoding; alphabet to numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad1da166",
   "metadata": {},
   "outputs": [],
   "source": [
    "AlphaEncode = []\n",
    "for word in Dataset.values:\n",
    "    SubVec = []\n",
    "    for char in word[0]:\n",
    "        CharIDX = char_to_int[char.upper()]\n",
    "        SubVec.append(AlphOneHot.iloc[CharIDX].values[None])\n",
    "    SubVec = np.concatenate(SubVec, axis=0)\n",
    "    AlphaEncode.append(SubVec[None])\n",
    "AlphaEncode = np.concatenate(AlphaEncode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf35522",
   "metadata": {},
   "source": [
    "### Data selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d11e87e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data selection for input and output \n",
    "InpData = AlphaEncode[:,:7, :] # Five input alphabets\n",
    "TargetData = AlphaEncode[:,7:, :] # Five output alphabets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118551e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0d19771",
   "metadata": {},
   "source": [
    "### Model save directory setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7a0443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './Results/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7898a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1008604",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c795f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordsModel ():\n",
    "    InpL = Input(shape=(InpData.shape[1], InpData.shape[2]))\n",
    "    LSTML = LSTM(30, return_sequences=True)(InpL)\n",
    "    LSTMOut = LSTM(30, return_sequences=False)(LSTML)\n",
    "\n",
    "    LSTMOut = RepeatVector(TargetData.shape[1])(LSTMOut)\n",
    "    Output = TimeDistributed(Dense(30))(LSTMOut)\n",
    "    Output = LSTM(30, return_sequences=True)(Output)\n",
    "    Output = Dense(InpData.shape[2], activation='softmax')(Output)\n",
    "    return Model(InpL,Output)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ace4d56",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "26/26 [==============================] - 3s 6ms/step - loss: 3.2308 - accuracy: 0.0922\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.18626, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 2/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.8935 - accuracy: 0.1580\n",
      "\n",
      "Epoch 00002: loss improved from 3.18626 to 2.82942, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 3/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.7153 - accuracy: 0.2342\n",
      "\n",
      "Epoch 00003: loss improved from 2.82942 to 2.69057, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 4/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2.6355 - accuracy: 0.2304\n",
      "\n",
      "Epoch 00004: loss improved from 2.69057 to 2.62113, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 5/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.5893 - accuracy: 0.2272\n",
      "\n",
      "Epoch 00005: loss improved from 2.62113 to 2.57907, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 6/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2.5305 - accuracy: 0.2350\n",
      "\n",
      "Epoch 00006: loss improved from 2.57907 to 2.55052, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 7/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.5406 - accuracy: 0.2347\n",
      "\n",
      "Epoch 00007: loss improved from 2.55052 to 2.53368, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 8/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.5190 - accuracy: 0.2356\n",
      "\n",
      "Epoch 00008: loss improved from 2.53368 to 2.51915, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 9/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.5234 - accuracy: 0.2308\n",
      "\n",
      "Epoch 00009: loss improved from 2.51915 to 2.51178, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 10/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.5005 - accuracy: 0.2336\n",
      "\n",
      "Epoch 00010: loss improved from 2.51178 to 2.49940, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 11/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.4876 - accuracy: 0.2349\n",
      "\n",
      "Epoch 00011: loss improved from 2.49940 to 2.48953, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 12/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.4848 - accuracy: 0.2422\n",
      "\n",
      "Epoch 00012: loss improved from 2.48953 to 2.48060, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 13/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.4768 - accuracy: 0.2338\n",
      "\n",
      "Epoch 00013: loss improved from 2.48060 to 2.47100, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 14/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2.4634 - accuracy: 0.2435\n",
      "\n",
      "Epoch 00014: loss improved from 2.47100 to 2.46259, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 15/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2.4446 - accuracy: 0.2385\n",
      "\n",
      "Epoch 00015: loss improved from 2.46259 to 2.45626, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 16/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2.4407 - accuracy: 0.2472\n",
      "\n",
      "Epoch 00016: loss improved from 2.45626 to 2.44590, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 17/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2.4493 - accuracy: 0.2549\n",
      "\n",
      "Epoch 00017: loss improved from 2.44590 to 2.43434, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 18/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2.4216 - accuracy: 0.2658\n",
      "\n",
      "Epoch 00018: loss improved from 2.43434 to 2.41545, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 19/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2.3877 - accuracy: 0.2881\n",
      "\n",
      "Epoch 00019: loss improved from 2.41545 to 2.39574, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 20/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.3750 - accuracy: 0.2818\n",
      "\n",
      "Epoch 00020: loss improved from 2.39574 to 2.37795, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 21/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.3624 - accuracy: 0.2915\n",
      "\n",
      "Epoch 00021: loss improved from 2.37795 to 2.36694, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 22/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.3389 - accuracy: 0.2938\n",
      "\n",
      "Epoch 00022: loss improved from 2.36694 to 2.35217, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 23/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.3439 - accuracy: 0.2903\n",
      "\n",
      "Epoch 00023: loss improved from 2.35217 to 2.34184, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 24/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.3504 - accuracy: 0.2888\n",
      "\n",
      "Epoch 00024: loss improved from 2.34184 to 2.32895, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 25/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2.3331 - accuracy: 0.2933\n",
      "\n",
      "Epoch 00025: loss improved from 2.32895 to 2.31694, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 26/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.2987 - accuracy: 0.3104\n",
      "\n",
      "Epoch 00026: loss improved from 2.31694 to 2.30644, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 27/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.2954 - accuracy: 0.3124\n",
      "\n",
      "Epoch 00027: loss improved from 2.30644 to 2.29661, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 28/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.2844 - accuracy: 0.3170\n",
      "\n",
      "Epoch 00028: loss improved from 2.29661 to 2.28489, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 29/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.2968 - accuracy: 0.3053\n",
      "\n",
      "Epoch 00029: loss improved from 2.28489 to 2.27594, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 30/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2.2633 - accuracy: 0.3084\n",
      "\n",
      "Epoch 00030: loss improved from 2.27594 to 2.26110, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 31/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.2483 - accuracy: 0.3083\n",
      "\n",
      "Epoch 00031: loss improved from 2.26110 to 2.24599, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 32/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.2353 - accuracy: 0.3109\n",
      "\n",
      "Epoch 00032: loss improved from 2.24599 to 2.23407, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 33/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.2314 - accuracy: 0.3119\n",
      "\n",
      "Epoch 00033: loss improved from 2.23407 to 2.22227, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 34/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.1929 - accuracy: 0.3271\n",
      "\n",
      "Epoch 00034: loss improved from 2.22227 to 2.21028, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 35/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.1558 - accuracy: 0.3416\n",
      "\n",
      "Epoch 00035: loss improved from 2.21028 to 2.20111, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 36/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.1990 - accuracy: 0.3258\n",
      "\n",
      "Epoch 00036: loss improved from 2.20111 to 2.18467, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 37/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.1884 - accuracy: 0.3222\n",
      "\n",
      "Epoch 00037: loss improved from 2.18467 to 2.17837, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 38/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.1464 - accuracy: 0.3359\n",
      "\n",
      "Epoch 00038: loss improved from 2.17837 to 2.16783, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 39/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.1611 - accuracy: 0.3264\n",
      "\n",
      "Epoch 00039: loss improved from 2.16783 to 2.15817, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 40/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.1314 - accuracy: 0.3373\n",
      "\n",
      "Epoch 00040: loss improved from 2.15817 to 2.14724, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 41/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.1521 - accuracy: 0.3414\n",
      "\n",
      "Epoch 00041: loss improved from 2.14724 to 2.14283, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 42/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.1223 - accuracy: 0.3513\n",
      "\n",
      "Epoch 00042: loss improved from 2.14283 to 2.13508, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 43/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2.1307 - accuracy: 0.3391\n",
      "\n",
      "Epoch 00043: loss improved from 2.13508 to 2.12373, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 44/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.1125 - accuracy: 0.3493\n",
      "\n",
      "Epoch 00044: loss improved from 2.12373 to 2.11616, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 45/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2.0824 - accuracy: 0.3502\n",
      "\n",
      "Epoch 00045: loss improved from 2.11616 to 2.10872, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 46/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2.1053 - accuracy: 0.3487\n",
      "\n",
      "Epoch 00046: loss improved from 2.10872 to 2.10114, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 47/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 2.1076 - accuracy: 0.3423\n",
      "\n",
      "Epoch 00047: loss improved from 2.10114 to 2.09231, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 48/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2.0693 - accuracy: 0.3571\n",
      "\n",
      "Epoch 00048: loss improved from 2.09231 to 2.08703, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 49/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.0954 - accuracy: 0.3435\n",
      "\n",
      "Epoch 00049: loss improved from 2.08703 to 2.08192, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 50/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2.0997 - accuracy: 0.3477\n",
      "\n",
      "Epoch 00050: loss improved from 2.08192 to 2.07198, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 51/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2.0796 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00051: loss improved from 2.07198 to 2.06748, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 52/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2.0441 - accuracy: 0.3599\n",
      "\n",
      "Epoch 00052: loss improved from 2.06748 to 2.06099, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 53/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.0570 - accuracy: 0.3529\n",
      "\n",
      "Epoch 00053: loss improved from 2.06099 to 2.05324, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 54/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.0658 - accuracy: 0.3526\n",
      "\n",
      "Epoch 00054: loss improved from 2.05324 to 2.04381, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 55/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2.0353 - accuracy: 0.3708\n",
      "\n",
      "Epoch 00055: loss improved from 2.04381 to 2.03854, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 56/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 2.0293 - accuracy: 0.3628\n",
      "\n",
      "Epoch 00056: loss improved from 2.03854 to 2.03188, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 57/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.0508 - accuracy: 0.3530\n",
      "\n",
      "Epoch 00057: loss improved from 2.03188 to 2.02820, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 58/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2.0217 - accuracy: 0.3696\n",
      "\n",
      "Epoch 00058: loss improved from 2.02820 to 2.02183, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 59/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2.0257 - accuracy: 0.3616\n",
      "\n",
      "Epoch 00059: loss improved from 2.02183 to 2.01294, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 60/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0190 - accuracy: 0.3699\n",
      "\n",
      "Epoch 00060: loss improved from 2.01294 to 2.00364, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 61/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.9895 - accuracy: 0.3716\n",
      "\n",
      "Epoch 00061: loss improved from 2.00364 to 1.99893, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 62/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.9731 - accuracy: 0.3773\n",
      "\n",
      "Epoch 00062: loss improved from 1.99893 to 1.99008, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 63/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.9981 - accuracy: 0.3731\n",
      "\n",
      "Epoch 00063: loss improved from 1.99008 to 1.98550, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 64/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.9881 - accuracy: 0.3755\n",
      "\n",
      "Epoch 00064: loss improved from 1.98550 to 1.97603, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 65/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.9797 - accuracy: 0.3830\n",
      "\n",
      "Epoch 00065: loss improved from 1.97603 to 1.97136, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 66/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.9651 - accuracy: 0.3882\n",
      "\n",
      "Epoch 00066: loss improved from 1.97136 to 1.96032, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 67/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.9888 - accuracy: 0.3798\n",
      "\n",
      "Epoch 00067: loss did not improve from 1.96032\n",
      "Epoch 68/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.9480 - accuracy: 0.3955\n",
      "\n",
      "Epoch 00068: loss improved from 1.96032 to 1.95430, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 69/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9584 - accuracy: 0.3868\n",
      "\n",
      "Epoch 00069: loss improved from 1.95430 to 1.94171, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 70/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.9436 - accuracy: 0.3989\n",
      "\n",
      "Epoch 00070: loss improved from 1.94171 to 1.93512, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 71/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.9505 - accuracy: 0.3895\n",
      "\n",
      "Epoch 00071: loss improved from 1.93512 to 1.92910, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 72/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.9241 - accuracy: 0.3979\n",
      "\n",
      "Epoch 00072: loss improved from 1.92910 to 1.92224, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 73/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.9119 - accuracy: 0.4052\n",
      "\n",
      "Epoch 00073: loss did not improve from 1.92224\n",
      "Epoch 74/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.8935 - accuracy: 0.4118\n",
      "\n",
      "Epoch 00074: loss improved from 1.92224 to 1.91262, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 75/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.9259 - accuracy: 0.4051\n",
      "\n",
      "Epoch 00075: loss improved from 1.91262 to 1.90861, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 76/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9086 - accuracy: 0.4090\n",
      "\n",
      "Epoch 00076: loss improved from 1.90861 to 1.90065, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 77/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.8639 - accuracy: 0.4251\n",
      "\n",
      "Epoch 00077: loss improved from 1.90065 to 1.89355, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 78/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.8846 - accuracy: 0.4157\n",
      "\n",
      "Epoch 00078: loss improved from 1.89355 to 1.88770, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 79/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.8428 - accuracy: 0.4273\n",
      "\n",
      "Epoch 00079: loss improved from 1.88770 to 1.88030, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 80/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.8558 - accuracy: 0.4209\n",
      "\n",
      "Epoch 00080: loss improved from 1.88030 to 1.87276, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 81/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.8877 - accuracy: 0.4128\n",
      "\n",
      "Epoch 00081: loss improved from 1.87276 to 1.86921, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 82/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.8451 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00082: loss improved from 1.86921 to 1.86461, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 83/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.8615 - accuracy: 0.4236\n",
      "\n",
      "Epoch 00083: loss improved from 1.86461 to 1.85504, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 84/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8303 - accuracy: 0.4343\n",
      "\n",
      "Epoch 00084: loss improved from 1.85504 to 1.84718, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 85/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.8609 - accuracy: 0.4161\n",
      "\n",
      "Epoch 00085: loss improved from 1.84718 to 1.84210, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 86/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.8439 - accuracy: 0.4344\n",
      "\n",
      "Epoch 00086: loss improved from 1.84210 to 1.84196, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 87/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.8141 - accuracy: 0.4340\n",
      "\n",
      "Epoch 00087: loss improved from 1.84196 to 1.83844, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 88/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.8438 - accuracy: 0.4264\n",
      "\n",
      "Epoch 00088: loss improved from 1.83844 to 1.82553, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 89/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.8529 - accuracy: 0.4358\n",
      "\n",
      "Epoch 00089: loss improved from 1.82553 to 1.82108, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 90/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.8130 - accuracy: 0.4408\n",
      "\n",
      "Epoch 00090: loss improved from 1.82108 to 1.81295, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 91/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.7936 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00091: loss improved from 1.81295 to 1.80974, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 92/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.8120 - accuracy: 0.4373\n",
      "\n",
      "Epoch 00092: loss improved from 1.80974 to 1.80527, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 93/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.7951 - accuracy: 0.4502\n",
      "\n",
      "Epoch 00093: loss improved from 1.80527 to 1.79570, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 94/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.7927 - accuracy: 0.4477\n",
      "\n",
      "Epoch 00094: loss improved from 1.79570 to 1.79347, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 95/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8052 - accuracy: 0.4464\n",
      "\n",
      "Epoch 00095: loss improved from 1.79347 to 1.78403, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 96/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.7587 - accuracy: 0.4580\n",
      "\n",
      "Epoch 00096: loss improved from 1.78403 to 1.78095, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 97/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.7720 - accuracy: 0.4464\n",
      "\n",
      "Epoch 00097: loss improved from 1.78095 to 1.77367, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 98/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.7665 - accuracy: 0.4532\n",
      "\n",
      "Epoch 00098: loss improved from 1.77367 to 1.76867, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 99/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.7532 - accuracy: 0.4570\n",
      "\n",
      "Epoch 00099: loss improved from 1.76867 to 1.76303, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 100/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.7561 - accuracy: 0.4598\n",
      "\n",
      "Epoch 00100: loss improved from 1.76303 to 1.76181, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 101/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.7516 - accuracy: 0.4541\n",
      "\n",
      "Epoch 00101: loss improved from 1.76181 to 1.75938, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 102/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.7465 - accuracy: 0.4513\n",
      "\n",
      "Epoch 00102: loss improved from 1.75938 to 1.74919, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 103/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.7302 - accuracy: 0.4710\n",
      "\n",
      "Epoch 00103: loss improved from 1.74919 to 1.74346, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 104/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.7241 - accuracy: 0.4647\n",
      "\n",
      "Epoch 00104: loss improved from 1.74346 to 1.74229, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 105/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.7241 - accuracy: 0.4696\n",
      "\n",
      "Epoch 00105: loss improved from 1.74229 to 1.73869, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 106/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.7228 - accuracy: 0.4680\n",
      "\n",
      "Epoch 00106: loss improved from 1.73869 to 1.72591, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 107/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.7193 - accuracy: 0.4673\n",
      "\n",
      "Epoch 00107: loss improved from 1.72591 to 1.71893, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 108/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.7369 - accuracy: 0.4636\n",
      "\n",
      "Epoch 00108: loss improved from 1.71893 to 1.71205, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 109/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.6935 - accuracy: 0.4791\n",
      "\n",
      "Epoch 00109: loss did not improve from 1.71205\n",
      "Epoch 110/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.7053 - accuracy: 0.4724\n",
      "\n",
      "Epoch 00110: loss improved from 1.71205 to 1.70301, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 111/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.6924 - accuracy: 0.4817\n",
      "\n",
      "Epoch 00111: loss improved from 1.70301 to 1.69524, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 112/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.6895 - accuracy: 0.4867\n",
      "\n",
      "Epoch 00112: loss improved from 1.69524 to 1.69096, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 113/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.7102 - accuracy: 0.4699\n",
      "\n",
      "Epoch 00113: loss improved from 1.69096 to 1.68563, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 114/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.6657 - accuracy: 0.4840\n",
      "\n",
      "Epoch 00114: loss improved from 1.68563 to 1.67850, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 115/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.6917 - accuracy: 0.4704\n",
      "\n",
      "Epoch 00115: loss improved from 1.67850 to 1.67654, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 116/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.6630 - accuracy: 0.4828\n",
      "\n",
      "Epoch 00116: loss improved from 1.67654 to 1.67117, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 117/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.6750 - accuracy: 0.4873\n",
      "\n",
      "Epoch 00117: loss improved from 1.67117 to 1.66873, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 118/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.6668 - accuracy: 0.4875\n",
      "\n",
      "Epoch 00118: loss improved from 1.66873 to 1.66026, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 119/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.6740 - accuracy: 0.4814\n",
      "\n",
      "Epoch 00119: loss improved from 1.66026 to 1.65600, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 120/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.6535 - accuracy: 0.4877\n",
      "\n",
      "Epoch 00120: loss improved from 1.65600 to 1.64794, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 121/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.6264 - accuracy: 0.4996\n",
      "\n",
      "Epoch 00121: loss improved from 1.64794 to 1.64160, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 122/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.6421 - accuracy: 0.4922\n",
      "\n",
      "Epoch 00122: loss improved from 1.64160 to 1.63998, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 123/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.6105 - accuracy: 0.5008\n",
      "\n",
      "Epoch 00123: loss improved from 1.63998 to 1.63105, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 124/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.6183 - accuracy: 0.5022\n",
      "\n",
      "Epoch 00124: loss improved from 1.63105 to 1.63021, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 125/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.5902 - accuracy: 0.5125\n",
      "\n",
      "Epoch 00125: loss improved from 1.63021 to 1.62442, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 126/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.5963 - accuracy: 0.5079\n",
      "\n",
      "Epoch 00126: loss improved from 1.62442 to 1.61456, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 127/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.6004 - accuracy: 0.5121\n",
      "\n",
      "Epoch 00127: loss improved from 1.61456 to 1.61318, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 128/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6208 - accuracy: 0.4984\n",
      "\n",
      "Epoch 00128: loss improved from 1.61318 to 1.60987, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 129/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.6369 - accuracy: 0.4987\n",
      "\n",
      "Epoch 00129: loss did not improve from 1.60987\n",
      "Epoch 130/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.5717 - accuracy: 0.5183\n",
      "\n",
      "Epoch 00130: loss improved from 1.60987 to 1.60272, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 131/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.6149 - accuracy: 0.5051\n",
      "\n",
      "Epoch 00131: loss improved from 1.60272 to 1.59078, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 132/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.5925 - accuracy: 0.5105\n",
      "\n",
      "Epoch 00132: loss improved from 1.59078 to 1.58509, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 133/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.6050 - accuracy: 0.5039\n",
      "\n",
      "Epoch 00133: loss improved from 1.58509 to 1.58259, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 134/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.5673 - accuracy: 0.5183\n",
      "\n",
      "Epoch 00134: loss improved from 1.58259 to 1.57773, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 135/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.5847 - accuracy: 0.5088\n",
      "\n",
      "Epoch 00135: loss improved from 1.57773 to 1.57577, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 136/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.5140 - accuracy: 0.5340\n",
      "\n",
      "Epoch 00136: loss improved from 1.57577 to 1.57000, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 137/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5415 - accuracy: 0.5243\n",
      "\n",
      "Epoch 00137: loss improved from 1.57000 to 1.56366, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 138/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.5487 - accuracy: 0.5213\n",
      "\n",
      "Epoch 00138: loss improved from 1.56366 to 1.56073, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 139/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.5330 - accuracy: 0.5316\n",
      "\n",
      "Epoch 00139: loss improved from 1.56073 to 1.54870, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 140/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.5616 - accuracy: 0.5280\n",
      "\n",
      "Epoch 00140: loss did not improve from 1.54870\n",
      "Epoch 141/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.5561 - accuracy: 0.5218\n",
      "\n",
      "Epoch 00141: loss improved from 1.54870 to 1.54416, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 142/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.5091 - accuracy: 0.5318\n",
      "\n",
      "Epoch 00142: loss improved from 1.54416 to 1.54199, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 143/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.5107 - accuracy: 0.5369\n",
      "\n",
      "Epoch 00143: loss improved from 1.54199 to 1.53365, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 144/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.5115 - accuracy: 0.5362\n",
      "\n",
      "Epoch 00144: loss improved from 1.53365 to 1.52534, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 145/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.5012 - accuracy: 0.5422\n",
      "\n",
      "Epoch 00145: loss improved from 1.52534 to 1.52179, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 146/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.5323 - accuracy: 0.5225\n",
      "\n",
      "Epoch 00146: loss improved from 1.52179 to 1.51952, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 147/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.4847 - accuracy: 0.5391\n",
      "\n",
      "Epoch 00147: loss improved from 1.51952 to 1.51667, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 148/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.5163 - accuracy: 0.5399\n",
      "\n",
      "Epoch 00148: loss improved from 1.51667 to 1.50899, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 149/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.5165 - accuracy: 0.5240\n",
      "\n",
      "Epoch 00149: loss did not improve from 1.50899\n",
      "Epoch 150/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.5024 - accuracy: 0.5431\n",
      "\n",
      "Epoch 00150: loss improved from 1.50899 to 1.50719, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 151/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.5172 - accuracy: 0.5349\n",
      "\n",
      "Epoch 00151: loss improved from 1.50719 to 1.50038, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 152/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.4864 - accuracy: 0.5467\n",
      "\n",
      "Epoch 00152: loss improved from 1.50038 to 1.49234, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 153/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.4802 - accuracy: 0.5410\n",
      "\n",
      "Epoch 00153: loss improved from 1.49234 to 1.48699, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 154/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.4816 - accuracy: 0.5439\n",
      "\n",
      "Epoch 00154: loss did not improve from 1.48699\n",
      "Epoch 155/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.4629 - accuracy: 0.5488\n",
      "\n",
      "Epoch 00155: loss improved from 1.48699 to 1.48147, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 156/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5060 - accuracy: 0.5429\n",
      "\n",
      "Epoch 00156: loss improved from 1.48147 to 1.47624, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 157/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.4729 - accuracy: 0.5518\n",
      "\n",
      "Epoch 00157: loss improved from 1.47624 to 1.46850, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 158/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.4514 - accuracy: 0.5552\n",
      "\n",
      "Epoch 00158: loss improved from 1.46850 to 1.46079, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 159/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.4741 - accuracy: 0.5443\n",
      "\n",
      "Epoch 00159: loss improved from 1.46079 to 1.45904, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 160/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.4456 - accuracy: 0.5547\n",
      "\n",
      "Epoch 00160: loss improved from 1.45904 to 1.45441, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 161/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.4413 - accuracy: 0.5562\n",
      "\n",
      "Epoch 00161: loss improved from 1.45441 to 1.45283, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 162/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.4461 - accuracy: 0.5600\n",
      "\n",
      "Epoch 00162: loss improved from 1.45283 to 1.44563, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 163/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.4544 - accuracy: 0.5544\n",
      "\n",
      "Epoch 00163: loss improved from 1.44563 to 1.44048, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 164/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.4377 - accuracy: 0.5557\n",
      "\n",
      "Epoch 00164: loss improved from 1.44048 to 1.43638, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 165/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.4284 - accuracy: 0.5670\n",
      "\n",
      "Epoch 00165: loss improved from 1.43638 to 1.43139, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 166/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.4326 - accuracy: 0.5607\n",
      "\n",
      "Epoch 00166: loss improved from 1.43139 to 1.42429, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 167/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.4211 - accuracy: 0.5712\n",
      "\n",
      "Epoch 00167: loss improved from 1.42429 to 1.42052, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 168/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.4053 - accuracy: 0.5721\n",
      "\n",
      "Epoch 00168: loss improved from 1.42052 to 1.41747, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 169/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.3957 - accuracy: 0.5617\n",
      "\n",
      "Epoch 00169: loss improved from 1.41747 to 1.41504, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 170/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.3895 - accuracy: 0.5775\n",
      "\n",
      "Epoch 00170: loss improved from 1.41504 to 1.41124, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 171/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.3828 - accuracy: 0.5758\n",
      "\n",
      "Epoch 00171: loss improved from 1.41124 to 1.40279, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 172/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.3993 - accuracy: 0.5756\n",
      "\n",
      "Epoch 00172: loss improved from 1.40279 to 1.40095, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 173/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.3951 - accuracy: 0.5774\n",
      "\n",
      "Epoch 00173: loss did not improve from 1.40095\n",
      "Epoch 174/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.3734 - accuracy: 0.5838\n",
      "\n",
      "Epoch 00174: loss improved from 1.40095 to 1.39358, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 175/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.3936 - accuracy: 0.5758\n",
      "\n",
      "Epoch 00175: loss improved from 1.39358 to 1.38926, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 176/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.3801 - accuracy: 0.5815\n",
      "\n",
      "Epoch 00176: loss did not improve from 1.38926\n",
      "Epoch 177/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.3772 - accuracy: 0.5776\n",
      "\n",
      "Epoch 00177: loss improved from 1.38926 to 1.38076, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 178/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.3774 - accuracy: 0.5826\n",
      "\n",
      "Epoch 00178: loss did not improve from 1.38076\n",
      "Epoch 179/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.3793 - accuracy: 0.5700\n",
      "\n",
      "Epoch 00179: loss improved from 1.38076 to 1.37669, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 180/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.3579 - accuracy: 0.5882\n",
      "\n",
      "Epoch 00180: loss improved from 1.37669 to 1.36755, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 181/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.3583 - accuracy: 0.5822\n",
      "\n",
      "Epoch 00181: loss improved from 1.36755 to 1.36386, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 182/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.3334 - accuracy: 0.5977\n",
      "\n",
      "Epoch 00182: loss improved from 1.36386 to 1.35971, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 183/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.3443 - accuracy: 0.5907\n",
      "\n",
      "Epoch 00183: loss improved from 1.35971 to 1.35676, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 184/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.3711 - accuracy: 0.5801\n",
      "\n",
      "Epoch 00184: loss improved from 1.35676 to 1.35557, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 185/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.3356 - accuracy: 0.5908\n",
      "\n",
      "Epoch 00185: loss improved from 1.35557 to 1.34735, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 186/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.3437 - accuracy: 0.5862\n",
      "\n",
      "Epoch 00186: loss improved from 1.34735 to 1.34366, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 187/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.3451 - accuracy: 0.5877\n",
      "\n",
      "Epoch 00187: loss improved from 1.34366 to 1.33660, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 188/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.3379 - accuracy: 0.5856\n",
      "\n",
      "Epoch 00188: loss did not improve from 1.33660\n",
      "Epoch 189/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.3232 - accuracy: 0.6004\n",
      "\n",
      "Epoch 00189: loss improved from 1.33660 to 1.33079, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 190/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.3210 - accuracy: 0.5965\n",
      "\n",
      "Epoch 00190: loss improved from 1.33079 to 1.32246, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 191/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.3121 - accuracy: 0.6006\n",
      "\n",
      "Epoch 00191: loss improved from 1.32246 to 1.31610, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 192/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.3010 - accuracy: 0.6064\n",
      "\n",
      "Epoch 00192: loss improved from 1.31610 to 1.31353, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 193/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.3113 - accuracy: 0.5895\n",
      "\n",
      "Epoch 00193: loss improved from 1.31353 to 1.31207, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 194/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.2885 - accuracy: 0.6073\n",
      "\n",
      "Epoch 00194: loss improved from 1.31207 to 1.31024, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 195/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.3040 - accuracy: 0.6025\n",
      "\n",
      "Epoch 00195: loss improved from 1.31024 to 1.30421, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 196/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.2917 - accuracy: 0.6009\n",
      "\n",
      "Epoch 00196: loss improved from 1.30421 to 1.30109, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 197/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2988 - accuracy: 0.6015\n",
      "\n",
      "Epoch 00197: loss improved from 1.30109 to 1.29674, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 198/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.2910 - accuracy: 0.6084\n",
      "\n",
      "Epoch 00198: loss improved from 1.29674 to 1.29204, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 199/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.2860 - accuracy: 0.6065\n",
      "\n",
      "Epoch 00199: loss improved from 1.29204 to 1.28638, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 200/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.2612 - accuracy: 0.6207\n",
      "\n",
      "Epoch 00200: loss improved from 1.28638 to 1.28482, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 201/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.2609 - accuracy: 0.6165\n",
      "\n",
      "Epoch 00201: loss improved from 1.28482 to 1.28150, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 202/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.2920 - accuracy: 0.6048\n",
      "\n",
      "Epoch 00202: loss did not improve from 1.28150\n",
      "Epoch 203/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.2578 - accuracy: 0.6113\n",
      "\n",
      "Epoch 00203: loss improved from 1.28150 to 1.27041, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 204/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.2517 - accuracy: 0.6145\n",
      "\n",
      "Epoch 00204: loss improved from 1.27041 to 1.26744, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 205/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.2634 - accuracy: 0.6128\n",
      "\n",
      "Epoch 00205: loss improved from 1.26744 to 1.26131, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 206/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.2679 - accuracy: 0.6164\n",
      "\n",
      "Epoch 00206: loss improved from 1.26131 to 1.25781, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 207/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.2278 - accuracy: 0.6255\n",
      "\n",
      "Epoch 00207: loss improved from 1.25781 to 1.25694, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 208/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.2325 - accuracy: 0.6239\n",
      "\n",
      "Epoch 00208: loss improved from 1.25694 to 1.25325, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 209/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.2330 - accuracy: 0.6222\n",
      "\n",
      "Epoch 00209: loss improved from 1.25325 to 1.25038, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 210/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.2296 - accuracy: 0.6191\n",
      "\n",
      "Epoch 00210: loss improved from 1.25038 to 1.24161, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 211/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.2168 - accuracy: 0.6305\n",
      "\n",
      "Epoch 00211: loss improved from 1.24161 to 1.24143, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 212/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.2264 - accuracy: 0.6344\n",
      "\n",
      "Epoch 00212: loss improved from 1.24143 to 1.23721, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 213/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.2335 - accuracy: 0.6256\n",
      "\n",
      "Epoch 00213: loss improved from 1.23721 to 1.23070, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 214/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.2294 - accuracy: 0.6268\n",
      "\n",
      "Epoch 00214: loss did not improve from 1.23070\n",
      "Epoch 215/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.2303 - accuracy: 0.6247\n",
      "\n",
      "Epoch 00215: loss improved from 1.23070 to 1.23022, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 216/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.2648 - accuracy: 0.6111\n",
      "\n",
      "Epoch 00216: loss improved from 1.23022 to 1.22868, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 217/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.2231 - accuracy: 0.6250\n",
      "\n",
      "Epoch 00217: loss improved from 1.22868 to 1.22299, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 218/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.2071 - accuracy: 0.6314\n",
      "\n",
      "Epoch 00218: loss improved from 1.22299 to 1.21393, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 219/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.2292 - accuracy: 0.6211\n",
      "\n",
      "Epoch 00219: loss improved from 1.21393 to 1.20976, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 220/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.2205 - accuracy: 0.6269\n",
      "\n",
      "Epoch 00220: loss improved from 1.20976 to 1.20776, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 221/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.2041 - accuracy: 0.6326\n",
      "\n",
      "Epoch 00221: loss did not improve from 1.20776\n",
      "Epoch 222/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.1902 - accuracy: 0.6370\n",
      "\n",
      "Epoch 00222: loss improved from 1.20776 to 1.20005, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 223/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.1683 - accuracy: 0.6423\n",
      "\n",
      "Epoch 00223: loss improved from 1.20005 to 1.19551, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 224/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.1630 - accuracy: 0.6509\n",
      "\n",
      "Epoch 00224: loss improved from 1.19551 to 1.19347, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 225/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.1750 - accuracy: 0.6421\n",
      "\n",
      "Epoch 00225: loss did not improve from 1.19347\n",
      "Epoch 226/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.2048 - accuracy: 0.6248\n",
      "\n",
      "Epoch 00226: loss improved from 1.19347 to 1.18738, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 227/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.2054 - accuracy: 0.6347\n",
      "\n",
      "Epoch 00227: loss improved from 1.18738 to 1.17998, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 228/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.1680 - accuracy: 0.6427\n",
      "\n",
      "Epoch 00228: loss improved from 1.17998 to 1.17527, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 229/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.1715 - accuracy: 0.6486\n",
      "\n",
      "Epoch 00229: loss did not improve from 1.17527\n",
      "Epoch 230/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.1443 - accuracy: 0.6443\n",
      "\n",
      "Epoch 00230: loss improved from 1.17527 to 1.17081, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 231/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.1525 - accuracy: 0.6458\n",
      "\n",
      "Epoch 00231: loss improved from 1.17081 to 1.16671, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 232/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.1381 - accuracy: 0.6614\n",
      "\n",
      "Epoch 00232: loss improved from 1.16671 to 1.15815, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 233/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.1264 - accuracy: 0.6563\n",
      "\n",
      "Epoch 00233: loss improved from 1.15815 to 1.14958, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 234/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.1624 - accuracy: 0.6597\n",
      "\n",
      "Epoch 00234: loss improved from 1.14958 to 1.14668, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 235/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.1780 - accuracy: 0.6497\n",
      "\n",
      "Epoch 00235: loss did not improve from 1.14668\n",
      "Epoch 236/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.1370 - accuracy: 0.6539\n",
      "\n",
      "Epoch 00236: loss did not improve from 1.14668\n",
      "Epoch 237/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.1775 - accuracy: 0.6442\n",
      "\n",
      "Epoch 00237: loss improved from 1.14668 to 1.14604, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 238/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.1204 - accuracy: 0.6601\n",
      "\n",
      "Epoch 00238: loss improved from 1.14604 to 1.13880, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 239/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.1560 - accuracy: 0.6501\n",
      "\n",
      "Epoch 00239: loss improved from 1.13880 to 1.13514, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 240/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.1439 - accuracy: 0.6555\n",
      "\n",
      "Epoch 00240: loss did not improve from 1.13514\n",
      "Epoch 241/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.1155 - accuracy: 0.6647\n",
      "\n",
      "Epoch 00241: loss improved from 1.13514 to 1.12853, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 242/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.1044 - accuracy: 0.6618\n",
      "\n",
      "Epoch 00242: loss improved from 1.12853 to 1.12250, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 243/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.1125 - accuracy: 0.6637\n",
      "\n",
      "Epoch 00243: loss improved from 1.12250 to 1.11960, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 244/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.1128 - accuracy: 0.6667\n",
      "\n",
      "Epoch 00244: loss improved from 1.11960 to 1.11533, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 245/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0927 - accuracy: 0.6742\n",
      "\n",
      "Epoch 00245: loss improved from 1.11533 to 1.10900, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 246/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0978 - accuracy: 0.6683\n",
      "\n",
      "Epoch 00246: loss did not improve from 1.10900\n",
      "Epoch 247/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0736 - accuracy: 0.6738\n",
      "\n",
      "Epoch 00247: loss improved from 1.10900 to 1.10652, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 248/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0935 - accuracy: 0.6775\n",
      "\n",
      "Epoch 00248: loss improved from 1.10652 to 1.10339, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 249/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0992 - accuracy: 0.6718\n",
      "\n",
      "Epoch 00249: loss improved from 1.10339 to 1.09853, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 250/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.1095 - accuracy: 0.6701\n",
      "\n",
      "Epoch 00250: loss improved from 1.09853 to 1.09656, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 251/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0611 - accuracy: 0.6756\n",
      "\n",
      "Epoch 00251: loss improved from 1.09656 to 1.09195, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 252/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0987 - accuracy: 0.6656\n",
      "\n",
      "Epoch 00252: loss improved from 1.09195 to 1.08663, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 253/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0864 - accuracy: 0.6700\n",
      "\n",
      "Epoch 00253: loss did not improve from 1.08663\n",
      "Epoch 254/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0843 - accuracy: 0.6716\n",
      "\n",
      "Epoch 00254: loss did not improve from 1.08663\n",
      "Epoch 255/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0836 - accuracy: 0.6721\n",
      "\n",
      "Epoch 00255: loss improved from 1.08663 to 1.08195, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 256/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0703 - accuracy: 0.6752\n",
      "\n",
      "Epoch 00256: loss improved from 1.08195 to 1.07681, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 257/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0897 - accuracy: 0.6662\n",
      "\n",
      "Epoch 00257: loss improved from 1.07681 to 1.07233, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 258/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.1027 - accuracy: 0.6669\n",
      "\n",
      "Epoch 00258: loss improved from 1.07233 to 1.07133, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 259/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0662 - accuracy: 0.6793\n",
      "\n",
      "Epoch 00259: loss improved from 1.07133 to 1.06783, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 260/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0901 - accuracy: 0.6692\n",
      "\n",
      "Epoch 00260: loss did not improve from 1.06783\n",
      "Epoch 261/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0392 - accuracy: 0.6819\n",
      "\n",
      "Epoch 00261: loss did not improve from 1.06783\n",
      "Epoch 262/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.0507 - accuracy: 0.6804\n",
      "\n",
      "Epoch 00262: loss improved from 1.06783 to 1.05760, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 263/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0429 - accuracy: 0.6887\n",
      "\n",
      "Epoch 00263: loss improved from 1.05760 to 1.05091, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 264/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0385 - accuracy: 0.6851\n",
      "\n",
      "Epoch 00264: loss improved from 1.05091 to 1.04996, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 265/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0521 - accuracy: 0.6863\n",
      "\n",
      "Epoch 00265: loss improved from 1.04996 to 1.04810, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 266/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0429 - accuracy: 0.6863\n",
      "\n",
      "Epoch 00266: loss did not improve from 1.04810\n",
      "Epoch 267/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0290 - accuracy: 0.6860\n",
      "\n",
      "Epoch 00267: loss improved from 1.04810 to 1.04433, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 268/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.0520 - accuracy: 0.6814\n",
      "\n",
      "Epoch 00268: loss improved from 1.04433 to 1.03777, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 269/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.0192 - accuracy: 0.6879\n",
      "\n",
      "Epoch 00269: loss improved from 1.03777 to 1.03664, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 270/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.0236 - accuracy: 0.6934\n",
      "\n",
      "Epoch 00270: loss improved from 1.03664 to 1.03064, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 271/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0406 - accuracy: 0.6828\n",
      "\n",
      "Epoch 00271: loss improved from 1.03064 to 1.02657, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 272/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0302 - accuracy: 0.6908\n",
      "\n",
      "Epoch 00272: loss did not improve from 1.02657\n",
      "Epoch 273/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0354 - accuracy: 0.6848\n",
      "\n",
      "Epoch 00273: loss improved from 1.02657 to 1.02309, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 274/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0184 - accuracy: 0.6934\n",
      "\n",
      "Epoch 00274: loss improved from 1.02309 to 1.01889, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 275/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0355 - accuracy: 0.6904\n",
      "\n",
      "Epoch 00275: loss improved from 1.01889 to 1.01177, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 276/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.0032 - accuracy: 0.6990\n",
      "\n",
      "Epoch 00276: loss did not improve from 1.01177\n",
      "Epoch 277/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0362 - accuracy: 0.6857\n",
      "\n",
      "Epoch 00277: loss did not improve from 1.01177\n",
      "Epoch 278/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.0124 - accuracy: 0.6914\n",
      "\n",
      "Epoch 00278: loss improved from 1.01177 to 1.01163, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 279/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.9891 - accuracy: 0.7045\n",
      "\n",
      "Epoch 00279: loss improved from 1.01163 to 1.00512, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 280/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.0164 - accuracy: 0.6907\n",
      "\n",
      "Epoch 00280: loss improved from 1.00512 to 1.00460, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 281/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0008 - accuracy: 0.7016\n",
      "\n",
      "Epoch 00281: loss improved from 1.00460 to 1.00128, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 282/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.9901 - accuracy: 0.7006\n",
      "\n",
      "Epoch 00282: loss improved from 1.00128 to 0.99847, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 283/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.9725 - accuracy: 0.7146\n",
      "\n",
      "Epoch 00283: loss improved from 0.99847 to 0.99257, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 284/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.9874 - accuracy: 0.7005\n",
      "\n",
      "Epoch 00284: loss did not improve from 0.99257\n",
      "Epoch 285/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.9817 - accuracy: 0.7067\n",
      "\n",
      "Epoch 00285: loss improved from 0.99257 to 0.98729, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 286/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.9929 - accuracy: 0.6998\n",
      "\n",
      "Epoch 00286: loss improved from 0.98729 to 0.98274, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 287/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.9864 - accuracy: 0.7020\n",
      "\n",
      "Epoch 00287: loss improved from 0.98274 to 0.98014, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 288/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.9528 - accuracy: 0.7192\n",
      "\n",
      "Epoch 00288: loss did not improve from 0.98014\n",
      "Epoch 289/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.9848 - accuracy: 0.7064\n",
      "\n",
      "Epoch 00289: loss improved from 0.98014 to 0.97640, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 290/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.9851 - accuracy: 0.6999\n",
      "\n",
      "Epoch 00290: loss improved from 0.97640 to 0.96713, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 291/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.9696 - accuracy: 0.7075\n",
      "\n",
      "Epoch 00291: loss did not improve from 0.96713\n",
      "Epoch 292/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.9445 - accuracy: 0.7201\n",
      "\n",
      "Epoch 00292: loss improved from 0.96713 to 0.96560, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 293/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.9538 - accuracy: 0.7107\n",
      "\n",
      "Epoch 00293: loss improved from 0.96560 to 0.95899, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 294/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.9173 - accuracy: 0.7307\n",
      "\n",
      "Epoch 00294: loss improved from 0.95899 to 0.95614, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 295/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.9607 - accuracy: 0.7106\n",
      "\n",
      "Epoch 00295: loss did not improve from 0.95614\n",
      "Epoch 296/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.9736 - accuracy: 0.7018\n",
      "\n",
      "Epoch 00296: loss did not improve from 0.95614\n",
      "Epoch 297/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.9512 - accuracy: 0.7101\n",
      "\n",
      "Epoch 00297: loss did not improve from 0.95614\n",
      "Epoch 298/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.9486 - accuracy: 0.7068\n",
      "\n",
      "Epoch 00298: loss improved from 0.95614 to 0.95358, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 299/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.9551 - accuracy: 0.7158\n",
      "\n",
      "Epoch 00299: loss improved from 0.95358 to 0.94840, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 300/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9360 - accuracy: 0.7188\n",
      "\n",
      "Epoch 00300: loss improved from 0.94840 to 0.94093, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 301/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.9180 - accuracy: 0.7257\n",
      "\n",
      "Epoch 00301: loss did not improve from 0.94093\n",
      "Epoch 302/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.9333 - accuracy: 0.7169\n",
      "\n",
      "Epoch 00302: loss did not improve from 0.94093\n",
      "Epoch 303/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.9413 - accuracy: 0.7149\n",
      "\n",
      "Epoch 00303: loss improved from 0.94093 to 0.94059, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 304/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.9228 - accuracy: 0.7192\n",
      "\n",
      "Epoch 00304: loss improved from 0.94059 to 0.93595, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 305/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.9129 - accuracy: 0.7209\n",
      "\n",
      "Epoch 00305: loss improved from 0.93595 to 0.93292, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 306/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9189 - accuracy: 0.7223\n",
      "\n",
      "Epoch 00306: loss improved from 0.93292 to 0.92853, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 307/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.9384 - accuracy: 0.7181\n",
      "\n",
      "Epoch 00307: loss did not improve from 0.92853\n",
      "Epoch 308/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.9083 - accuracy: 0.7240\n",
      "\n",
      "Epoch 00308: loss improved from 0.92853 to 0.92105, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 309/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.8917 - accuracy: 0.7269\n",
      "\n",
      "Epoch 00309: loss did not improve from 0.92105\n",
      "Epoch 310/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.9181 - accuracy: 0.7189\n",
      "\n",
      "Epoch 00310: loss did not improve from 0.92105\n",
      "Epoch 311/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.9085 - accuracy: 0.7261\n",
      "\n",
      "Epoch 00311: loss improved from 0.92105 to 0.91365, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 312/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.9100 - accuracy: 0.7215\n",
      "\n",
      "Epoch 00312: loss improved from 0.91365 to 0.90755, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 313/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.8982 - accuracy: 0.7209\n",
      "\n",
      "Epoch 00313: loss did not improve from 0.90755\n",
      "Epoch 314/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.9350 - accuracy: 0.7156\n",
      "\n",
      "Epoch 00314: loss did not improve from 0.90755\n",
      "Epoch 315/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.8876 - accuracy: 0.7312\n",
      "\n",
      "Epoch 00315: loss improved from 0.90755 to 0.90703, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 316/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.8867 - accuracy: 0.7320\n",
      "\n",
      "Epoch 00316: loss improved from 0.90703 to 0.90094, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 317/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.8876 - accuracy: 0.7343\n",
      "\n",
      "Epoch 00317: loss improved from 0.90094 to 0.89433, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 318/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.8676 - accuracy: 0.7370\n",
      "\n",
      "Epoch 00318: loss did not improve from 0.89433\n",
      "Epoch 319/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.8766 - accuracy: 0.7370\n",
      "\n",
      "Epoch 00319: loss did not improve from 0.89433\n",
      "Epoch 320/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.8872 - accuracy: 0.7217\n",
      "\n",
      "Epoch 00320: loss improved from 0.89433 to 0.88755, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 321/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8798 - accuracy: 0.7337\n",
      "\n",
      "Epoch 00321: loss improved from 0.88755 to 0.88685, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 322/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.8598 - accuracy: 0.7353\n",
      "\n",
      "Epoch 00322: loss did not improve from 0.88685\n",
      "Epoch 323/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.8816 - accuracy: 0.7329\n",
      "\n",
      "Epoch 00323: loss did not improve from 0.88685\n",
      "Epoch 324/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.8959 - accuracy: 0.7251\n",
      "\n",
      "Epoch 00324: loss improved from 0.88685 to 0.87853, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 325/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.8779 - accuracy: 0.7321\n",
      "\n",
      "Epoch 00325: loss did not improve from 0.87853\n",
      "Epoch 326/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.8497 - accuracy: 0.7373\n",
      "\n",
      "Epoch 00326: loss improved from 0.87853 to 0.87804, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 327/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.8695 - accuracy: 0.7344\n",
      "\n",
      "Epoch 00327: loss did not improve from 0.87804\n",
      "Epoch 328/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.8603 - accuracy: 0.7391\n",
      "\n",
      "Epoch 00328: loss improved from 0.87804 to 0.87333, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 329/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.8638 - accuracy: 0.7467\n",
      "\n",
      "Epoch 00329: loss did not improve from 0.87333\n",
      "Epoch 330/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.8760 - accuracy: 0.7358\n",
      "\n",
      "Epoch 00330: loss did not improve from 0.87333\n",
      "Epoch 331/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8543 - accuracy: 0.7415\n",
      "\n",
      "Epoch 00331: loss improved from 0.87333 to 0.87050, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 332/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.8343 - accuracy: 0.7525\n",
      "\n",
      "Epoch 00332: loss improved from 0.87050 to 0.86286, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 333/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.8573 - accuracy: 0.7380\n",
      "\n",
      "Epoch 00333: loss did not improve from 0.86286\n",
      "Epoch 334/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.8857 - accuracy: 0.7237\n",
      "\n",
      "Epoch 00334: loss did not improve from 0.86286\n",
      "Epoch 335/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.8433 - accuracy: 0.7408\n",
      "\n",
      "Epoch 00335: loss improved from 0.86286 to 0.85834, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 336/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8662 - accuracy: 0.7383\n",
      "\n",
      "Epoch 00336: loss improved from 0.85834 to 0.85332, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 337/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.8482 - accuracy: 0.7467\n",
      "\n",
      "Epoch 00337: loss did not improve from 0.85332\n",
      "Epoch 338/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.8319 - accuracy: 0.7475\n",
      "\n",
      "Epoch 00338: loss improved from 0.85332 to 0.85065, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 339/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.8453 - accuracy: 0.7426\n",
      "\n",
      "Epoch 00339: loss improved from 0.85065 to 0.84321, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 340/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.8354 - accuracy: 0.7481\n",
      "\n",
      "Epoch 00340: loss improved from 0.84321 to 0.84258, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 341/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8308 - accuracy: 0.7508\n",
      "\n",
      "Epoch 00341: loss did not improve from 0.84258\n",
      "Epoch 342/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.8706 - accuracy: 0.7360\n",
      "\n",
      "Epoch 00342: loss improved from 0.84258 to 0.84123, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 343/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.8318 - accuracy: 0.7493\n",
      "\n",
      "Epoch 00343: loss improved from 0.84123 to 0.83545, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 344/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.8573 - accuracy: 0.7474\n",
      "\n",
      "Epoch 00344: loss did not improve from 0.83545\n",
      "Epoch 345/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.8248 - accuracy: 0.7511\n",
      "\n",
      "Epoch 00345: loss did not improve from 0.83545\n",
      "Epoch 346/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8371 - accuracy: 0.7516\n",
      "\n",
      "Epoch 00346: loss improved from 0.83545 to 0.83058, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 347/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.8018 - accuracy: 0.7598\n",
      "\n",
      "Epoch 00347: loss improved from 0.83058 to 0.82480, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 348/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.8317 - accuracy: 0.7497\n",
      "\n",
      "Epoch 00348: loss improved from 0.82480 to 0.82368, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 349/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.8185 - accuracy: 0.7545\n",
      "\n",
      "Epoch 00349: loss improved from 0.82368 to 0.82149, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 350/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.8251 - accuracy: 0.7525\n",
      "\n",
      "Epoch 00350: loss improved from 0.82149 to 0.82110, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 351/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.7815 - accuracy: 0.7595\n",
      "\n",
      "Epoch 00351: loss improved from 0.82110 to 0.81907, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 352/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.8232 - accuracy: 0.7530\n",
      "\n",
      "Epoch 00352: loss improved from 0.81907 to 0.81420, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 353/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.8094 - accuracy: 0.7538\n",
      "\n",
      "Epoch 00353: loss did not improve from 0.81420\n",
      "Epoch 354/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.8099 - accuracy: 0.7589\n",
      "\n",
      "Epoch 00354: loss improved from 0.81420 to 0.81246, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 355/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.8275 - accuracy: 0.7500\n",
      "\n",
      "Epoch 00355: loss did not improve from 0.81246\n",
      "Epoch 356/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.8156 - accuracy: 0.7532\n",
      "\n",
      "Epoch 00356: loss did not improve from 0.81246\n",
      "Epoch 357/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.8002 - accuracy: 0.7640\n",
      "\n",
      "Epoch 00357: loss improved from 0.81246 to 0.80883, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 358/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.8199 - accuracy: 0.7487\n",
      "\n",
      "Epoch 00358: loss improved from 0.80883 to 0.80580, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 359/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7935 - accuracy: 0.7600\n",
      "\n",
      "Epoch 00359: loss improved from 0.80580 to 0.80566, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 360/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.8078 - accuracy: 0.7551\n",
      "\n",
      "Epoch 00360: loss improved from 0.80566 to 0.80197, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 361/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7960 - accuracy: 0.7626\n",
      "\n",
      "Epoch 00361: loss did not improve from 0.80197\n",
      "Epoch 362/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7818 - accuracy: 0.7626\n",
      "\n",
      "Epoch 00362: loss improved from 0.80197 to 0.79654, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 363/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7947 - accuracy: 0.7544\n",
      "\n",
      "Epoch 00363: loss improved from 0.79654 to 0.79211, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 364/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7986 - accuracy: 0.7535\n",
      "\n",
      "Epoch 00364: loss improved from 0.79211 to 0.78769, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 365/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7637 - accuracy: 0.7720\n",
      "\n",
      "Epoch 00365: loss improved from 0.78769 to 0.78557, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 366/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7895 - accuracy: 0.7566\n",
      "\n",
      "Epoch 00366: loss did not improve from 0.78557\n",
      "Epoch 367/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7923 - accuracy: 0.7635\n",
      "\n",
      "Epoch 00367: loss did not improve from 0.78557\n",
      "Epoch 368/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.7717 - accuracy: 0.7656\n",
      "\n",
      "Epoch 00368: loss did not improve from 0.78557\n",
      "Epoch 369/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.7607 - accuracy: 0.7666\n",
      "\n",
      "Epoch 00369: loss improved from 0.78557 to 0.77747, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 370/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.7592 - accuracy: 0.7665\n",
      "\n",
      "Epoch 00370: loss improved from 0.77747 to 0.77651, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 371/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7875 - accuracy: 0.7578\n",
      "\n",
      "Epoch 00371: loss improved from 0.77651 to 0.77279, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 372/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7565 - accuracy: 0.7727\n",
      "\n",
      "Epoch 00372: loss improved from 0.77279 to 0.77145, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 373/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.7606 - accuracy: 0.7626\n",
      "\n",
      "Epoch 00373: loss did not improve from 0.77145\n",
      "Epoch 374/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7706 - accuracy: 0.7726\n",
      "\n",
      "Epoch 00374: loss did not improve from 0.77145\n",
      "Epoch 375/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7828 - accuracy: 0.7567\n",
      "\n",
      "Epoch 00375: loss did not improve from 0.77145\n",
      "Epoch 376/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7588 - accuracy: 0.7730\n",
      "\n",
      "Epoch 00376: loss improved from 0.77145 to 0.76672, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 377/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.7707 - accuracy: 0.7681\n",
      "\n",
      "Epoch 00377: loss improved from 0.76672 to 0.76229, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 378/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7376 - accuracy: 0.7783\n",
      "\n",
      "Epoch 00378: loss improved from 0.76229 to 0.75912, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 379/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7693 - accuracy: 0.7731\n",
      "\n",
      "Epoch 00379: loss improved from 0.75912 to 0.75663, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 380/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7675 - accuracy: 0.7679\n",
      "\n",
      "Epoch 00380: loss did not improve from 0.75663\n",
      "Epoch 381/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7596 - accuracy: 0.7670\n",
      "\n",
      "Epoch 00381: loss did not improve from 0.75663\n",
      "Epoch 382/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7521 - accuracy: 0.7690\n",
      "\n",
      "Epoch 00382: loss improved from 0.75663 to 0.75134, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 383/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7220 - accuracy: 0.7862\n",
      "\n",
      "Epoch 00383: loss did not improve from 0.75134\n",
      "Epoch 384/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.7164 - accuracy: 0.7846\n",
      "\n",
      "Epoch 00384: loss did not improve from 0.75134\n",
      "Epoch 385/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.7545 - accuracy: 0.7729\n",
      "\n",
      "Epoch 00385: loss improved from 0.75134 to 0.74826, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 386/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.7454 - accuracy: 0.7787\n",
      "\n",
      "Epoch 00386: loss improved from 0.74826 to 0.74544, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 387/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7576 - accuracy: 0.7717\n",
      "\n",
      "Epoch 00387: loss did not improve from 0.74544\n",
      "Epoch 388/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.7497 - accuracy: 0.7706\n",
      "\n",
      "Epoch 00388: loss did not improve from 0.74544\n",
      "Epoch 389/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.7503 - accuracy: 0.7768\n",
      "\n",
      "Epoch 00389: loss improved from 0.74544 to 0.74461, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 390/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7776 - accuracy: 0.7699\n",
      "\n",
      "Epoch 00390: loss improved from 0.74461 to 0.74316, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 391/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7183 - accuracy: 0.7799\n",
      "\n",
      "Epoch 00391: loss improved from 0.74316 to 0.73691, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 392/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.7376 - accuracy: 0.7736\n",
      "\n",
      "Epoch 00392: loss did not improve from 0.73691\n",
      "Epoch 393/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.7462 - accuracy: 0.7786\n",
      "\n",
      "Epoch 00393: loss improved from 0.73691 to 0.73631, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 394/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.7164 - accuracy: 0.7886\n",
      "\n",
      "Epoch 00394: loss improved from 0.73631 to 0.73112, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 395/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7210 - accuracy: 0.7827\n",
      "\n",
      "Epoch 00395: loss did not improve from 0.73112\n",
      "Epoch 396/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7331 - accuracy: 0.7785\n",
      "\n",
      "Epoch 00396: loss improved from 0.73112 to 0.73077, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 397/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.7234 - accuracy: 0.7863\n",
      "\n",
      "Epoch 00397: loss did not improve from 0.73077\n",
      "Epoch 398/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7211 - accuracy: 0.7785\n",
      "\n",
      "Epoch 00398: loss did not improve from 0.73077\n",
      "Epoch 399/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7185 - accuracy: 0.7850\n",
      "\n",
      "Epoch 00399: loss improved from 0.73077 to 0.72663, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 400/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.7082 - accuracy: 0.7873\n",
      "\n",
      "Epoch 00400: loss improved from 0.72663 to 0.72443, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 401/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6850 - accuracy: 0.7950\n",
      "\n",
      "Epoch 00401: loss improved from 0.72443 to 0.72013, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 402/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7165 - accuracy: 0.7850\n",
      "\n",
      "Epoch 00402: loss did not improve from 0.72013\n",
      "Epoch 403/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7287 - accuracy: 0.7820\n",
      "\n",
      "Epoch 00403: loss improved from 0.72013 to 0.71485, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 404/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.7190 - accuracy: 0.7845\n",
      "\n",
      "Epoch 00404: loss did not improve from 0.71485\n",
      "Epoch 405/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6967 - accuracy: 0.7831\n",
      "\n",
      "Epoch 00405: loss did not improve from 0.71485\n",
      "Epoch 406/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.7277 - accuracy: 0.7778\n",
      "\n",
      "Epoch 00406: loss did not improve from 0.71485\n",
      "Epoch 407/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.7241 - accuracy: 0.7795\n",
      "\n",
      "Epoch 00407: loss improved from 0.71485 to 0.71352, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 408/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6711 - accuracy: 0.7938\n",
      "\n",
      "Epoch 00408: loss improved from 0.71352 to 0.70533, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 409/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6879 - accuracy: 0.7967\n",
      "\n",
      "Epoch 00409: loss improved from 0.70533 to 0.70137, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 410/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.7087 - accuracy: 0.7842\n",
      "\n",
      "Epoch 00410: loss improved from 0.70137 to 0.69423, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 411/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6713 - accuracy: 0.7948\n",
      "\n",
      "Epoch 00411: loss did not improve from 0.69423\n",
      "Epoch 412/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6455 - accuracy: 0.8055\n",
      "\n",
      "Epoch 00412: loss improved from 0.69423 to 0.68927, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 413/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6877 - accuracy: 0.7906\n",
      "\n",
      "Epoch 00413: loss did not improve from 0.68927\n",
      "Epoch 414/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6601 - accuracy: 0.8007\n",
      "\n",
      "Epoch 00414: loss did not improve from 0.68927\n",
      "Epoch 415/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6776 - accuracy: 0.7997\n",
      "\n",
      "Epoch 00415: loss improved from 0.68927 to 0.68460, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 416/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6818 - accuracy: 0.7938\n",
      "\n",
      "Epoch 00416: loss improved from 0.68460 to 0.68335, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 417/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.7022 - accuracy: 0.7966\n",
      "\n",
      "Epoch 00417: loss did not improve from 0.68335\n",
      "Epoch 418/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6849 - accuracy: 0.7893\n",
      "\n",
      "Epoch 00418: loss did not improve from 0.68335\n",
      "Epoch 419/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6952 - accuracy: 0.7900\n",
      "\n",
      "Epoch 00419: loss did not improve from 0.68335\n",
      "Epoch 420/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6868 - accuracy: 0.7958\n",
      "\n",
      "Epoch 00420: loss did not improve from 0.68335\n",
      "Epoch 421/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6795 - accuracy: 0.7949\n",
      "\n",
      "Epoch 00421: loss improved from 0.68335 to 0.68049, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 422/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6849 - accuracy: 0.8006\n",
      "\n",
      "Epoch 00422: loss improved from 0.68049 to 0.67537, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 423/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6771 - accuracy: 0.7945\n",
      "\n",
      "Epoch 00423: loss improved from 0.67537 to 0.67314, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 424/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6749 - accuracy: 0.7926\n",
      "\n",
      "Epoch 00424: loss did not improve from 0.67314\n",
      "Epoch 425/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6893 - accuracy: 0.7919\n",
      "\n",
      "Epoch 00425: loss did not improve from 0.67314\n",
      "Epoch 426/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6957 - accuracy: 0.7908\n",
      "\n",
      "Epoch 00426: loss did not improve from 0.67314\n",
      "Epoch 427/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6708 - accuracy: 0.7983\n",
      "\n",
      "Epoch 00427: loss improved from 0.67314 to 0.67211, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 428/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6797 - accuracy: 0.7961\n",
      "\n",
      "Epoch 00428: loss improved from 0.67211 to 0.67081, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 429/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6665 - accuracy: 0.8010\n",
      "\n",
      "Epoch 00429: loss did not improve from 0.67081\n",
      "Epoch 430/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6513 - accuracy: 0.8096\n",
      "\n",
      "Epoch 00430: loss improved from 0.67081 to 0.66486, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 431/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6511 - accuracy: 0.7999\n",
      "\n",
      "Epoch 00431: loss did not improve from 0.66486\n",
      "Epoch 432/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6696 - accuracy: 0.7967\n",
      "\n",
      "Epoch 00432: loss did not improve from 0.66486\n",
      "Epoch 433/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6587 - accuracy: 0.7988\n",
      "\n",
      "Epoch 00433: loss did not improve from 0.66486\n",
      "Epoch 434/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6782 - accuracy: 0.7944\n",
      "\n",
      "Epoch 00434: loss did not improve from 0.66486\n",
      "Epoch 435/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6450 - accuracy: 0.8067\n",
      "\n",
      "Epoch 00435: loss did not improve from 0.66486\n",
      "Epoch 436/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6739 - accuracy: 0.7983\n",
      "\n",
      "Epoch 00436: loss did not improve from 0.66486\n",
      "Epoch 437/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6550 - accuracy: 0.8088\n",
      "\n",
      "Epoch 00437: loss improved from 0.66486 to 0.65834, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 438/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6310 - accuracy: 0.8067\n",
      "\n",
      "Epoch 00438: loss improved from 0.65834 to 0.65229, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 439/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6321 - accuracy: 0.8134\n",
      "\n",
      "Epoch 00439: loss improved from 0.65229 to 0.65100, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 440/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6442 - accuracy: 0.8022\n",
      "\n",
      "Epoch 00440: loss did not improve from 0.65100\n",
      "Epoch 441/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6725 - accuracy: 0.7976\n",
      "\n",
      "Epoch 00441: loss improved from 0.65100 to 0.64765, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 442/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6505 - accuracy: 0.7972\n",
      "\n",
      "Epoch 00442: loss did not improve from 0.64765\n",
      "Epoch 443/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6333 - accuracy: 0.8113\n",
      "\n",
      "Epoch 00443: loss did not improve from 0.64765\n",
      "Epoch 444/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6365 - accuracy: 0.8005\n",
      "\n",
      "Epoch 00444: loss did not improve from 0.64765\n",
      "Epoch 445/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6454 - accuracy: 0.8029\n",
      "\n",
      "Epoch 00445: loss did not improve from 0.64765\n",
      "Epoch 446/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6318 - accuracy: 0.8110\n",
      "\n",
      "Epoch 00446: loss did not improve from 0.64765\n",
      "Epoch 447/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6462 - accuracy: 0.8060\n",
      "\n",
      "Epoch 00447: loss improved from 0.64765 to 0.64244, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 448/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6254 - accuracy: 0.8126\n",
      "\n",
      "Epoch 00448: loss improved from 0.64244 to 0.63359, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 449/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6386 - accuracy: 0.8113\n",
      "\n",
      "Epoch 00449: loss improved from 0.63359 to 0.63149, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 450/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6036 - accuracy: 0.8173\n",
      "\n",
      "Epoch 00450: loss improved from 0.63149 to 0.63014, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 451/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6167 - accuracy: 0.8086\n",
      "\n",
      "Epoch 00451: loss did not improve from 0.63014\n",
      "Epoch 452/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6306 - accuracy: 0.8083\n",
      "\n",
      "Epoch 00452: loss did not improve from 0.63014\n",
      "Epoch 453/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6186 - accuracy: 0.8114\n",
      "\n",
      "Epoch 00453: loss did not improve from 0.63014\n",
      "Epoch 454/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6176 - accuracy: 0.8158\n",
      "\n",
      "Epoch 00454: loss did not improve from 0.63014\n",
      "Epoch 455/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6326 - accuracy: 0.8133\n",
      "\n",
      "Epoch 00455: loss improved from 0.63014 to 0.62852, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 456/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6169 - accuracy: 0.8132\n",
      "\n",
      "Epoch 00456: loss improved from 0.62852 to 0.62704, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 457/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6414 - accuracy: 0.8045\n",
      "\n",
      "Epoch 00457: loss did not improve from 0.62704\n",
      "Epoch 458/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6130 - accuracy: 0.8142\n",
      "\n",
      "Epoch 00458: loss improved from 0.62704 to 0.62075, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 459/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6083 - accuracy: 0.8207\n",
      "\n",
      "Epoch 00459: loss improved from 0.62075 to 0.61740, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 460/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6239 - accuracy: 0.8136\n",
      "\n",
      "Epoch 00460: loss did not improve from 0.61740\n",
      "Epoch 461/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6358 - accuracy: 0.8064\n",
      "\n",
      "Epoch 00461: loss did not improve from 0.61740\n",
      "Epoch 462/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6462 - accuracy: 0.8001\n",
      "\n",
      "Epoch 00462: loss did not improve from 0.61740\n",
      "Epoch 463/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6061 - accuracy: 0.8194\n",
      "\n",
      "Epoch 00463: loss did not improve from 0.61740\n",
      "Epoch 464/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5904 - accuracy: 0.8219\n",
      "\n",
      "Epoch 00464: loss improved from 0.61740 to 0.61523, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 465/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6228 - accuracy: 0.8166\n",
      "\n",
      "Epoch 00465: loss did not improve from 0.61523\n",
      "Epoch 466/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6360 - accuracy: 0.8048\n",
      "\n",
      "Epoch 00466: loss did not improve from 0.61523\n",
      "Epoch 467/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6324 - accuracy: 0.8117\n",
      "\n",
      "Epoch 00467: loss did not improve from 0.61523\n",
      "Epoch 468/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6292 - accuracy: 0.8078\n",
      "\n",
      "Epoch 00468: loss improved from 0.61523 to 0.61176, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 469/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5959 - accuracy: 0.8204\n",
      "\n",
      "Epoch 00469: loss improved from 0.61176 to 0.60079, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 470/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5894 - accuracy: 0.8255\n",
      "\n",
      "Epoch 00470: loss improved from 0.60079 to 0.59966, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 471/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5803 - accuracy: 0.8274\n",
      "\n",
      "Epoch 00471: loss improved from 0.59966 to 0.59883, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 472/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6005 - accuracy: 0.8182\n",
      "\n",
      "Epoch 00472: loss did not improve from 0.59883\n",
      "Epoch 473/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5945 - accuracy: 0.8271\n",
      "\n",
      "Epoch 00473: loss did not improve from 0.59883\n",
      "Epoch 474/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5914 - accuracy: 0.8217\n",
      "\n",
      "Epoch 00474: loss did not improve from 0.59883\n",
      "Epoch 475/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5949 - accuracy: 0.8209\n",
      "\n",
      "Epoch 00475: loss improved from 0.59883 to 0.59446, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 476/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5794 - accuracy: 0.8238\n",
      "\n",
      "Epoch 00476: loss did not improve from 0.59446\n",
      "Epoch 477/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5977 - accuracy: 0.8250\n",
      "\n",
      "Epoch 00477: loss improved from 0.59446 to 0.59250, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 478/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5854 - accuracy: 0.8222\n",
      "\n",
      "Epoch 00478: loss did not improve from 0.59250\n",
      "Epoch 479/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.6135 - accuracy: 0.8243\n",
      "\n",
      "Epoch 00479: loss improved from 0.59250 to 0.58557, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 480/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5824 - accuracy: 0.8267\n",
      "\n",
      "Epoch 00480: loss did not improve from 0.58557\n",
      "Epoch 481/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5831 - accuracy: 0.8276\n",
      "\n",
      "Epoch 00481: loss did not improve from 0.58557\n",
      "Epoch 482/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5954 - accuracy: 0.8260\n",
      "\n",
      "Epoch 00482: loss did not improve from 0.58557\n",
      "Epoch 483/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5945 - accuracy: 0.8140\n",
      "\n",
      "Epoch 00483: loss did not improve from 0.58557\n",
      "Epoch 484/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5807 - accuracy: 0.8264\n",
      "\n",
      "Epoch 00484: loss did not improve from 0.58557\n",
      "Epoch 485/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5712 - accuracy: 0.8290\n",
      "\n",
      "Epoch 00485: loss improved from 0.58557 to 0.58548, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 486/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5606 - accuracy: 0.8297\n",
      "\n",
      "Epoch 00486: loss improved from 0.58548 to 0.58353, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 487/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.6071 - accuracy: 0.8196\n",
      "\n",
      "Epoch 00487: loss improved from 0.58353 to 0.58347, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 488/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5745 - accuracy: 0.8289\n",
      "\n",
      "Epoch 00488: loss did not improve from 0.58347\n",
      "Epoch 489/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5779 - accuracy: 0.8290\n",
      "\n",
      "Epoch 00489: loss improved from 0.58347 to 0.57700, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 490/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5726 - accuracy: 0.8237\n",
      "\n",
      "Epoch 00490: loss improved from 0.57700 to 0.57556, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 491/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5622 - accuracy: 0.8310\n",
      "\n",
      "Epoch 00491: loss did not improve from 0.57556\n",
      "Epoch 492/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5588 - accuracy: 0.8333\n",
      "\n",
      "Epoch 00492: loss did not improve from 0.57556\n",
      "Epoch 493/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5657 - accuracy: 0.8304\n",
      "\n",
      "Epoch 00493: loss did not improve from 0.57556\n",
      "Epoch 494/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5552 - accuracy: 0.8327\n",
      "\n",
      "Epoch 00494: loss improved from 0.57556 to 0.57022, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 495/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5643 - accuracy: 0.8285\n",
      "\n",
      "Epoch 00495: loss did not improve from 0.57022\n",
      "Epoch 496/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5695 - accuracy: 0.8298\n",
      "\n",
      "Epoch 00496: loss did not improve from 0.57022\n",
      "Epoch 497/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5672 - accuracy: 0.8326\n",
      "\n",
      "Epoch 00497: loss improved from 0.57022 to 0.56756, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 498/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5475 - accuracy: 0.8365\n",
      "\n",
      "Epoch 00498: loss improved from 0.56756 to 0.56544, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 499/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5768 - accuracy: 0.8225\n",
      "\n",
      "Epoch 00499: loss did not improve from 0.56544\n",
      "Epoch 500/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5569 - accuracy: 0.8283\n",
      "\n",
      "Epoch 00500: loss did not improve from 0.56544\n",
      "Epoch 501/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5871 - accuracy: 0.8220\n",
      "\n",
      "Epoch 00501: loss did not improve from 0.56544\n",
      "Epoch 502/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5530 - accuracy: 0.8366\n",
      "\n",
      "Epoch 00502: loss improved from 0.56544 to 0.56509, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 503/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5648 - accuracy: 0.8272\n",
      "\n",
      "Epoch 00503: loss did not improve from 0.56509\n",
      "Epoch 504/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5496 - accuracy: 0.8401\n",
      "\n",
      "Epoch 00504: loss improved from 0.56509 to 0.55733, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 505/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5416 - accuracy: 0.8377\n",
      "\n",
      "Epoch 00505: loss improved from 0.55733 to 0.55313, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 506/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5647 - accuracy: 0.8320\n",
      "\n",
      "Epoch 00506: loss did not improve from 0.55313\n",
      "Epoch 507/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5617 - accuracy: 0.8330\n",
      "\n",
      "Epoch 00507: loss did not improve from 0.55313\n",
      "Epoch 508/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5641 - accuracy: 0.8327\n",
      "\n",
      "Epoch 00508: loss did not improve from 0.55313\n",
      "Epoch 509/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5420 - accuracy: 0.8341\n",
      "\n",
      "Epoch 00509: loss improved from 0.55313 to 0.55200, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 510/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5424 - accuracy: 0.8387\n",
      "\n",
      "Epoch 00510: loss improved from 0.55200 to 0.55087, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 511/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5240 - accuracy: 0.8372\n",
      "\n",
      "Epoch 00511: loss improved from 0.55087 to 0.54857, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 512/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5272 - accuracy: 0.8413\n",
      "\n",
      "Epoch 00512: loss did not improve from 0.54857\n",
      "Epoch 513/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5347 - accuracy: 0.8371\n",
      "\n",
      "Epoch 00513: loss improved from 0.54857 to 0.54728, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 514/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5571 - accuracy: 0.8346\n",
      "\n",
      "Epoch 00514: loss did not improve from 0.54728\n",
      "Epoch 515/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5515 - accuracy: 0.8338\n",
      "\n",
      "Epoch 00515: loss improved from 0.54728 to 0.54467, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 516/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5097 - accuracy: 0.8484\n",
      "\n",
      "Epoch 00516: loss improved from 0.54467 to 0.54071, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 517/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5338 - accuracy: 0.8447\n",
      "\n",
      "Epoch 00517: loss did not improve from 0.54071\n",
      "Epoch 518/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5312 - accuracy: 0.8415\n",
      "\n",
      "Epoch 00518: loss did not improve from 0.54071\n",
      "Epoch 519/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5435 - accuracy: 0.8334\n",
      "\n",
      "Epoch 00519: loss improved from 0.54071 to 0.53910, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 520/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5307 - accuracy: 0.8432\n",
      "\n",
      "Epoch 00520: loss did not improve from 0.53910\n",
      "Epoch 521/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5173 - accuracy: 0.8445\n",
      "\n",
      "Epoch 00521: loss improved from 0.53910 to 0.53567, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 522/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5209 - accuracy: 0.8404\n",
      "\n",
      "Epoch 00522: loss improved from 0.53567 to 0.53180, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 523/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5199 - accuracy: 0.8423\n",
      "\n",
      "Epoch 00523: loss did not improve from 0.53180\n",
      "Epoch 524/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5236 - accuracy: 0.8396\n",
      "\n",
      "Epoch 00524: loss did not improve from 0.53180\n",
      "Epoch 525/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5155 - accuracy: 0.8434\n",
      "\n",
      "Epoch 00525: loss improved from 0.53180 to 0.52741, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 526/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5181 - accuracy: 0.8425\n",
      "\n",
      "Epoch 00526: loss improved from 0.52741 to 0.52331, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 527/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5468 - accuracy: 0.8407\n",
      "\n",
      "Epoch 00527: loss did not improve from 0.52331\n",
      "Epoch 528/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5276 - accuracy: 0.8389\n",
      "\n",
      "Epoch 00528: loss did not improve from 0.52331\n",
      "Epoch 529/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5129 - accuracy: 0.8424\n",
      "\n",
      "Epoch 00529: loss did not improve from 0.52331\n",
      "Epoch 530/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5381 - accuracy: 0.8376\n",
      "\n",
      "Epoch 00530: loss did not improve from 0.52331\n",
      "Epoch 531/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5272 - accuracy: 0.8406\n",
      "\n",
      "Epoch 00531: loss did not improve from 0.52331\n",
      "Epoch 532/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5164 - accuracy: 0.8478\n",
      "\n",
      "Epoch 00532: loss improved from 0.52331 to 0.52212, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 533/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5180 - accuracy: 0.8473\n",
      "\n",
      "Epoch 00533: loss improved from 0.52212 to 0.51975, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 534/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5145 - accuracy: 0.8489\n",
      "\n",
      "Epoch 00534: loss improved from 0.51975 to 0.51727, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 535/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5104 - accuracy: 0.8441\n",
      "\n",
      "Epoch 00535: loss did not improve from 0.51727\n",
      "Epoch 536/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5252 - accuracy: 0.8474\n",
      "\n",
      "Epoch 00536: loss did not improve from 0.51727\n",
      "Epoch 537/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5379 - accuracy: 0.8334\n",
      "\n",
      "Epoch 00537: loss did not improve from 0.51727\n",
      "Epoch 538/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5408 - accuracy: 0.8364\n",
      "\n",
      "Epoch 00538: loss did not improve from 0.51727\n",
      "Epoch 539/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5228 - accuracy: 0.8389\n",
      "\n",
      "Epoch 00539: loss did not improve from 0.51727\n",
      "Epoch 540/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.5392 - accuracy: 0.8345\n",
      "\n",
      "Epoch 00540: loss did not improve from 0.51727\n",
      "Epoch 541/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5245 - accuracy: 0.8378\n",
      "\n",
      "Epoch 00541: loss did not improve from 0.51727\n",
      "Epoch 542/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.5052 - accuracy: 0.8512\n",
      "\n",
      "Epoch 00542: loss improved from 0.51727 to 0.51682, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 543/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4982 - accuracy: 0.8570\n",
      "\n",
      "Epoch 00543: loss improved from 0.51682 to 0.50885, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 544/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.4918 - accuracy: 0.8538\n",
      "\n",
      "Epoch 00544: loss improved from 0.50885 to 0.50594, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 545/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4953 - accuracy: 0.8528\n",
      "\n",
      "Epoch 00545: loss did not improve from 0.50594\n",
      "Epoch 546/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5057 - accuracy: 0.8461\n",
      "\n",
      "Epoch 00546: loss improved from 0.50594 to 0.50113, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 547/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5084 - accuracy: 0.8444\n",
      "\n",
      "Epoch 00547: loss did not improve from 0.50113\n",
      "Epoch 548/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4924 - accuracy: 0.8550\n",
      "\n",
      "Epoch 00548: loss did not improve from 0.50113\n",
      "Epoch 549/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4790 - accuracy: 0.8501\n",
      "\n",
      "Epoch 00549: loss improved from 0.50113 to 0.49750, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 550/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4969 - accuracy: 0.8434\n",
      "\n",
      "Epoch 00550: loss improved from 0.49750 to 0.49302, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 551/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4739 - accuracy: 0.8563\n",
      "\n",
      "Epoch 00551: loss did not improve from 0.49302\n",
      "Epoch 552/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4850 - accuracy: 0.8562\n",
      "\n",
      "Epoch 00552: loss did not improve from 0.49302\n",
      "Epoch 553/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5341 - accuracy: 0.8383\n",
      "\n",
      "Epoch 00553: loss did not improve from 0.49302\n",
      "Epoch 554/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4817 - accuracy: 0.8515\n",
      "\n",
      "Epoch 00554: loss did not improve from 0.49302\n",
      "Epoch 555/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5045 - accuracy: 0.8511\n",
      "\n",
      "Epoch 00555: loss did not improve from 0.49302\n",
      "Epoch 556/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4901 - accuracy: 0.8516\n",
      "\n",
      "Epoch 00556: loss did not improve from 0.49302\n",
      "Epoch 557/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4760 - accuracy: 0.8566\n",
      "\n",
      "Epoch 00557: loss improved from 0.49302 to 0.49038, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 558/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4852 - accuracy: 0.8546\n",
      "\n",
      "Epoch 00558: loss did not improve from 0.49038\n",
      "Epoch 559/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4855 - accuracy: 0.8539\n",
      "\n",
      "Epoch 00559: loss did not improve from 0.49038\n",
      "Epoch 560/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4915 - accuracy: 0.8531\n",
      "\n",
      "Epoch 00560: loss did not improve from 0.49038\n",
      "Epoch 561/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5092 - accuracy: 0.8419\n",
      "\n",
      "Epoch 00561: loss improved from 0.49038 to 0.49005, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 562/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4953 - accuracy: 0.8471\n",
      "\n",
      "Epoch 00562: loss improved from 0.49005 to 0.48649, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 563/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4699 - accuracy: 0.8564\n",
      "\n",
      "Epoch 00563: loss improved from 0.48649 to 0.48542, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 564/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.5003 - accuracy: 0.8464\n",
      "\n",
      "Epoch 00564: loss did not improve from 0.48542\n",
      "Epoch 565/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4793 - accuracy: 0.8538\n",
      "\n",
      "Epoch 00565: loss did not improve from 0.48542\n",
      "Epoch 566/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4731 - accuracy: 0.8610\n",
      "\n",
      "Epoch 00566: loss improved from 0.48542 to 0.48110, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 567/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4648 - accuracy: 0.8554\n",
      "\n",
      "Epoch 00567: loss did not improve from 0.48110\n",
      "Epoch 568/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4729 - accuracy: 0.8581\n",
      "\n",
      "Epoch 00568: loss did not improve from 0.48110\n",
      "Epoch 569/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4841 - accuracy: 0.8550\n",
      "\n",
      "Epoch 00569: loss did not improve from 0.48110\n",
      "Epoch 570/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4723 - accuracy: 0.8566\n",
      "\n",
      "Epoch 00570: loss did not improve from 0.48110\n",
      "Epoch 571/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4807 - accuracy: 0.8560\n",
      "\n",
      "Epoch 00571: loss did not improve from 0.48110\n",
      "Epoch 572/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4824 - accuracy: 0.8537\n",
      "\n",
      "Epoch 00572: loss improved from 0.48110 to 0.47545, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 573/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4704 - accuracy: 0.8567\n",
      "\n",
      "Epoch 00573: loss improved from 0.47545 to 0.47033, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 574/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4523 - accuracy: 0.8622\n",
      "\n",
      "Epoch 00574: loss improved from 0.47033 to 0.46989, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 575/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4466 - accuracy: 0.8678\n",
      "\n",
      "Epoch 00575: loss did not improve from 0.46989\n",
      "Epoch 576/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4724 - accuracy: 0.8532\n",
      "\n",
      "Epoch 00576: loss did not improve from 0.46989\n",
      "Epoch 577/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4558 - accuracy: 0.8620\n",
      "\n",
      "Epoch 00577: loss improved from 0.46989 to 0.46680, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 578/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4547 - accuracy: 0.8631\n",
      "\n",
      "Epoch 00578: loss improved from 0.46680 to 0.46354, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 579/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4641 - accuracy: 0.8628\n",
      "\n",
      "Epoch 00579: loss did not improve from 0.46354\n",
      "Epoch 580/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4735 - accuracy: 0.8528\n",
      "\n",
      "Epoch 00580: loss did not improve from 0.46354\n",
      "Epoch 581/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4676 - accuracy: 0.8622\n",
      "\n",
      "Epoch 00581: loss did not improve from 0.46354\n",
      "Epoch 582/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4787 - accuracy: 0.8577\n",
      "\n",
      "Epoch 00582: loss did not improve from 0.46354\n",
      "Epoch 583/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4664 - accuracy: 0.8631\n",
      "\n",
      "Epoch 00583: loss did not improve from 0.46354\n",
      "Epoch 584/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4696 - accuracy: 0.8520\n",
      "\n",
      "Epoch 00584: loss did not improve from 0.46354\n",
      "Epoch 585/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4630 - accuracy: 0.8568\n",
      "\n",
      "Epoch 00585: loss did not improve from 0.46354\n",
      "Epoch 586/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4550 - accuracy: 0.8603\n",
      "\n",
      "Epoch 00586: loss did not improve from 0.46354\n",
      "Epoch 587/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4490 - accuracy: 0.8634\n",
      "\n",
      "Epoch 00587: loss improved from 0.46354 to 0.45657, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 588/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4526 - accuracy: 0.8667\n",
      "\n",
      "Epoch 00588: loss improved from 0.45657 to 0.45433, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 589/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4685 - accuracy: 0.8587\n",
      "\n",
      "Epoch 00589: loss did not improve from 0.45433\n",
      "Epoch 590/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4700 - accuracy: 0.8591\n",
      "\n",
      "Epoch 00590: loss did not improve from 0.45433\n",
      "Epoch 591/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4463 - accuracy: 0.8611\n",
      "\n",
      "Epoch 00591: loss did not improve from 0.45433\n",
      "Epoch 592/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4404 - accuracy: 0.8596\n",
      "\n",
      "Epoch 00592: loss improved from 0.45433 to 0.45332, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 593/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4604 - accuracy: 0.8624\n",
      "\n",
      "Epoch 00593: loss did not improve from 0.45332\n",
      "Epoch 594/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4487 - accuracy: 0.8626\n",
      "\n",
      "Epoch 00594: loss did not improve from 0.45332\n",
      "Epoch 595/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4519 - accuracy: 0.8621\n",
      "\n",
      "Epoch 00595: loss did not improve from 0.45332\n",
      "Epoch 596/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4469 - accuracy: 0.8675\n",
      "\n",
      "Epoch 00596: loss did not improve from 0.45332\n",
      "Epoch 597/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4435 - accuracy: 0.8622\n",
      "\n",
      "Epoch 00597: loss did not improve from 0.45332\n",
      "Epoch 598/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4664 - accuracy: 0.8582\n",
      "\n",
      "Epoch 00598: loss did not improve from 0.45332\n",
      "Epoch 599/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4829 - accuracy: 0.8470\n",
      "\n",
      "Epoch 00599: loss did not improve from 0.45332\n",
      "Epoch 600/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4717 - accuracy: 0.8526\n",
      "\n",
      "Epoch 00600: loss did not improve from 0.45332\n",
      "Epoch 601/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4528 - accuracy: 0.8590\n",
      "\n",
      "Epoch 00601: loss did not improve from 0.45332\n",
      "Epoch 602/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4454 - accuracy: 0.8654\n",
      "\n",
      "Epoch 00602: loss improved from 0.45332 to 0.44580, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 603/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4406 - accuracy: 0.8641\n",
      "\n",
      "Epoch 00603: loss improved from 0.44580 to 0.44179, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 604/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4417 - accuracy: 0.8666\n",
      "\n",
      "Epoch 00604: loss improved from 0.44179 to 0.43789, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 605/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4355 - accuracy: 0.8685\n",
      "\n",
      "Epoch 00605: loss did not improve from 0.43789\n",
      "Epoch 606/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4342 - accuracy: 0.8649\n",
      "\n",
      "Epoch 00606: loss did not improve from 0.43789\n",
      "Epoch 607/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4363 - accuracy: 0.8659\n",
      "\n",
      "Epoch 00607: loss did not improve from 0.43789\n",
      "Epoch 608/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4361 - accuracy: 0.8667\n",
      "\n",
      "Epoch 00608: loss did not improve from 0.43789\n",
      "Epoch 609/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4377 - accuracy: 0.8630\n",
      "\n",
      "Epoch 00609: loss did not improve from 0.43789\n",
      "Epoch 610/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4388 - accuracy: 0.8599\n",
      "\n",
      "Epoch 00610: loss improved from 0.43789 to 0.43730, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 611/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4254 - accuracy: 0.8717\n",
      "\n",
      "Epoch 00611: loss improved from 0.43730 to 0.43583, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 612/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4227 - accuracy: 0.8712\n",
      "\n",
      "Epoch 00612: loss improved from 0.43583 to 0.43202, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 613/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4450 - accuracy: 0.8686\n",
      "\n",
      "Epoch 00613: loss did not improve from 0.43202\n",
      "Epoch 614/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4441 - accuracy: 0.8669\n",
      "\n",
      "Epoch 00614: loss did not improve from 0.43202\n",
      "Epoch 615/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.4379 - accuracy: 0.8672\n",
      "\n",
      "Epoch 00615: loss did not improve from 0.43202\n",
      "Epoch 616/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4161 - accuracy: 0.8754\n",
      "\n",
      "Epoch 00616: loss did not improve from 0.43202\n",
      "Epoch 617/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4282 - accuracy: 0.8706\n",
      "\n",
      "Epoch 00617: loss did not improve from 0.43202\n",
      "Epoch 618/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4308 - accuracy: 0.8720\n",
      "\n",
      "Epoch 00618: loss did not improve from 0.43202\n",
      "Epoch 619/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4298 - accuracy: 0.8702\n",
      "\n",
      "Epoch 00619: loss did not improve from 0.43202\n",
      "Epoch 620/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4284 - accuracy: 0.8745\n",
      "\n",
      "Epoch 00620: loss improved from 0.43202 to 0.43060, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 621/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4300 - accuracy: 0.8671\n",
      "\n",
      "Epoch 00621: loss did not improve from 0.43060\n",
      "Epoch 622/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4211 - accuracy: 0.8711\n",
      "\n",
      "Epoch 00622: loss did not improve from 0.43060\n",
      "Epoch 623/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4265 - accuracy: 0.8706\n",
      "\n",
      "Epoch 00623: loss did not improve from 0.43060\n",
      "Epoch 624/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4262 - accuracy: 0.8701\n",
      "\n",
      "Epoch 00624: loss did not improve from 0.43060\n",
      "Epoch 625/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4228 - accuracy: 0.8687\n",
      "\n",
      "Epoch 00625: loss improved from 0.43060 to 0.42654, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 626/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4163 - accuracy: 0.8755\n",
      "\n",
      "Epoch 00626: loss improved from 0.42654 to 0.42320, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 627/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4216 - accuracy: 0.8710\n",
      "\n",
      "Epoch 00627: loss did not improve from 0.42320\n",
      "Epoch 628/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4104 - accuracy: 0.8714\n",
      "\n",
      "Epoch 00628: loss improved from 0.42320 to 0.42171, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 629/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4079 - accuracy: 0.8767\n",
      "\n",
      "Epoch 00629: loss improved from 0.42171 to 0.42148, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 630/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4011 - accuracy: 0.8754\n",
      "\n",
      "Epoch 00630: loss did not improve from 0.42148\n",
      "Epoch 631/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4092 - accuracy: 0.8724\n",
      "\n",
      "Epoch 00631: loss did not improve from 0.42148\n",
      "Epoch 632/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4341 - accuracy: 0.8720\n",
      "\n",
      "Epoch 00632: loss did not improve from 0.42148\n",
      "Epoch 633/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4096 - accuracy: 0.8779\n",
      "\n",
      "Epoch 00633: loss did not improve from 0.42148\n",
      "Epoch 634/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4198 - accuracy: 0.8688\n",
      "\n",
      "Epoch 00634: loss improved from 0.42148 to 0.41878, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 635/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4091 - accuracy: 0.8771\n",
      "\n",
      "Epoch 00635: loss improved from 0.41878 to 0.41695, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 636/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4270 - accuracy: 0.8721\n",
      "\n",
      "Epoch 00636: loss did not improve from 0.41695\n",
      "Epoch 637/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4289 - accuracy: 0.8668\n",
      "\n",
      "Epoch 00637: loss did not improve from 0.41695\n",
      "Epoch 638/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4217 - accuracy: 0.8701\n",
      "\n",
      "Epoch 00638: loss did not improve from 0.41695\n",
      "Epoch 639/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4236 - accuracy: 0.8677\n",
      "\n",
      "Epoch 00639: loss improved from 0.41695 to 0.41600, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 640/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4217 - accuracy: 0.8645\n",
      "\n",
      "Epoch 00640: loss did not improve from 0.41600\n",
      "Epoch 641/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3983 - accuracy: 0.8777\n",
      "\n",
      "Epoch 00641: loss did not improve from 0.41600\n",
      "Epoch 642/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4171 - accuracy: 0.8723\n",
      "\n",
      "Epoch 00642: loss did not improve from 0.41600\n",
      "Epoch 643/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4158 - accuracy: 0.8742\n",
      "\n",
      "Epoch 00643: loss did not improve from 0.41600\n",
      "Epoch 644/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3991 - accuracy: 0.8755\n",
      "\n",
      "Epoch 00644: loss improved from 0.41600 to 0.41361, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 645/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4144 - accuracy: 0.8775\n",
      "\n",
      "Epoch 00645: loss did not improve from 0.41361\n",
      "Epoch 646/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4065 - accuracy: 0.8739\n",
      "\n",
      "Epoch 00646: loss improved from 0.41361 to 0.41029, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 647/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4134 - accuracy: 0.8759\n",
      "\n",
      "Epoch 00647: loss improved from 0.41029 to 0.40765, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 648/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3942 - accuracy: 0.8759\n",
      "\n",
      "Epoch 00648: loss improved from 0.40765 to 0.40554, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 649/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4013 - accuracy: 0.8758\n",
      "\n",
      "Epoch 00649: loss did not improve from 0.40554\n",
      "Epoch 650/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4177 - accuracy: 0.8734\n",
      "\n",
      "Epoch 00650: loss did not improve from 0.40554\n",
      "Epoch 651/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4078 - accuracy: 0.8749\n",
      "\n",
      "Epoch 00651: loss did not improve from 0.40554\n",
      "Epoch 652/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4022 - accuracy: 0.8743\n",
      "\n",
      "Epoch 00652: loss did not improve from 0.40554\n",
      "Epoch 653/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3987 - accuracy: 0.8745\n",
      "\n",
      "Epoch 00653: loss did not improve from 0.40554\n",
      "Epoch 654/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4263 - accuracy: 0.8639\n",
      "\n",
      "Epoch 00654: loss did not improve from 0.40554\n",
      "Epoch 655/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3939 - accuracy: 0.8769\n",
      "\n",
      "Epoch 00655: loss improved from 0.40554 to 0.40508, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 656/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3955 - accuracy: 0.8824\n",
      "\n",
      "Epoch 00656: loss improved from 0.40508 to 0.39751, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 657/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3663 - accuracy: 0.8833\n",
      "\n",
      "Epoch 00657: loss improved from 0.39751 to 0.39278, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 658/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3918 - accuracy: 0.8798\n",
      "\n",
      "Epoch 00658: loss improved from 0.39278 to 0.39181, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 659/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3898 - accuracy: 0.8806\n",
      "\n",
      "Epoch 00659: loss did not improve from 0.39181\n",
      "Epoch 660/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3925 - accuracy: 0.8772\n",
      "\n",
      "Epoch 00660: loss did not improve from 0.39181\n",
      "Epoch 661/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3894 - accuracy: 0.8812\n",
      "\n",
      "Epoch 00661: loss improved from 0.39181 to 0.39145, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 662/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4108 - accuracy: 0.8747\n",
      "\n",
      "Epoch 00662: loss did not improve from 0.39145\n",
      "Epoch 663/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3983 - accuracy: 0.8741\n",
      "\n",
      "Epoch 00663: loss did not improve from 0.39145\n",
      "Epoch 664/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3812 - accuracy: 0.8827\n",
      "\n",
      "Epoch 00664: loss did not improve from 0.39145\n",
      "Epoch 665/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3815 - accuracy: 0.8832\n",
      "\n",
      "Epoch 00665: loss did not improve from 0.39145\n",
      "Epoch 666/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3822 - accuracy: 0.8827\n",
      "\n",
      "Epoch 00666: loss did not improve from 0.39145\n",
      "Epoch 667/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4031 - accuracy: 0.8749\n",
      "\n",
      "Epoch 00667: loss did not improve from 0.39145\n",
      "Epoch 668/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3942 - accuracy: 0.8781\n",
      "\n",
      "Epoch 00668: loss did not improve from 0.39145\n",
      "Epoch 669/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4044 - accuracy: 0.8746\n",
      "\n",
      "Epoch 00669: loss did not improve from 0.39145\n",
      "Epoch 670/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4025 - accuracy: 0.8757\n",
      "\n",
      "Epoch 00670: loss did not improve from 0.39145\n",
      "Epoch 671/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3894 - accuracy: 0.8816\n",
      "\n",
      "Epoch 00671: loss did not improve from 0.39145\n",
      "Epoch 672/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3916 - accuracy: 0.8786\n",
      "\n",
      "Epoch 00672: loss did not improve from 0.39145\n",
      "Epoch 673/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3847 - accuracy: 0.8770\n",
      "\n",
      "Epoch 00673: loss did not improve from 0.39145\n",
      "Epoch 674/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.3991 - accuracy: 0.8735\n",
      "\n",
      "Epoch 00674: loss did not improve from 0.39145\n",
      "Epoch 675/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.3880 - accuracy: 0.8825\n",
      "\n",
      "Epoch 00675: loss did not improve from 0.39145\n",
      "Epoch 676/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3835 - accuracy: 0.8785\n",
      "\n",
      "Epoch 00676: loss did not improve from 0.39145\n",
      "Epoch 677/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.4030 - accuracy: 0.8703\n",
      "\n",
      "Epoch 00677: loss did not improve from 0.39145\n",
      "Epoch 678/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3967 - accuracy: 0.8743\n",
      "\n",
      "Epoch 00678: loss did not improve from 0.39145\n",
      "Epoch 679/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3888 - accuracy: 0.8756\n",
      "\n",
      "Epoch 00679: loss improved from 0.39145 to 0.38794, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 680/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3820 - accuracy: 0.8820\n",
      "\n",
      "Epoch 00680: loss did not improve from 0.38794\n",
      "Epoch 681/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3912 - accuracy: 0.8772\n",
      "\n",
      "Epoch 00681: loss did not improve from 0.38794\n",
      "Epoch 682/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3806 - accuracy: 0.8789\n",
      "\n",
      "Epoch 00682: loss did not improve from 0.38794\n",
      "Epoch 683/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3794 - accuracy: 0.8806\n",
      "\n",
      "Epoch 00683: loss did not improve from 0.38794\n",
      "Epoch 684/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3687 - accuracy: 0.8834\n",
      "\n",
      "Epoch 00684: loss improved from 0.38794 to 0.38098, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 685/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3745 - accuracy: 0.8847\n",
      "\n",
      "Epoch 00685: loss improved from 0.38098 to 0.37727, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 686/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3689 - accuracy: 0.8794\n",
      "\n",
      "Epoch 00686: loss improved from 0.37727 to 0.37585, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 687/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3821 - accuracy: 0.8771\n",
      "\n",
      "Epoch 00687: loss did not improve from 0.37585\n",
      "Epoch 688/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3748 - accuracy: 0.8831\n",
      "\n",
      "Epoch 00688: loss did not improve from 0.37585\n",
      "Epoch 689/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3690 - accuracy: 0.8848\n",
      "\n",
      "Epoch 00689: loss did not improve from 0.37585\n",
      "Epoch 690/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.3776 - accuracy: 0.8817\n",
      "\n",
      "Epoch 00690: loss did not improve from 0.37585\n",
      "Epoch 691/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3650 - accuracy: 0.8832\n",
      "\n",
      "Epoch 00691: loss did not improve from 0.37585\n",
      "Epoch 692/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3971 - accuracy: 0.8736\n",
      "\n",
      "Epoch 00692: loss did not improve from 0.37585\n",
      "Epoch 693/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3937 - accuracy: 0.8750\n",
      "\n",
      "Epoch 00693: loss did not improve from 0.37585\n",
      "Epoch 694/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3620 - accuracy: 0.8842\n",
      "\n",
      "Epoch 00694: loss did not improve from 0.37585\n",
      "Epoch 695/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3720 - accuracy: 0.8886\n",
      "\n",
      "Epoch 00695: loss did not improve from 0.37585\n",
      "Epoch 696/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3673 - accuracy: 0.8821\n",
      "\n",
      "Epoch 00696: loss improved from 0.37585 to 0.37534, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 697/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3580 - accuracy: 0.8818\n",
      "\n",
      "Epoch 00697: loss did not improve from 0.37534\n",
      "Epoch 698/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3581 - accuracy: 0.8902\n",
      "\n",
      "Epoch 00698: loss improved from 0.37534 to 0.37011, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 699/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3728 - accuracy: 0.8835\n",
      "\n",
      "Epoch 00699: loss did not improve from 0.37011\n",
      "Epoch 700/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3706 - accuracy: 0.8862\n",
      "\n",
      "Epoch 00700: loss did not improve from 0.37011\n",
      "Epoch 701/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3779 - accuracy: 0.8828\n",
      "\n",
      "Epoch 00701: loss did not improve from 0.37011\n",
      "Epoch 702/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3670 - accuracy: 0.8814\n",
      "\n",
      "Epoch 00702: loss did not improve from 0.37011\n",
      "Epoch 703/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3856 - accuracy: 0.8799\n",
      "\n",
      "Epoch 00703: loss did not improve from 0.37011\n",
      "Epoch 704/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3688 - accuracy: 0.8797\n",
      "\n",
      "Epoch 00704: loss did not improve from 0.37011\n",
      "Epoch 705/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3660 - accuracy: 0.8806\n",
      "\n",
      "Epoch 00705: loss improved from 0.37011 to 0.36506, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 706/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3616 - accuracy: 0.8860\n",
      "\n",
      "Epoch 00706: loss improved from 0.36506 to 0.36280, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 707/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3685 - accuracy: 0.8884\n",
      "\n",
      "Epoch 00707: loss did not improve from 0.36280\n",
      "Epoch 708/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3731 - accuracy: 0.8732\n",
      "\n",
      "Epoch 00708: loss did not improve from 0.36280\n",
      "Epoch 709/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3871 - accuracy: 0.8810\n",
      "\n",
      "Epoch 00709: loss did not improve from 0.36280\n",
      "Epoch 710/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3669 - accuracy: 0.8868\n",
      "\n",
      "Epoch 00710: loss did not improve from 0.36280\n",
      "Epoch 711/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3582 - accuracy: 0.8834\n",
      "\n",
      "Epoch 00711: loss did not improve from 0.36280\n",
      "Epoch 712/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3645 - accuracy: 0.8838\n",
      "\n",
      "Epoch 00712: loss did not improve from 0.36280\n",
      "Epoch 713/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3530 - accuracy: 0.8903\n",
      "\n",
      "Epoch 00713: loss did not improve from 0.36280\n",
      "Epoch 714/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3539 - accuracy: 0.8897\n",
      "\n",
      "Epoch 00714: loss improved from 0.36280 to 0.36166, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 715/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3344 - accuracy: 0.8956\n",
      "\n",
      "Epoch 00715: loss improved from 0.36166 to 0.36124, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 716/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3576 - accuracy: 0.8847\n",
      "\n",
      "Epoch 00716: loss did not improve from 0.36124\n",
      "Epoch 717/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3327 - accuracy: 0.8962\n",
      "\n",
      "Epoch 00717: loss improved from 0.36124 to 0.35515, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 718/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3424 - accuracy: 0.8940\n",
      "\n",
      "Epoch 00718: loss did not improve from 0.35515\n",
      "Epoch 719/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3622 - accuracy: 0.8848\n",
      "\n",
      "Epoch 00719: loss did not improve from 0.35515\n",
      "Epoch 720/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3714 - accuracy: 0.8776\n",
      "\n",
      "Epoch 00720: loss did not improve from 0.35515\n",
      "Epoch 721/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3664 - accuracy: 0.8833\n",
      "\n",
      "Epoch 00721: loss did not improve from 0.35515\n",
      "Epoch 722/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3615 - accuracy: 0.8835\n",
      "\n",
      "Epoch 00722: loss did not improve from 0.35515\n",
      "Epoch 723/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3500 - accuracy: 0.8919\n",
      "\n",
      "Epoch 00723: loss did not improve from 0.35515\n",
      "Epoch 724/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3674 - accuracy: 0.8857\n",
      "\n",
      "Epoch 00724: loss did not improve from 0.35515\n",
      "Epoch 725/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3566 - accuracy: 0.8885\n",
      "\n",
      "Epoch 00725: loss did not improve from 0.35515\n",
      "Epoch 726/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3745 - accuracy: 0.8816\n",
      "\n",
      "Epoch 00726: loss did not improve from 0.35515\n",
      "Epoch 727/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3529 - accuracy: 0.8881\n",
      "\n",
      "Epoch 00727: loss did not improve from 0.35515\n",
      "Epoch 728/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3476 - accuracy: 0.8886\n",
      "\n",
      "Epoch 00728: loss did not improve from 0.35515\n",
      "Epoch 729/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3545 - accuracy: 0.8839\n",
      "\n",
      "Epoch 00729: loss did not improve from 0.35515\n",
      "Epoch 730/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3603 - accuracy: 0.8829\n",
      "\n",
      "Epoch 00730: loss did not improve from 0.35515\n",
      "Epoch 731/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3596 - accuracy: 0.8865\n",
      "\n",
      "Epoch 00731: loss did not improve from 0.35515\n",
      "Epoch 732/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3480 - accuracy: 0.8952\n",
      "\n",
      "Epoch 00732: loss did not improve from 0.35515\n",
      "Epoch 733/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3309 - accuracy: 0.8975\n",
      "\n",
      "Epoch 00733: loss improved from 0.35515 to 0.34821, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 734/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3374 - accuracy: 0.8957\n",
      "\n",
      "Epoch 00734: loss improved from 0.34821 to 0.34319, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 735/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3221 - accuracy: 0.9037\n",
      "\n",
      "Epoch 00735: loss did not improve from 0.34319\n",
      "Epoch 736/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3447 - accuracy: 0.8900\n",
      "\n",
      "Epoch 00736: loss did not improve from 0.34319\n",
      "Epoch 737/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3492 - accuracy: 0.8845\n",
      "\n",
      "Epoch 00737: loss did not improve from 0.34319\n",
      "Epoch 738/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3362 - accuracy: 0.8885\n",
      "\n",
      "Epoch 00738: loss improved from 0.34319 to 0.34301, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 739/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3390 - accuracy: 0.8911\n",
      "\n",
      "Epoch 00739: loss did not improve from 0.34301\n",
      "Epoch 740/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3329 - accuracy: 0.8952\n",
      "\n",
      "Epoch 00740: loss did not improve from 0.34301\n",
      "Epoch 741/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3414 - accuracy: 0.8938\n",
      "\n",
      "Epoch 00741: loss did not improve from 0.34301\n",
      "Epoch 742/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3459 - accuracy: 0.8907\n",
      "\n",
      "Epoch 00742: loss did not improve from 0.34301\n",
      "Epoch 743/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3291 - accuracy: 0.8974\n",
      "\n",
      "Epoch 00743: loss improved from 0.34301 to 0.34049, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 744/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3509 - accuracy: 0.8879\n",
      "\n",
      "Epoch 00744: loss did not improve from 0.34049\n",
      "Epoch 745/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3298 - accuracy: 0.8933\n",
      "\n",
      "Epoch 00745: loss improved from 0.34049 to 0.33926, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 746/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3423 - accuracy: 0.8912\n",
      "\n",
      "Epoch 00746: loss did not improve from 0.33926\n",
      "Epoch 747/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3353 - accuracy: 0.8929\n",
      "\n",
      "Epoch 00747: loss did not improve from 0.33926\n",
      "Epoch 748/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3505 - accuracy: 0.8868\n",
      "\n",
      "Epoch 00748: loss did not improve from 0.33926\n",
      "Epoch 749/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3357 - accuracy: 0.8913\n",
      "\n",
      "Epoch 00749: loss did not improve from 0.33926\n",
      "Epoch 750/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3647 - accuracy: 0.8828\n",
      "\n",
      "Epoch 00750: loss did not improve from 0.33926\n",
      "Epoch 751/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3486 - accuracy: 0.8848\n",
      "\n",
      "Epoch 00751: loss did not improve from 0.33926\n",
      "Epoch 752/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3470 - accuracy: 0.8850\n",
      "\n",
      "Epoch 00752: loss did not improve from 0.33926\n",
      "Epoch 753/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3293 - accuracy: 0.8929\n",
      "\n",
      "Epoch 00753: loss did not improve from 0.33926\n",
      "Epoch 754/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3259 - accuracy: 0.8993\n",
      "\n",
      "Epoch 00754: loss improved from 0.33926 to 0.33600, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 755/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3424 - accuracy: 0.8857\n",
      "\n",
      "Epoch 00755: loss did not improve from 0.33600\n",
      "Epoch 756/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3391 - accuracy: 0.8922\n",
      "\n",
      "Epoch 00756: loss did not improve from 0.33600\n",
      "Epoch 757/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3390 - accuracy: 0.8925\n",
      "\n",
      "Epoch 00757: loss did not improve from 0.33600\n",
      "Epoch 758/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3472 - accuracy: 0.8878\n",
      "\n",
      "Epoch 00758: loss did not improve from 0.33600\n",
      "Epoch 759/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3627 - accuracy: 0.8816\n",
      "\n",
      "Epoch 00759: loss did not improve from 0.33600\n",
      "Epoch 760/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3766 - accuracy: 0.8784\n",
      "\n",
      "Epoch 00760: loss did not improve from 0.33600\n",
      "Epoch 761/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3738 - accuracy: 0.8787\n",
      "\n",
      "Epoch 00761: loss did not improve from 0.33600\n",
      "Epoch 762/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3333 - accuracy: 0.8925\n",
      "\n",
      "Epoch 00762: loss did not improve from 0.33600\n",
      "Epoch 763/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3495 - accuracy: 0.8819\n",
      "\n",
      "Epoch 00763: loss did not improve from 0.33600\n",
      "Epoch 764/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3125 - accuracy: 0.8964\n",
      "\n",
      "Epoch 00764: loss improved from 0.33600 to 0.32774, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 765/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3350 - accuracy: 0.8979\n",
      "\n",
      "Epoch 00765: loss did not improve from 0.32774\n",
      "Epoch 766/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3211 - accuracy: 0.8948\n",
      "\n",
      "Epoch 00766: loss did not improve from 0.32774\n",
      "Epoch 767/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3229 - accuracy: 0.8917\n",
      "\n",
      "Epoch 00767: loss improved from 0.32774 to 0.32763, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 768/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3239 - accuracy: 0.8981\n",
      "\n",
      "Epoch 00768: loss improved from 0.32763 to 0.32720, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 769/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3260 - accuracy: 0.8958\n",
      "\n",
      "Epoch 00769: loss did not improve from 0.32720\n",
      "Epoch 770/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3187 - accuracy: 0.9000\n",
      "\n",
      "Epoch 00770: loss did not improve from 0.32720\n",
      "Epoch 771/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3352 - accuracy: 0.8893\n",
      "\n",
      "Epoch 00771: loss improved from 0.32720 to 0.32347, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 772/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3194 - accuracy: 0.8994\n",
      "\n",
      "Epoch 00772: loss did not improve from 0.32347\n",
      "Epoch 773/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3249 - accuracy: 0.8988\n",
      "\n",
      "Epoch 00773: loss did not improve from 0.32347\n",
      "Epoch 774/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3290 - accuracy: 0.8904\n",
      "\n",
      "Epoch 00774: loss did not improve from 0.32347\n",
      "Epoch 775/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3126 - accuracy: 0.8956\n",
      "\n",
      "Epoch 00775: loss did not improve from 0.32347\n",
      "Epoch 776/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3436 - accuracy: 0.8920\n",
      "\n",
      "Epoch 00776: loss did not improve from 0.32347\n",
      "Epoch 777/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3513 - accuracy: 0.8845\n",
      "\n",
      "Epoch 00777: loss did not improve from 0.32347\n",
      "Epoch 778/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3314 - accuracy: 0.8929\n",
      "\n",
      "Epoch 00778: loss did not improve from 0.32347\n",
      "Epoch 779/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.9022\n",
      "\n",
      "Epoch 00779: loss did not improve from 0.32347\n",
      "Epoch 780/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3193 - accuracy: 0.8958\n",
      "\n",
      "Epoch 00780: loss improved from 0.32347 to 0.32129, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 781/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3066 - accuracy: 0.8996\n",
      "\n",
      "Epoch 00781: loss improved from 0.32129 to 0.31687, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 782/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3100 - accuracy: 0.8998\n",
      "\n",
      "Epoch 00782: loss did not improve from 0.31687\n",
      "Epoch 783/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3086 - accuracy: 0.8974\n",
      "\n",
      "Epoch 00783: loss did not improve from 0.31687\n",
      "Epoch 784/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3033 - accuracy: 0.9010\n",
      "\n",
      "Epoch 00784: loss did not improve from 0.31687\n",
      "Epoch 785/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3235 - accuracy: 0.8936\n",
      "\n",
      "Epoch 00785: loss did not improve from 0.31687\n",
      "Epoch 786/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3331 - accuracy: 0.8914\n",
      "\n",
      "Epoch 00786: loss did not improve from 0.31687\n",
      "Epoch 787/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3260 - accuracy: 0.8932\n",
      "\n",
      "Epoch 00787: loss did not improve from 0.31687\n",
      "Epoch 788/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3390 - accuracy: 0.8879\n",
      "\n",
      "Epoch 00788: loss did not improve from 0.31687\n",
      "Epoch 789/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3298 - accuracy: 0.8940\n",
      "\n",
      "Epoch 00789: loss did not improve from 0.31687\n",
      "Epoch 790/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3394 - accuracy: 0.8907\n",
      "\n",
      "Epoch 00790: loss did not improve from 0.31687\n",
      "Epoch 791/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3414 - accuracy: 0.8923\n",
      "\n",
      "Epoch 00791: loss did not improve from 0.31687\n",
      "Epoch 792/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3439 - accuracy: 0.8838\n",
      "\n",
      "Epoch 00792: loss did not improve from 0.31687\n",
      "Epoch 793/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3113 - accuracy: 0.8996\n",
      "\n",
      "Epoch 00793: loss did not improve from 0.31687\n",
      "Epoch 794/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.8999\n",
      "\n",
      "Epoch 00794: loss did not improve from 0.31687\n",
      "Epoch 795/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3273 - accuracy: 0.8920\n",
      "\n",
      "Epoch 00795: loss did not improve from 0.31687\n",
      "Epoch 796/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3057 - accuracy: 0.9024\n",
      "\n",
      "Epoch 00796: loss improved from 0.31687 to 0.31346, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 797/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3121 - accuracy: 0.8969\n",
      "\n",
      "Epoch 00797: loss improved from 0.31346 to 0.31038, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 798/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2888 - accuracy: 0.9068\n",
      "\n",
      "Epoch 00798: loss improved from 0.31038 to 0.30869, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 799/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2945 - accuracy: 0.8991\n",
      "\n",
      "Epoch 00799: loss did not improve from 0.30869\n",
      "Epoch 800/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2929 - accuracy: 0.9015\n",
      "\n",
      "Epoch 00800: loss improved from 0.30869 to 0.30863, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 801/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3034 - accuracy: 0.8972\n",
      "\n",
      "Epoch 00801: loss improved from 0.30863 to 0.30722, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 802/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2997 - accuracy: 0.9015\n",
      "\n",
      "Epoch 00802: loss improved from 0.30722 to 0.30638, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 803/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2851 - accuracy: 0.9000\n",
      "\n",
      "Epoch 00803: loss improved from 0.30638 to 0.30443, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 804/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3169 - accuracy: 0.8959\n",
      "\n",
      "Epoch 00804: loss did not improve from 0.30443\n",
      "Epoch 805/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3032 - accuracy: 0.9043\n",
      "\n",
      "Epoch 00805: loss did not improve from 0.30443\n",
      "Epoch 806/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3228 - accuracy: 0.8978\n",
      "\n",
      "Epoch 00806: loss did not improve from 0.30443\n",
      "Epoch 807/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3043 - accuracy: 0.9012\n",
      "\n",
      "Epoch 00807: loss did not improve from 0.30443\n",
      "Epoch 808/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2978 - accuracy: 0.9039\n",
      "\n",
      "Epoch 00808: loss did not improve from 0.30443\n",
      "Epoch 809/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2919 - accuracy: 0.8993\n",
      "\n",
      "Epoch 00809: loss did not improve from 0.30443\n",
      "Epoch 810/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3286 - accuracy: 0.8889\n",
      "\n",
      "Epoch 00810: loss did not improve from 0.30443\n",
      "Epoch 811/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3061 - accuracy: 0.8992\n",
      "\n",
      "Epoch 00811: loss did not improve from 0.30443\n",
      "Epoch 812/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3051 - accuracy: 0.9011\n",
      "\n",
      "Epoch 00812: loss did not improve from 0.30443\n",
      "Epoch 813/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3289 - accuracy: 0.8890\n",
      "\n",
      "Epoch 00813: loss did not improve from 0.30443\n",
      "Epoch 814/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3040 - accuracy: 0.8991\n",
      "\n",
      "Epoch 00814: loss did not improve from 0.30443\n",
      "Epoch 815/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3108 - accuracy: 0.9006\n",
      "\n",
      "Epoch 00815: loss did not improve from 0.30443\n",
      "Epoch 816/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3302 - accuracy: 0.8924\n",
      "\n",
      "Epoch 00816: loss did not improve from 0.30443\n",
      "Epoch 817/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2953 - accuracy: 0.9029\n",
      "\n",
      "Epoch 00817: loss did not improve from 0.30443\n",
      "Epoch 818/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3188 - accuracy: 0.8965\n",
      "\n",
      "Epoch 00818: loss did not improve from 0.30443\n",
      "Epoch 819/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2965 - accuracy: 0.9000\n",
      "\n",
      "Epoch 00819: loss did not improve from 0.30443\n",
      "Epoch 820/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3084 - accuracy: 0.8942\n",
      "\n",
      "Epoch 00820: loss did not improve from 0.30443\n",
      "Epoch 821/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3244 - accuracy: 0.8903\n",
      "\n",
      "Epoch 00821: loss did not improve from 0.30443\n",
      "Epoch 822/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3116 - accuracy: 0.8936\n",
      "\n",
      "Epoch 00822: loss did not improve from 0.30443\n",
      "Epoch 823/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3190 - accuracy: 0.8928\n",
      "\n",
      "Epoch 00823: loss did not improve from 0.30443\n",
      "Epoch 824/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2882 - accuracy: 0.9086\n",
      "\n",
      "Epoch 00824: loss improved from 0.30443 to 0.30010, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 825/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2937 - accuracy: 0.8996\n",
      "\n",
      "Epoch 00825: loss improved from 0.30010 to 0.29685, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 826/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3026 - accuracy: 0.9005\n",
      "\n",
      "Epoch 00826: loss did not improve from 0.29685\n",
      "Epoch 827/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3072 - accuracy: 0.8994\n",
      "\n",
      "Epoch 00827: loss did not improve from 0.29685\n",
      "Epoch 828/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2980 - accuracy: 0.8980\n",
      "\n",
      "Epoch 00828: loss did not improve from 0.29685\n",
      "Epoch 829/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2923 - accuracy: 0.9007\n",
      "\n",
      "Epoch 00829: loss did not improve from 0.29685\n",
      "Epoch 830/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2921 - accuracy: 0.9024\n",
      "\n",
      "Epoch 00830: loss did not improve from 0.29685\n",
      "Epoch 831/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2913 - accuracy: 0.9022\n",
      "\n",
      "Epoch 00831: loss did not improve from 0.29685\n",
      "Epoch 832/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3022 - accuracy: 0.9040\n",
      "\n",
      "Epoch 00832: loss did not improve from 0.29685\n",
      "Epoch 833/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3084 - accuracy: 0.8988\n",
      "\n",
      "Epoch 00833: loss did not improve from 0.29685\n",
      "Epoch 834/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2959 - accuracy: 0.8999\n",
      "\n",
      "Epoch 00834: loss did not improve from 0.29685\n",
      "Epoch 835/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2971 - accuracy: 0.8967\n",
      "\n",
      "Epoch 00835: loss did not improve from 0.29685\n",
      "Epoch 836/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2874 - accuracy: 0.9061\n",
      "\n",
      "Epoch 00836: loss did not improve from 0.29685\n",
      "Epoch 837/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3066 - accuracy: 0.8994\n",
      "\n",
      "Epoch 00837: loss did not improve from 0.29685\n",
      "Epoch 838/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3097 - accuracy: 0.8957\n",
      "\n",
      "Epoch 00838: loss did not improve from 0.29685\n",
      "Epoch 839/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2957 - accuracy: 0.8964\n",
      "\n",
      "Epoch 00839: loss did not improve from 0.29685\n",
      "Epoch 840/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2805 - accuracy: 0.9079\n",
      "\n",
      "Epoch 00840: loss improved from 0.29685 to 0.28960, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 841/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3036 - accuracy: 0.8988\n",
      "\n",
      "Epoch 00841: loss did not improve from 0.28960\n",
      "Epoch 842/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2915 - accuracy: 0.9025\n",
      "\n",
      "Epoch 00842: loss did not improve from 0.28960\n",
      "Epoch 843/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2949 - accuracy: 0.9025\n",
      "\n",
      "Epoch 00843: loss did not improve from 0.28960\n",
      "Epoch 844/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3035 - accuracy: 0.9000\n",
      "\n",
      "Epoch 00844: loss did not improve from 0.28960\n",
      "Epoch 845/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2959 - accuracy: 0.8994\n",
      "\n",
      "Epoch 00845: loss did not improve from 0.28960\n",
      "Epoch 846/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3026 - accuracy: 0.8969\n",
      "\n",
      "Epoch 00846: loss did not improve from 0.28960\n",
      "Epoch 847/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2900 - accuracy: 0.9030\n",
      "\n",
      "Epoch 00847: loss did not improve from 0.28960\n",
      "Epoch 848/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3017 - accuracy: 0.8982\n",
      "\n",
      "Epoch 00848: loss did not improve from 0.28960\n",
      "Epoch 849/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3006 - accuracy: 0.8987\n",
      "\n",
      "Epoch 00849: loss did not improve from 0.28960\n",
      "Epoch 850/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2965 - accuracy: 0.9021\n",
      "\n",
      "Epoch 00850: loss did not improve from 0.28960\n",
      "Epoch 851/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2963 - accuracy: 0.9052\n",
      "\n",
      "Epoch 00851: loss did not improve from 0.28960\n",
      "Epoch 852/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2888 - accuracy: 0.9071\n",
      "\n",
      "Epoch 00852: loss did not improve from 0.28960\n",
      "Epoch 853/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3006 - accuracy: 0.8985\n",
      "\n",
      "Epoch 00853: loss did not improve from 0.28960\n",
      "Epoch 854/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2853 - accuracy: 0.9069\n",
      "\n",
      "Epoch 00854: loss did not improve from 0.28960\n",
      "Epoch 855/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2986 - accuracy: 0.9012\n",
      "\n",
      "Epoch 00855: loss did not improve from 0.28960\n",
      "Epoch 856/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2677 - accuracy: 0.9114\n",
      "\n",
      "Epoch 00856: loss improved from 0.28960 to 0.28959, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 857/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2688 - accuracy: 0.9078\n",
      "\n",
      "Epoch 00857: loss improved from 0.28959 to 0.28665, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 858/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2741 - accuracy: 0.9103\n",
      "\n",
      "Epoch 00858: loss did not improve from 0.28665\n",
      "Epoch 859/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2725 - accuracy: 0.9116\n",
      "\n",
      "Epoch 00859: loss did not improve from 0.28665\n",
      "Epoch 860/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2810 - accuracy: 0.9093\n",
      "\n",
      "Epoch 00860: loss improved from 0.28665 to 0.28490, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 861/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2793 - accuracy: 0.9051\n",
      "\n",
      "Epoch 00861: loss improved from 0.28490 to 0.28333, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 862/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.2807 - accuracy: 0.9037\n",
      "\n",
      "Epoch 00862: loss did not improve from 0.28333\n",
      "Epoch 863/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2877 - accuracy: 0.9012\n",
      "\n",
      "Epoch 00863: loss did not improve from 0.28333\n",
      "Epoch 864/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2859 - accuracy: 0.9047\n",
      "\n",
      "Epoch 00864: loss did not improve from 0.28333\n",
      "Epoch 865/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2797 - accuracy: 0.9068\n",
      "\n",
      "Epoch 00865: loss did not improve from 0.28333\n",
      "Epoch 866/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2996 - accuracy: 0.9014\n",
      "\n",
      "Epoch 00866: loss did not improve from 0.28333\n",
      "Epoch 867/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2759 - accuracy: 0.9037\n",
      "\n",
      "Epoch 00867: loss did not improve from 0.28333\n",
      "Epoch 868/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2738 - accuracy: 0.9092\n",
      "\n",
      "Epoch 00868: loss did not improve from 0.28333\n",
      "Epoch 869/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2645 - accuracy: 0.9141\n",
      "\n",
      "Epoch 00869: loss did not improve from 0.28333\n",
      "Epoch 870/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2832 - accuracy: 0.9048\n",
      "\n",
      "Epoch 00870: loss did not improve from 0.28333\n",
      "Epoch 871/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3177 - accuracy: 0.8950\n",
      "\n",
      "Epoch 00871: loss did not improve from 0.28333\n",
      "Epoch 872/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2903 - accuracy: 0.9046\n",
      "\n",
      "Epoch 00872: loss did not improve from 0.28333\n",
      "Epoch 873/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2846 - accuracy: 0.9042\n",
      "\n",
      "Epoch 00873: loss did not improve from 0.28333\n",
      "Epoch 874/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2973 - accuracy: 0.8963\n",
      "\n",
      "Epoch 00874: loss did not improve from 0.28333\n",
      "Epoch 875/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.2909 - accuracy: 0.9024\n",
      "\n",
      "Epoch 00875: loss did not improve from 0.28333\n",
      "Epoch 876/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3145 - accuracy: 0.8944\n",
      "\n",
      "Epoch 00876: loss did not improve from 0.28333\n",
      "Epoch 877/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2946 - accuracy: 0.9048\n",
      "\n",
      "Epoch 00877: loss did not improve from 0.28333\n",
      "Epoch 878/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3026 - accuracy: 0.8934\n",
      "\n",
      "Epoch 00878: loss did not improve from 0.28333\n",
      "Epoch 879/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2749 - accuracy: 0.9082\n",
      "\n",
      "Epoch 00879: loss did not improve from 0.28333\n",
      "Epoch 880/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2829 - accuracy: 0.9063\n",
      "\n",
      "Epoch 00880: loss improved from 0.28333 to 0.27855, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 881/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2776 - accuracy: 0.9110\n",
      "\n",
      "Epoch 00881: loss did not improve from 0.27855\n",
      "Epoch 882/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2637 - accuracy: 0.9105\n",
      "\n",
      "Epoch 00882: loss improved from 0.27855 to 0.27276, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 883/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2789 - accuracy: 0.9050\n",
      "\n",
      "Epoch 00883: loss did not improve from 0.27276\n",
      "Epoch 884/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2748 - accuracy: 0.9027\n",
      "\n",
      "Epoch 00884: loss did not improve from 0.27276\n",
      "Epoch 885/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2761 - accuracy: 0.9070\n",
      "\n",
      "Epoch 00885: loss did not improve from 0.27276\n",
      "Epoch 886/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.2686 - accuracy: 0.9148\n",
      "\n",
      "Epoch 00886: loss did not improve from 0.27276\n",
      "Epoch 887/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2783 - accuracy: 0.9006\n",
      "\n",
      "Epoch 00887: loss did not improve from 0.27276\n",
      "Epoch 888/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2961 - accuracy: 0.8982\n",
      "\n",
      "Epoch 00888: loss did not improve from 0.27276\n",
      "Epoch 889/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2755 - accuracy: 0.9036\n",
      "\n",
      "Epoch 00889: loss did not improve from 0.27276\n",
      "Epoch 890/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2762 - accuracy: 0.9052\n",
      "\n",
      "Epoch 00890: loss did not improve from 0.27276\n",
      "Epoch 891/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.2706 - accuracy: 0.9086\n",
      "\n",
      "Epoch 00891: loss did not improve from 0.27276\n",
      "Epoch 892/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2745 - accuracy: 0.9068\n",
      "\n",
      "Epoch 00892: loss did not improve from 0.27276\n",
      "Epoch 893/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2635 - accuracy: 0.9100\n",
      "\n",
      "Epoch 00893: loss did not improve from 0.27276\n",
      "Epoch 894/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2875 - accuracy: 0.9038\n",
      "\n",
      "Epoch 00894: loss did not improve from 0.27276\n",
      "Epoch 895/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2838 - accuracy: 0.9012\n",
      "\n",
      "Epoch 00895: loss did not improve from 0.27276\n",
      "Epoch 896/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2810 - accuracy: 0.9052\n",
      "\n",
      "Epoch 00896: loss did not improve from 0.27276\n",
      "Epoch 897/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2878 - accuracy: 0.9022\n",
      "\n",
      "Epoch 00897: loss did not improve from 0.27276\n",
      "Epoch 898/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2708 - accuracy: 0.9086\n",
      "\n",
      "Epoch 00898: loss did not improve from 0.27276\n",
      "Epoch 899/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2673 - accuracy: 0.9094\n",
      "\n",
      "Epoch 00899: loss did not improve from 0.27276\n",
      "Epoch 900/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2657 - accuracy: 0.9073\n",
      "\n",
      "Epoch 00900: loss improved from 0.27276 to 0.26516, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 901/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.2751 - accuracy: 0.9035\n",
      "\n",
      "Epoch 00901: loss improved from 0.26516 to 0.26219, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 902/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2587 - accuracy: 0.9116\n",
      "\n",
      "Epoch 00902: loss did not improve from 0.26219\n",
      "Epoch 903/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2619 - accuracy: 0.9087\n",
      "\n",
      "Epoch 00903: loss did not improve from 0.26219\n",
      "Epoch 904/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2620 - accuracy: 0.9113\n",
      "\n",
      "Epoch 00904: loss did not improve from 0.26219\n",
      "Epoch 905/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.2541 - accuracy: 0.9118\n",
      "\n",
      "Epoch 00905: loss did not improve from 0.26219\n",
      "Epoch 906/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2484 - accuracy: 0.9159\n",
      "\n",
      "Epoch 00906: loss did not improve from 0.26219\n",
      "Epoch 907/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2559 - accuracy: 0.9119\n",
      "\n",
      "Epoch 00907: loss did not improve from 0.26219\n",
      "Epoch 908/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2428 - accuracy: 0.9157\n",
      "\n",
      "Epoch 00908: loss did not improve from 0.26219\n",
      "Epoch 909/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2748 - accuracy: 0.9066\n",
      "\n",
      "Epoch 00909: loss did not improve from 0.26219\n",
      "Epoch 910/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.2684 - accuracy: 0.9084\n",
      "\n",
      "Epoch 00910: loss did not improve from 0.26219\n",
      "Epoch 911/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2605 - accuracy: 0.9104\n",
      "\n",
      "Epoch 00911: loss did not improve from 0.26219\n",
      "Epoch 912/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2687 - accuracy: 0.9079\n",
      "\n",
      "Epoch 00912: loss did not improve from 0.26219\n",
      "Epoch 913/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2606 - accuracy: 0.9098\n",
      "\n",
      "Epoch 00913: loss did not improve from 0.26219\n",
      "Epoch 914/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2697 - accuracy: 0.9051\n",
      "\n",
      "Epoch 00914: loss did not improve from 0.26219\n",
      "Epoch 915/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2622 - accuracy: 0.9100\n",
      "\n",
      "Epoch 00915: loss did not improve from 0.26219\n",
      "Epoch 916/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2673 - accuracy: 0.9070\n",
      "\n",
      "Epoch 00916: loss did not improve from 0.26219\n",
      "Epoch 917/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2573 - accuracy: 0.9142\n",
      "\n",
      "Epoch 00917: loss did not improve from 0.26219\n",
      "Epoch 918/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2766 - accuracy: 0.9020\n",
      "\n",
      "Epoch 00918: loss did not improve from 0.26219\n",
      "Epoch 919/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2500 - accuracy: 0.9107\n",
      "\n",
      "Epoch 00919: loss did not improve from 0.26219\n",
      "Epoch 920/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2657 - accuracy: 0.9107\n",
      "\n",
      "Epoch 00920: loss did not improve from 0.26219\n",
      "Epoch 921/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2684 - accuracy: 0.9082\n",
      "\n",
      "Epoch 00921: loss did not improve from 0.26219\n",
      "Epoch 922/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2722 - accuracy: 0.9078\n",
      "\n",
      "Epoch 00922: loss did not improve from 0.26219\n",
      "Epoch 923/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2790 - accuracy: 0.9047\n",
      "\n",
      "Epoch 00923: loss did not improve from 0.26219\n",
      "Epoch 924/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2669 - accuracy: 0.9107\n",
      "\n",
      "Epoch 00924: loss did not improve from 0.26219\n",
      "Epoch 925/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2892 - accuracy: 0.8990\n",
      "\n",
      "Epoch 00925: loss did not improve from 0.26219\n",
      "Epoch 926/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2870 - accuracy: 0.8965\n",
      "\n",
      "Epoch 00926: loss did not improve from 0.26219\n",
      "Epoch 927/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2731 - accuracy: 0.9058\n",
      "\n",
      "Epoch 00927: loss did not improve from 0.26219\n",
      "Epoch 928/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3070 - accuracy: 0.8898\n",
      "\n",
      "Epoch 00928: loss did not improve from 0.26219\n",
      "Epoch 929/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3191 - accuracy: 0.8875\n",
      "\n",
      "Epoch 00929: loss did not improve from 0.26219\n",
      "Epoch 930/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3054 - accuracy: 0.8952\n",
      "\n",
      "Epoch 00930: loss did not improve from 0.26219\n",
      "Epoch 931/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3436 - accuracy: 0.8848\n",
      "\n",
      "Epoch 00931: loss did not improve from 0.26219\n",
      "Epoch 932/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3533 - accuracy: 0.8718\n",
      "\n",
      "Epoch 00932: loss did not improve from 0.26219\n",
      "Epoch 933/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2926 - accuracy: 0.8976\n",
      "\n",
      "Epoch 00933: loss did not improve from 0.26219\n",
      "Epoch 934/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2662 - accuracy: 0.9106\n",
      "\n",
      "Epoch 00934: loss did not improve from 0.26219\n",
      "Epoch 935/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2588 - accuracy: 0.9108\n",
      "\n",
      "Epoch 00935: loss did not improve from 0.26219\n",
      "Epoch 936/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2667 - accuracy: 0.9073\n",
      "\n",
      "Epoch 00936: loss improved from 0.26219 to 0.26039, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 937/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2466 - accuracy: 0.9145\n",
      "\n",
      "Epoch 00937: loss improved from 0.26039 to 0.25528, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 938/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2399 - accuracy: 0.9167\n",
      "\n",
      "Epoch 00938: loss improved from 0.25528 to 0.25015, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 939/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2480 - accuracy: 0.9183\n",
      "\n",
      "Epoch 00939: loss improved from 0.25015 to 0.24830, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 940/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2626 - accuracy: 0.9111\n",
      "\n",
      "Epoch 00940: loss improved from 0.24830 to 0.24751, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 941/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2540 - accuracy: 0.9082\n",
      "\n",
      "Epoch 00941: loss did not improve from 0.24751\n",
      "Epoch 942/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2472 - accuracy: 0.9131\n",
      "\n",
      "Epoch 00942: loss improved from 0.24751 to 0.24645, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 943/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2381 - accuracy: 0.9153\n",
      "\n",
      "Epoch 00943: loss did not improve from 0.24645\n",
      "Epoch 944/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2495 - accuracy: 0.9074\n",
      "\n",
      "Epoch 00944: loss did not improve from 0.24645\n",
      "Epoch 945/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2475 - accuracy: 0.9087\n",
      "\n",
      "Epoch 00945: loss did not improve from 0.24645\n",
      "Epoch 946/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2420 - accuracy: 0.9109\n",
      "\n",
      "Epoch 00946: loss did not improve from 0.24645\n",
      "Epoch 947/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2526 - accuracy: 0.9127\n",
      "\n",
      "Epoch 00947: loss did not improve from 0.24645\n",
      "Epoch 948/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2508 - accuracy: 0.9115\n",
      "\n",
      "Epoch 00948: loss improved from 0.24645 to 0.24440, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 949/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2362 - accuracy: 0.9152\n",
      "\n",
      "Epoch 00949: loss improved from 0.24440 to 0.24341, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 950/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2443 - accuracy: 0.9113\n",
      "\n",
      "Epoch 00950: loss did not improve from 0.24341\n",
      "Epoch 951/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2470 - accuracy: 0.9120\n",
      "\n",
      "Epoch 00951: loss did not improve from 0.24341\n",
      "Epoch 952/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2308 - accuracy: 0.9150\n",
      "\n",
      "Epoch 00952: loss did not improve from 0.24341\n",
      "Epoch 953/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2506 - accuracy: 0.9108\n",
      "\n",
      "Epoch 00953: loss did not improve from 0.24341\n",
      "Epoch 954/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2664 - accuracy: 0.9022\n",
      "\n",
      "Epoch 00954: loss did not improve from 0.24341\n",
      "Epoch 955/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2515 - accuracy: 0.9103\n",
      "\n",
      "Epoch 00955: loss did not improve from 0.24341\n",
      "Epoch 956/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2498 - accuracy: 0.9113\n",
      "\n",
      "Epoch 00956: loss did not improve from 0.24341\n",
      "Epoch 957/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2503 - accuracy: 0.9101\n",
      "\n",
      "Epoch 00957: loss did not improve from 0.24341\n",
      "Epoch 958/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2424 - accuracy: 0.9137\n",
      "\n",
      "Epoch 00958: loss did not improve from 0.24341\n",
      "Epoch 959/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2492 - accuracy: 0.9065\n",
      "\n",
      "Epoch 00959: loss did not improve from 0.24341\n",
      "Epoch 960/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2427 - accuracy: 0.9133\n",
      "\n",
      "Epoch 00960: loss did not improve from 0.24341\n",
      "Epoch 961/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2382 - accuracy: 0.9125\n",
      "\n",
      "Epoch 00961: loss did not improve from 0.24341\n",
      "Epoch 962/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2429 - accuracy: 0.9126\n",
      "\n",
      "Epoch 00962: loss did not improve from 0.24341\n",
      "Epoch 963/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2987 - accuracy: 0.8954\n",
      "\n",
      "Epoch 00963: loss did not improve from 0.24341\n",
      "Epoch 964/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3179 - accuracy: 0.8877\n",
      "\n",
      "Epoch 00964: loss did not improve from 0.24341\n",
      "Epoch 965/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3267 - accuracy: 0.8873\n",
      "\n",
      "Epoch 00965: loss did not improve from 0.24341\n",
      "Epoch 966/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2956 - accuracy: 0.8986\n",
      "\n",
      "Epoch 00966: loss did not improve from 0.24341\n",
      "Epoch 967/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2783 - accuracy: 0.9030\n",
      "\n",
      "Epoch 00967: loss did not improve from 0.24341\n",
      "Epoch 968/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2592 - accuracy: 0.9066\n",
      "\n",
      "Epoch 00968: loss did not improve from 0.24341\n",
      "Epoch 969/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2477 - accuracy: 0.9100\n",
      "\n",
      "Epoch 00969: loss did not improve from 0.24341\n",
      "Epoch 970/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2467 - accuracy: 0.9054\n",
      "\n",
      "Epoch 00970: loss did not improve from 0.24341\n",
      "Epoch 971/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2484 - accuracy: 0.9089\n",
      "\n",
      "Epoch 00971: loss improved from 0.24341 to 0.24058, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 972/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2332 - accuracy: 0.9191\n",
      "\n",
      "Epoch 00972: loss improved from 0.24058 to 0.23793, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 973/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2309 - accuracy: 0.9195\n",
      "\n",
      "Epoch 00973: loss did not improve from 0.23793\n",
      "Epoch 974/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2219 - accuracy: 0.9203\n",
      "\n",
      "Epoch 00974: loss improved from 0.23793 to 0.23610, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 975/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2308 - accuracy: 0.9215\n",
      "\n",
      "Epoch 00975: loss did not improve from 0.23610\n",
      "Epoch 976/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2295 - accuracy: 0.9148\n",
      "\n",
      "Epoch 00976: loss did not improve from 0.23610\n",
      "Epoch 977/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2465 - accuracy: 0.9109\n",
      "\n",
      "Epoch 00977: loss improved from 0.23610 to 0.23524, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 978/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2214 - accuracy: 0.9216\n",
      "\n",
      "Epoch 00978: loss did not improve from 0.23524\n",
      "Epoch 979/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2251 - accuracy: 0.9208\n",
      "\n",
      "Epoch 00979: loss improved from 0.23524 to 0.23513, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 980/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2365 - accuracy: 0.9145\n",
      "\n",
      "Epoch 00980: loss did not improve from 0.23513\n",
      "Epoch 981/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2280 - accuracy: 0.9171\n",
      "\n",
      "Epoch 00981: loss did not improve from 0.23513\n",
      "Epoch 982/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2387 - accuracy: 0.9136\n",
      "\n",
      "Epoch 00982: loss did not improve from 0.23513\n",
      "Epoch 983/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2341 - accuracy: 0.9152\n",
      "\n",
      "Epoch 00983: loss did not improve from 0.23513\n",
      "Epoch 984/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2336 - accuracy: 0.9153\n",
      "\n",
      "Epoch 00984: loss did not improve from 0.23513\n",
      "Epoch 985/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2420 - accuracy: 0.9140\n",
      "\n",
      "Epoch 00985: loss did not improve from 0.23513\n",
      "Epoch 986/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2249 - accuracy: 0.9235\n",
      "\n",
      "Epoch 00986: loss improved from 0.23513 to 0.23194, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 987/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2183 - accuracy: 0.9182\n",
      "\n",
      "Epoch 00987: loss improved from 0.23194 to 0.23032, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 988/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2263 - accuracy: 0.9194\n",
      "\n",
      "Epoch 00988: loss did not improve from 0.23032\n",
      "Epoch 989/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2317 - accuracy: 0.9154\n",
      "\n",
      "Epoch 00989: loss did not improve from 0.23032\n",
      "Epoch 990/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2289 - accuracy: 0.9177\n",
      "\n",
      "Epoch 00990: loss did not improve from 0.23032\n",
      "Epoch 991/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2571 - accuracy: 0.9041\n",
      "\n",
      "Epoch 00991: loss did not improve from 0.23032\n",
      "Epoch 992/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2636 - accuracy: 0.9081\n",
      "\n",
      "Epoch 00992: loss did not improve from 0.23032\n",
      "Epoch 993/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2734 - accuracy: 0.9042\n",
      "\n",
      "Epoch 00993: loss did not improve from 0.23032\n",
      "Epoch 994/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2626 - accuracy: 0.9039\n",
      "\n",
      "Epoch 00994: loss did not improve from 0.23032\n",
      "Epoch 995/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2591 - accuracy: 0.9056\n",
      "\n",
      "Epoch 00995: loss did not improve from 0.23032\n",
      "Epoch 996/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2578 - accuracy: 0.9098\n",
      "\n",
      "Epoch 00996: loss did not improve from 0.23032\n",
      "Epoch 997/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2729 - accuracy: 0.9006\n",
      "\n",
      "Epoch 00997: loss did not improve from 0.23032\n",
      "Epoch 998/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2481 - accuracy: 0.9145\n",
      "\n",
      "Epoch 00998: loss did not improve from 0.23032\n",
      "Epoch 999/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2403 - accuracy: 0.9210\n",
      "\n",
      "Epoch 00999: loss did not improve from 0.23032\n",
      "Epoch 1000/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2610 - accuracy: 0.9088\n",
      "\n",
      "Epoch 01000: loss did not improve from 0.23032\n",
      "Epoch 1001/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2594 - accuracy: 0.9105\n",
      "\n",
      "Epoch 01001: loss did not improve from 0.23032\n",
      "Epoch 1002/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2482 - accuracy: 0.9122\n",
      "\n",
      "Epoch 01002: loss did not improve from 0.23032\n",
      "Epoch 1003/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2431 - accuracy: 0.9096\n",
      "\n",
      "Epoch 01003: loss did not improve from 0.23032\n",
      "Epoch 1004/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2422 - accuracy: 0.9157\n",
      "\n",
      "Epoch 01004: loss did not improve from 0.23032\n",
      "Epoch 1005/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2424 - accuracy: 0.9107\n",
      "\n",
      "Epoch 01005: loss did not improve from 0.23032\n",
      "Epoch 1006/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2276 - accuracy: 0.9173\n",
      "\n",
      "Epoch 01006: loss did not improve from 0.23032\n",
      "Epoch 1007/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2478 - accuracy: 0.9102\n",
      "\n",
      "Epoch 01007: loss did not improve from 0.23032\n",
      "Epoch 1008/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2489 - accuracy: 0.9125\n",
      "\n",
      "Epoch 01008: loss did not improve from 0.23032\n",
      "Epoch 1009/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2392 - accuracy: 0.9113\n",
      "\n",
      "Epoch 01009: loss did not improve from 0.23032\n",
      "Epoch 1010/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2332 - accuracy: 0.9181\n",
      "\n",
      "Epoch 01010: loss did not improve from 0.23032\n",
      "Epoch 1011/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2258 - accuracy: 0.9180\n",
      "\n",
      "Epoch 01011: loss did not improve from 0.23032\n",
      "Epoch 1012/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2286 - accuracy: 0.9149\n",
      "\n",
      "Epoch 01012: loss improved from 0.23032 to 0.22866, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1013/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2219 - accuracy: 0.9143\n",
      "\n",
      "Epoch 01013: loss improved from 0.22866 to 0.22772, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1014/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2203 - accuracy: 0.9143\n",
      "\n",
      "Epoch 01014: loss did not improve from 0.22772\n",
      "Epoch 1015/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2232 - accuracy: 0.9183\n",
      "\n",
      "Epoch 01015: loss improved from 0.22772 to 0.22740, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1016/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2189 - accuracy: 0.9197\n",
      "\n",
      "Epoch 01016: loss improved from 0.22740 to 0.22314, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1017/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2186 - accuracy: 0.9178\n",
      "\n",
      "Epoch 01017: loss did not improve from 0.22314\n",
      "Epoch 1018/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2220 - accuracy: 0.9159\n",
      "\n",
      "Epoch 01018: loss did not improve from 0.22314\n",
      "Epoch 1019/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2224 - accuracy: 0.9184\n",
      "\n",
      "Epoch 01019: loss did not improve from 0.22314\n",
      "Epoch 1020/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2202 - accuracy: 0.9164\n",
      "\n",
      "Epoch 01020: loss did not improve from 0.22314\n",
      "Epoch 1021/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2276 - accuracy: 0.9162\n",
      "\n",
      "Epoch 01021: loss did not improve from 0.22314\n",
      "Epoch 1022/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2243 - accuracy: 0.9161\n",
      "\n",
      "Epoch 01022: loss improved from 0.22314 to 0.22270, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1023/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2136 - accuracy: 0.9222\n",
      "\n",
      "Epoch 01023: loss improved from 0.22270 to 0.22235, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1024/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2264 - accuracy: 0.9171\n",
      "\n",
      "Epoch 01024: loss did not improve from 0.22235\n",
      "Epoch 1025/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2119 - accuracy: 0.9227\n",
      "\n",
      "Epoch 01025: loss did not improve from 0.22235\n",
      "Epoch 1026/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2304 - accuracy: 0.9125\n",
      "\n",
      "Epoch 01026: loss did not improve from 0.22235\n",
      "Epoch 1027/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2263 - accuracy: 0.9174\n",
      "\n",
      "Epoch 01027: loss did not improve from 0.22235\n",
      "Epoch 1028/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2264 - accuracy: 0.9180\n",
      "\n",
      "Epoch 01028: loss did not improve from 0.22235\n",
      "Epoch 1029/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2286 - accuracy: 0.9134\n",
      "\n",
      "Epoch 01029: loss did not improve from 0.22235\n",
      "Epoch 1030/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2218 - accuracy: 0.9148\n",
      "\n",
      "Epoch 01030: loss did not improve from 0.22235\n",
      "Epoch 1031/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2300 - accuracy: 0.9145\n",
      "\n",
      "Epoch 01031: loss did not improve from 0.22235\n",
      "Epoch 1032/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2448 - accuracy: 0.9146\n",
      "\n",
      "Epoch 01032: loss did not improve from 0.22235\n",
      "Epoch 1033/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2818 - accuracy: 0.8994\n",
      "\n",
      "Epoch 01033: loss did not improve from 0.22235\n",
      "Epoch 1034/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2754 - accuracy: 0.9010\n",
      "\n",
      "Epoch 01034: loss did not improve from 0.22235\n",
      "Epoch 1035/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2585 - accuracy: 0.9068\n",
      "\n",
      "Epoch 01035: loss did not improve from 0.22235\n",
      "Epoch 1036/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2480 - accuracy: 0.9085\n",
      "\n",
      "Epoch 01036: loss did not improve from 0.22235\n",
      "Epoch 1037/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2373 - accuracy: 0.9145\n",
      "\n",
      "Epoch 01037: loss did not improve from 0.22235\n",
      "Epoch 1038/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2329 - accuracy: 0.9127\n",
      "\n",
      "Epoch 01038: loss did not improve from 0.22235\n",
      "Epoch 1039/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2256 - accuracy: 0.9127\n",
      "\n",
      "Epoch 01039: loss did not improve from 0.22235\n",
      "Epoch 1040/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2055 - accuracy: 0.9243\n",
      "\n",
      "Epoch 01040: loss did not improve from 0.22235\n",
      "Epoch 1041/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2191 - accuracy: 0.9212\n",
      "\n",
      "Epoch 01041: loss did not improve from 0.22235\n",
      "Epoch 1042/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2206 - accuracy: 0.9214\n",
      "\n",
      "Epoch 01042: loss did not improve from 0.22235\n",
      "Epoch 1043/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2296 - accuracy: 0.9153\n",
      "\n",
      "Epoch 01043: loss did not improve from 0.22235\n",
      "Epoch 1044/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2105 - accuracy: 0.9217\n",
      "\n",
      "Epoch 01044: loss did not improve from 0.22235\n",
      "Epoch 1045/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2273 - accuracy: 0.9115\n",
      "\n",
      "Epoch 01045: loss did not improve from 0.22235\n",
      "Epoch 1046/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2358 - accuracy: 0.9131\n",
      "\n",
      "Epoch 01046: loss did not improve from 0.22235\n",
      "Epoch 1047/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2198 - accuracy: 0.9204\n",
      "\n",
      "Epoch 01047: loss did not improve from 0.22235\n",
      "Epoch 1048/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2308 - accuracy: 0.9110\n",
      "\n",
      "Epoch 01048: loss did not improve from 0.22235\n",
      "Epoch 1049/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2198 - accuracy: 0.9207\n",
      "\n",
      "Epoch 01049: loss did not improve from 0.22235\n",
      "Epoch 1050/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2209 - accuracy: 0.9188\n",
      "\n",
      "Epoch 01050: loss did not improve from 0.22235\n",
      "Epoch 1051/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2185 - accuracy: 0.9163\n",
      "\n",
      "Epoch 01051: loss did not improve from 0.22235\n",
      "Epoch 1052/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2202 - accuracy: 0.9165\n",
      "\n",
      "Epoch 01052: loss did not improve from 0.22235\n",
      "Epoch 1053/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2331 - accuracy: 0.9152\n",
      "\n",
      "Epoch 01053: loss did not improve from 0.22235\n",
      "Epoch 1054/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2421 - accuracy: 0.9155\n",
      "\n",
      "Epoch 01054: loss did not improve from 0.22235\n",
      "Epoch 1055/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2754 - accuracy: 0.9030\n",
      "\n",
      "Epoch 01055: loss did not improve from 0.22235\n",
      "Epoch 1056/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2813 - accuracy: 0.8992\n",
      "\n",
      "Epoch 01056: loss did not improve from 0.22235\n",
      "Epoch 1057/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2640 - accuracy: 0.9052\n",
      "\n",
      "Epoch 01057: loss did not improve from 0.22235\n",
      "Epoch 1058/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2592 - accuracy: 0.9085\n",
      "\n",
      "Epoch 01058: loss did not improve from 0.22235\n",
      "Epoch 1059/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2508 - accuracy: 0.9091\n",
      "\n",
      "Epoch 01059: loss did not improve from 0.22235\n",
      "Epoch 1060/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2453 - accuracy: 0.9106\n",
      "\n",
      "Epoch 01060: loss did not improve from 0.22235\n",
      "Epoch 1061/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2287 - accuracy: 0.9136\n",
      "\n",
      "Epoch 01061: loss did not improve from 0.22235\n",
      "Epoch 1062/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2298 - accuracy: 0.9128\n",
      "\n",
      "Epoch 01062: loss did not improve from 0.22235\n",
      "Epoch 1063/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2219 - accuracy: 0.9104\n",
      "\n",
      "Epoch 01063: loss improved from 0.22235 to 0.21780, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1064/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2111 - accuracy: 0.9207\n",
      "\n",
      "Epoch 01064: loss improved from 0.21780 to 0.21779, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1065/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2096 - accuracy: 0.9220\n",
      "\n",
      "Epoch 01065: loss improved from 0.21779 to 0.21574, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1066/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2025 - accuracy: 0.9187\n",
      "\n",
      "Epoch 01066: loss improved from 0.21574 to 0.21278, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1067/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2110 - accuracy: 0.9174\n",
      "\n",
      "Epoch 01067: loss did not improve from 0.21278\n",
      "Epoch 1068/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2030 - accuracy: 0.9200\n",
      "\n",
      "Epoch 01068: loss did not improve from 0.21278\n",
      "Epoch 1069/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2126 - accuracy: 0.9179\n",
      "\n",
      "Epoch 01069: loss did not improve from 0.21278\n",
      "Epoch 1070/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2208 - accuracy: 0.9212\n",
      "\n",
      "Epoch 01070: loss did not improve from 0.21278\n",
      "Epoch 1071/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2070 - accuracy: 0.9191\n",
      "\n",
      "Epoch 01071: loss did not improve from 0.21278\n",
      "Epoch 1072/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2225 - accuracy: 0.9129\n",
      "\n",
      "Epoch 01072: loss did not improve from 0.21278\n",
      "Epoch 1073/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2106 - accuracy: 0.9181\n",
      "\n",
      "Epoch 01073: loss improved from 0.21278 to 0.21277, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1074/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2138 - accuracy: 0.9159\n",
      "\n",
      "Epoch 01074: loss did not improve from 0.21277\n",
      "Epoch 1075/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.2057 - accuracy: 0.9205\n",
      "\n",
      "Epoch 01075: loss did not improve from 0.21277\n",
      "Epoch 1076/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.2102 - accuracy: 0.9186\n",
      "\n",
      "Epoch 01076: loss did not improve from 0.21277\n",
      "Epoch 1077/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2083 - accuracy: 0.9198\n",
      "\n",
      "Epoch 01077: loss did not improve from 0.21277\n",
      "Epoch 1078/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2058 - accuracy: 0.9234\n",
      "\n",
      "Epoch 01078: loss improved from 0.21277 to 0.20961, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1079/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2018 - accuracy: 0.9202\n",
      "\n",
      "Epoch 01079: loss improved from 0.20961 to 0.20907, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1080/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1964 - accuracy: 0.9248\n",
      "\n",
      "Epoch 01080: loss did not improve from 0.20907\n",
      "Epoch 1081/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2103 - accuracy: 0.9172\n",
      "\n",
      "Epoch 01081: loss did not improve from 0.20907\n",
      "Epoch 1082/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2110 - accuracy: 0.9155\n",
      "\n",
      "Epoch 01082: loss did not improve from 0.20907\n",
      "Epoch 1083/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2263 - accuracy: 0.9120\n",
      "\n",
      "Epoch 01083: loss did not improve from 0.20907\n",
      "Epoch 1084/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2471 - accuracy: 0.9104\n",
      "\n",
      "Epoch 01084: loss did not improve from 0.20907\n",
      "Epoch 1085/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2535 - accuracy: 0.9106\n",
      "\n",
      "Epoch 01085: loss did not improve from 0.20907\n",
      "Epoch 1086/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3134 - accuracy: 0.8894\n",
      "\n",
      "Epoch 01086: loss did not improve from 0.20907\n",
      "Epoch 1087/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3017 - accuracy: 0.8913\n",
      "\n",
      "Epoch 01087: loss did not improve from 0.20907\n",
      "Epoch 1088/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2597 - accuracy: 0.9059\n",
      "\n",
      "Epoch 01088: loss did not improve from 0.20907\n",
      "Epoch 1089/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2355 - accuracy: 0.9109\n",
      "\n",
      "Epoch 01089: loss did not improve from 0.20907\n",
      "Epoch 1090/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2420 - accuracy: 0.9050\n",
      "\n",
      "Epoch 01090: loss did not improve from 0.20907\n",
      "Epoch 1091/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2193 - accuracy: 0.9151\n",
      "\n",
      "Epoch 01091: loss did not improve from 0.20907\n",
      "Epoch 1092/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2082 - accuracy: 0.9191\n",
      "\n",
      "Epoch 01092: loss did not improve from 0.20907\n",
      "Epoch 1093/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2064 - accuracy: 0.9188\n",
      "\n",
      "Epoch 01093: loss did not improve from 0.20907\n",
      "Epoch 1094/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1997 - accuracy: 0.9219\n",
      "\n",
      "Epoch 01094: loss did not improve from 0.20907\n",
      "Epoch 1095/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2011 - accuracy: 0.9204\n",
      "\n",
      "Epoch 01095: loss improved from 0.20907 to 0.20890, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1096/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1977 - accuracy: 0.9232\n",
      "\n",
      "Epoch 01096: loss improved from 0.20890 to 0.20750, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1097/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2069 - accuracy: 0.9192\n",
      "\n",
      "Epoch 01097: loss improved from 0.20750 to 0.20479, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1098/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2070 - accuracy: 0.9175\n",
      "\n",
      "Epoch 01098: loss improved from 0.20479 to 0.20462, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1099/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1929 - accuracy: 0.9244\n",
      "\n",
      "Epoch 01099: loss improved from 0.20462 to 0.20392, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1100/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2003 - accuracy: 0.9241\n",
      "\n",
      "Epoch 01100: loss did not improve from 0.20392\n",
      "Epoch 1101/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2003 - accuracy: 0.9170\n",
      "\n",
      "Epoch 01101: loss did not improve from 0.20392\n",
      "Epoch 1102/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2008 - accuracy: 0.9205\n",
      "\n",
      "Epoch 01102: loss did not improve from 0.20392\n",
      "Epoch 1103/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2118 - accuracy: 0.9198\n",
      "\n",
      "Epoch 01103: loss did not improve from 0.20392\n",
      "Epoch 1104/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1952 - accuracy: 0.9295\n",
      "\n",
      "Epoch 01104: loss did not improve from 0.20392\n",
      "Epoch 1105/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2087 - accuracy: 0.9171\n",
      "\n",
      "Epoch 01105: loss did not improve from 0.20392\n",
      "Epoch 1106/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.2082 - accuracy: 0.9178\n",
      "\n",
      "Epoch 01106: loss did not improve from 0.20392\n",
      "Epoch 1107/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1960 - accuracy: 0.9229\n",
      "\n",
      "Epoch 01107: loss improved from 0.20392 to 0.20289, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1108/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2017 - accuracy: 0.9187\n",
      "\n",
      "Epoch 01108: loss improved from 0.20289 to 0.20156, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1109/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1963 - accuracy: 0.9231\n",
      "\n",
      "Epoch 01109: loss did not improve from 0.20156\n",
      "Epoch 1110/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1869 - accuracy: 0.9248\n",
      "\n",
      "Epoch 01110: loss improved from 0.20156 to 0.20130, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1111/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1935 - accuracy: 0.9236\n",
      "\n",
      "Epoch 01111: loss improved from 0.20130 to 0.20075, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1112/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1996 - accuracy: 0.9229\n",
      "\n",
      "Epoch 01112: loss did not improve from 0.20075\n",
      "Epoch 1113/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2052 - accuracy: 0.9202\n",
      "\n",
      "Epoch 01113: loss did not improve from 0.20075\n",
      "Epoch 1114/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2049 - accuracy: 0.9192\n",
      "\n",
      "Epoch 01114: loss did not improve from 0.20075\n",
      "Epoch 1115/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1954 - accuracy: 0.9257\n",
      "\n",
      "Epoch 01115: loss did not improve from 0.20075\n",
      "Epoch 1116/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2197 - accuracy: 0.9126\n",
      "\n",
      "Epoch 01116: loss did not improve from 0.20075\n",
      "Epoch 1117/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2184 - accuracy: 0.9122\n",
      "\n",
      "Epoch 01117: loss did not improve from 0.20075\n",
      "Epoch 1118/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2043 - accuracy: 0.9182\n",
      "\n",
      "Epoch 01118: loss did not improve from 0.20075\n",
      "Epoch 1119/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2041 - accuracy: 0.9252\n",
      "\n",
      "Epoch 01119: loss did not improve from 0.20075\n",
      "Epoch 1120/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2105 - accuracy: 0.9159\n",
      "\n",
      "Epoch 01120: loss did not improve from 0.20075\n",
      "Epoch 1121/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2088 - accuracy: 0.9202\n",
      "\n",
      "Epoch 01121: loss did not improve from 0.20075\n",
      "Epoch 1122/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2003 - accuracy: 0.9229\n",
      "\n",
      "Epoch 01122: loss did not improve from 0.20075\n",
      "Epoch 1123/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2145 - accuracy: 0.9162\n",
      "\n",
      "Epoch 01123: loss did not improve from 0.20075\n",
      "Epoch 1124/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2156 - accuracy: 0.9181\n",
      "\n",
      "Epoch 01124: loss did not improve from 0.20075\n",
      "Epoch 1125/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2344 - accuracy: 0.9106\n",
      "\n",
      "Epoch 01125: loss did not improve from 0.20075\n",
      "Epoch 1126/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2296 - accuracy: 0.9120\n",
      "\n",
      "Epoch 01126: loss did not improve from 0.20075\n",
      "Epoch 1127/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2405 - accuracy: 0.9057\n",
      "\n",
      "Epoch 01127: loss did not improve from 0.20075\n",
      "Epoch 1128/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3250 - accuracy: 0.8841\n",
      "\n",
      "Epoch 01128: loss did not improve from 0.20075\n",
      "Epoch 1129/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3005 - accuracy: 0.8992\n",
      "\n",
      "Epoch 01129: loss did not improve from 0.20075\n",
      "Epoch 1130/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2919 - accuracy: 0.8919\n",
      "\n",
      "Epoch 01130: loss did not improve from 0.20075\n",
      "Epoch 1131/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2588 - accuracy: 0.9062\n",
      "\n",
      "Epoch 01131: loss did not improve from 0.20075\n",
      "Epoch 1132/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2503 - accuracy: 0.9061\n",
      "\n",
      "Epoch 01132: loss did not improve from 0.20075\n",
      "Epoch 1133/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2376 - accuracy: 0.9112\n",
      "\n",
      "Epoch 01133: loss did not improve from 0.20075\n",
      "Epoch 1134/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2043 - accuracy: 0.9217\n",
      "\n",
      "Epoch 01134: loss did not improve from 0.20075\n",
      "Epoch 1135/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2017 - accuracy: 0.9190\n",
      "\n",
      "Epoch 01135: loss did not improve from 0.20075\n",
      "Epoch 1136/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1968 - accuracy: 0.9239\n",
      "\n",
      "Epoch 01136: loss did not improve from 0.20075\n",
      "Epoch 1137/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2002 - accuracy: 0.9225\n",
      "\n",
      "Epoch 01137: loss did not improve from 0.20075\n",
      "Epoch 1138/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1977 - accuracy: 0.9187\n",
      "\n",
      "Epoch 01138: loss did not improve from 0.20075\n",
      "Epoch 1139/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1942 - accuracy: 0.9207\n",
      "\n",
      "Epoch 01139: loss improved from 0.20075 to 0.20013, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1140/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2032 - accuracy: 0.9243\n",
      "\n",
      "Epoch 01140: loss did not improve from 0.20013\n",
      "Epoch 1141/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2012 - accuracy: 0.9173\n",
      "\n",
      "Epoch 01141: loss improved from 0.20013 to 0.19999, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1142/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2055 - accuracy: 0.9200\n",
      "\n",
      "Epoch 01142: loss improved from 0.19999 to 0.19653, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1143/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1995 - accuracy: 0.9161\n",
      "\n",
      "Epoch 01143: loss did not improve from 0.19653\n",
      "Epoch 1144/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1980 - accuracy: 0.9186\n",
      "\n",
      "Epoch 01144: loss did not improve from 0.19653\n",
      "Epoch 1145/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1866 - accuracy: 0.9292\n",
      "\n",
      "Epoch 01145: loss did not improve from 0.19653\n",
      "Epoch 1146/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1866 - accuracy: 0.9262\n",
      "\n",
      "Epoch 01146: loss did not improve from 0.19653\n",
      "Epoch 1147/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1858 - accuracy: 0.9296\n",
      "\n",
      "Epoch 01147: loss did not improve from 0.19653\n",
      "Epoch 1148/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1939 - accuracy: 0.9234\n",
      "\n",
      "Epoch 01148: loss improved from 0.19653 to 0.19510, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1149/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1899 - accuracy: 0.9198\n",
      "\n",
      "Epoch 01149: loss improved from 0.19510 to 0.19467, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1150/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1852 - accuracy: 0.9264\n",
      "\n",
      "Epoch 01150: loss did not improve from 0.19467\n",
      "Epoch 1151/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1942 - accuracy: 0.9213\n",
      "\n",
      "Epoch 01151: loss did not improve from 0.19467\n",
      "Epoch 1152/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1989 - accuracy: 0.9215\n",
      "\n",
      "Epoch 01152: loss improved from 0.19467 to 0.19455, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1153/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1949 - accuracy: 0.9197\n",
      "\n",
      "Epoch 01153: loss improved from 0.19455 to 0.19378, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1154/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1867 - accuracy: 0.9223\n",
      "\n",
      "Epoch 01154: loss did not improve from 0.19378\n",
      "Epoch 1155/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1849 - accuracy: 0.9221\n",
      "\n",
      "Epoch 01155: loss improved from 0.19378 to 0.19133, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1156/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1903 - accuracy: 0.9201\n",
      "\n",
      "Epoch 01156: loss did not improve from 0.19133\n",
      "Epoch 1157/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1826 - accuracy: 0.9274\n",
      "\n",
      "Epoch 01157: loss did not improve from 0.19133\n",
      "Epoch 1158/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2257 - accuracy: 0.9105\n",
      "\n",
      "Epoch 01158: loss did not improve from 0.19133\n",
      "Epoch 1159/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2186 - accuracy: 0.9129\n",
      "\n",
      "Epoch 01159: loss did not improve from 0.19133\n",
      "Epoch 1160/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2466 - accuracy: 0.9038\n",
      "\n",
      "Epoch 01160: loss did not improve from 0.19133\n",
      "Epoch 1161/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2393 - accuracy: 0.9079\n",
      "\n",
      "Epoch 01161: loss did not improve from 0.19133\n",
      "Epoch 1162/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2010 - accuracy: 0.9207\n",
      "\n",
      "Epoch 01162: loss did not improve from 0.19133\n",
      "Epoch 1163/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2124 - accuracy: 0.9166\n",
      "\n",
      "Epoch 01163: loss did not improve from 0.19133\n",
      "Epoch 1164/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2051 - accuracy: 0.9219\n",
      "\n",
      "Epoch 01164: loss did not improve from 0.19133\n",
      "Epoch 1165/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2010 - accuracy: 0.9204\n",
      "\n",
      "Epoch 01165: loss did not improve from 0.19133\n",
      "Epoch 1166/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1990 - accuracy: 0.9180\n",
      "\n",
      "Epoch 01166: loss did not improve from 0.19133\n",
      "Epoch 1167/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1857 - accuracy: 0.9294\n",
      "\n",
      "Epoch 01167: loss did not improve from 0.19133\n",
      "Epoch 1168/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2015 - accuracy: 0.9172\n",
      "\n",
      "Epoch 01168: loss did not improve from 0.19133\n",
      "Epoch 1169/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1923 - accuracy: 0.9268\n",
      "\n",
      "Epoch 01169: loss did not improve from 0.19133\n",
      "Epoch 1170/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1844 - accuracy: 0.9269\n",
      "\n",
      "Epoch 01170: loss did not improve from 0.19133\n",
      "Epoch 1171/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1959 - accuracy: 0.9262\n",
      "\n",
      "Epoch 01171: loss did not improve from 0.19133\n",
      "Epoch 1172/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2061 - accuracy: 0.9192\n",
      "\n",
      "Epoch 01172: loss did not improve from 0.19133\n",
      "Epoch 1173/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2177 - accuracy: 0.9149\n",
      "\n",
      "Epoch 01173: loss did not improve from 0.19133\n",
      "Epoch 1174/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2105 - accuracy: 0.9160\n",
      "\n",
      "Epoch 01174: loss did not improve from 0.19133\n",
      "Epoch 1175/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2178 - accuracy: 0.9184\n",
      "\n",
      "Epoch 01175: loss did not improve from 0.19133\n",
      "Epoch 1176/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2065 - accuracy: 0.9174\n",
      "\n",
      "Epoch 01176: loss did not improve from 0.19133\n",
      "Epoch 1177/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1982 - accuracy: 0.9199\n",
      "\n",
      "Epoch 01177: loss did not improve from 0.19133\n",
      "Epoch 1178/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1864 - accuracy: 0.9205\n",
      "\n",
      "Epoch 01178: loss did not improve from 0.19133\n",
      "Epoch 1179/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1913 - accuracy: 0.9195\n",
      "\n",
      "Epoch 01179: loss did not improve from 0.19133\n",
      "Epoch 1180/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1815 - accuracy: 0.9249\n",
      "\n",
      "Epoch 01180: loss improved from 0.19133 to 0.18815, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1181/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1848 - accuracy: 0.9238\n",
      "\n",
      "Epoch 01181: loss did not improve from 0.18815\n",
      "Epoch 1182/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1808 - accuracy: 0.9258\n",
      "\n",
      "Epoch 01182: loss did not improve from 0.18815\n",
      "Epoch 1183/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1796 - accuracy: 0.9291\n",
      "\n",
      "Epoch 01183: loss did not improve from 0.18815\n",
      "Epoch 1184/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1957 - accuracy: 0.9184\n",
      "\n",
      "Epoch 01184: loss did not improve from 0.18815\n",
      "Epoch 1185/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1977 - accuracy: 0.9202\n",
      "\n",
      "Epoch 01185: loss did not improve from 0.18815\n",
      "Epoch 1186/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2064 - accuracy: 0.9179\n",
      "\n",
      "Epoch 01186: loss did not improve from 0.18815\n",
      "Epoch 1187/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1952 - accuracy: 0.9222\n",
      "\n",
      "Epoch 01187: loss did not improve from 0.18815\n",
      "Epoch 1188/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1980 - accuracy: 0.9221\n",
      "\n",
      "Epoch 01188: loss did not improve from 0.18815\n",
      "Epoch 1189/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1947 - accuracy: 0.9193\n",
      "\n",
      "Epoch 01189: loss did not improve from 0.18815\n",
      "Epoch 1190/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1977 - accuracy: 0.9186\n",
      "\n",
      "Epoch 01190: loss did not improve from 0.18815\n",
      "Epoch 1191/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1931 - accuracy: 0.9214\n",
      "\n",
      "Epoch 01191: loss did not improve from 0.18815\n",
      "Epoch 1192/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2107 - accuracy: 0.9155\n",
      "\n",
      "Epoch 01192: loss did not improve from 0.18815\n",
      "Epoch 1193/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1915 - accuracy: 0.9261\n",
      "\n",
      "Epoch 01193: loss did not improve from 0.18815\n",
      "Epoch 1194/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1870 - accuracy: 0.9290\n",
      "\n",
      "Epoch 01194: loss did not improve from 0.18815\n",
      "Epoch 1195/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2013 - accuracy: 0.9168\n",
      "\n",
      "Epoch 01195: loss did not improve from 0.18815\n",
      "Epoch 1196/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2128 - accuracy: 0.9159\n",
      "\n",
      "Epoch 01196: loss did not improve from 0.18815\n",
      "Epoch 1197/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2040 - accuracy: 0.9157\n",
      "\n",
      "Epoch 01197: loss did not improve from 0.18815\n",
      "Epoch 1198/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1954 - accuracy: 0.9175\n",
      "\n",
      "Epoch 01198: loss did not improve from 0.18815\n",
      "Epoch 1199/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1917 - accuracy: 0.9261\n",
      "\n",
      "Epoch 01199: loss did not improve from 0.18815\n",
      "Epoch 1200/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1918 - accuracy: 0.9176\n",
      "\n",
      "Epoch 01200: loss did not improve from 0.18815\n",
      "Epoch 1201/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1767 - accuracy: 0.9292\n",
      "\n",
      "Epoch 01201: loss did not improve from 0.18815\n",
      "Epoch 1202/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1867 - accuracy: 0.9292\n",
      "\n",
      "Epoch 01202: loss did not improve from 0.18815\n",
      "Epoch 1203/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2127 - accuracy: 0.9144\n",
      "\n",
      "Epoch 01203: loss did not improve from 0.18815\n",
      "Epoch 1204/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2504 - accuracy: 0.9087\n",
      "\n",
      "Epoch 01204: loss did not improve from 0.18815\n",
      "Epoch 1205/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2908 - accuracy: 0.8985\n",
      "\n",
      "Epoch 01205: loss did not improve from 0.18815\n",
      "Epoch 1206/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3048 - accuracy: 0.8927\n",
      "\n",
      "Epoch 01206: loss did not improve from 0.18815\n",
      "Epoch 1207/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3696 - accuracy: 0.8825\n",
      "\n",
      "Epoch 01207: loss did not improve from 0.18815\n",
      "Epoch 1208/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3016 - accuracy: 0.8881\n",
      "\n",
      "Epoch 01208: loss did not improve from 0.18815\n",
      "Epoch 1209/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2383 - accuracy: 0.9072\n",
      "\n",
      "Epoch 01209: loss did not improve from 0.18815\n",
      "Epoch 1210/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2038 - accuracy: 0.9228\n",
      "\n",
      "Epoch 01210: loss did not improve from 0.18815\n",
      "Epoch 1211/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1948 - accuracy: 0.9257\n",
      "\n",
      "Epoch 01211: loss did not improve from 0.18815\n",
      "Epoch 1212/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1845 - accuracy: 0.9281\n",
      "\n",
      "Epoch 01212: loss did not improve from 0.18815\n",
      "Epoch 1213/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1835 - accuracy: 0.9251\n",
      "\n",
      "Epoch 01213: loss did not improve from 0.18815\n",
      "Epoch 1214/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1882 - accuracy: 0.9213\n",
      "\n",
      "Epoch 01214: loss improved from 0.18815 to 0.18779, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1215/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1806 - accuracy: 0.9248\n",
      "\n",
      "Epoch 01215: loss improved from 0.18779 to 0.18525, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1216/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1745 - accuracy: 0.9311\n",
      "\n",
      "Epoch 01216: loss improved from 0.18525 to 0.18328, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1217/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1763 - accuracy: 0.9242\n",
      "\n",
      "Epoch 01217: loss did not improve from 0.18328\n",
      "Epoch 1218/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1783 - accuracy: 0.9224\n",
      "\n",
      "Epoch 01218: loss improved from 0.18328 to 0.18121, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1219/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1691 - accuracy: 0.9294\n",
      "\n",
      "Epoch 01219: loss improved from 0.18121 to 0.18061, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1220/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1785 - accuracy: 0.9210\n",
      "\n",
      "Epoch 01220: loss did not improve from 0.18061\n",
      "Epoch 1221/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1742 - accuracy: 0.9305\n",
      "\n",
      "Epoch 01221: loss did not improve from 0.18061\n",
      "Epoch 1222/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1729 - accuracy: 0.9302\n",
      "\n",
      "Epoch 01222: loss did not improve from 0.18061\n",
      "Epoch 1223/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1765 - accuracy: 0.9285\n",
      "\n",
      "Epoch 01223: loss did not improve from 0.18061\n",
      "Epoch 1224/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1803 - accuracy: 0.9228\n",
      "\n",
      "Epoch 01224: loss did not improve from 0.18061\n",
      "Epoch 1225/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1797 - accuracy: 0.9261\n",
      "\n",
      "Epoch 01225: loss did not improve from 0.18061\n",
      "Epoch 1226/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1657 - accuracy: 0.9289\n",
      "\n",
      "Epoch 01226: loss improved from 0.18061 to 0.17826, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1227/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1810 - accuracy: 0.9217\n",
      "\n",
      "Epoch 01227: loss did not improve from 0.17826\n",
      "Epoch 1228/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1785 - accuracy: 0.9234\n",
      "\n",
      "Epoch 01228: loss did not improve from 0.17826\n",
      "Epoch 1229/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1721 - accuracy: 0.9270\n",
      "\n",
      "Epoch 01229: loss did not improve from 0.17826\n",
      "Epoch 1230/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1812 - accuracy: 0.9243\n",
      "\n",
      "Epoch 01230: loss did not improve from 0.17826\n",
      "Epoch 1231/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1917 - accuracy: 0.9175\n",
      "\n",
      "Epoch 01231: loss did not improve from 0.17826\n",
      "Epoch 1232/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1910 - accuracy: 0.9168\n",
      "\n",
      "Epoch 01232: loss did not improve from 0.17826\n",
      "Epoch 1233/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1817 - accuracy: 0.9290\n",
      "\n",
      "Epoch 01233: loss did not improve from 0.17826\n",
      "Epoch 1234/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1963 - accuracy: 0.9202\n",
      "\n",
      "Epoch 01234: loss did not improve from 0.17826\n",
      "Epoch 1235/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1955 - accuracy: 0.9233\n",
      "\n",
      "Epoch 01235: loss did not improve from 0.17826\n",
      "Epoch 1236/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2091 - accuracy: 0.9163\n",
      "\n",
      "Epoch 01236: loss did not improve from 0.17826\n",
      "Epoch 1237/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2046 - accuracy: 0.9159\n",
      "\n",
      "Epoch 01237: loss did not improve from 0.17826\n",
      "Epoch 1238/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2491 - accuracy: 0.9108\n",
      "\n",
      "Epoch 01238: loss did not improve from 0.17826\n",
      "Epoch 1239/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2929 - accuracy: 0.8960\n",
      "\n",
      "Epoch 01239: loss did not improve from 0.17826\n",
      "Epoch 1240/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3047 - accuracy: 0.8912\n",
      "\n",
      "Epoch 01240: loss did not improve from 0.17826\n",
      "Epoch 1241/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2707 - accuracy: 0.8988\n",
      "\n",
      "Epoch 01241: loss did not improve from 0.17826\n",
      "Epoch 1242/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2427 - accuracy: 0.9116\n",
      "\n",
      "Epoch 01242: loss did not improve from 0.17826\n",
      "Epoch 1243/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2311 - accuracy: 0.9140\n",
      "\n",
      "Epoch 01243: loss did not improve from 0.17826\n",
      "Epoch 1244/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1994 - accuracy: 0.9202\n",
      "\n",
      "Epoch 01244: loss did not improve from 0.17826\n",
      "Epoch 1245/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1945 - accuracy: 0.9217\n",
      "\n",
      "Epoch 01245: loss did not improve from 0.17826\n",
      "Epoch 1246/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1688 - accuracy: 0.9287\n",
      "\n",
      "Epoch 01246: loss did not improve from 0.17826\n",
      "Epoch 1247/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1804 - accuracy: 0.9247\n",
      "\n",
      "Epoch 01247: loss did not improve from 0.17826\n",
      "Epoch 1248/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1780 - accuracy: 0.9238\n",
      "\n",
      "Epoch 01248: loss did not improve from 0.17826\n",
      "Epoch 1249/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1640 - accuracy: 0.9342\n",
      "\n",
      "Epoch 01249: loss did not improve from 0.17826\n",
      "Epoch 1250/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1662 - accuracy: 0.9328\n",
      "\n",
      "Epoch 01250: loss did not improve from 0.17826\n",
      "Epoch 1251/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1811 - accuracy: 0.9207\n",
      "\n",
      "Epoch 01251: loss improved from 0.17826 to 0.17777, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1252/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1684 - accuracy: 0.9251\n",
      "\n",
      "Epoch 01252: loss improved from 0.17777 to 0.17678, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1253/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1628 - accuracy: 0.9290\n",
      "\n",
      "Epoch 01253: loss improved from 0.17678 to 0.17447, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1254/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1697 - accuracy: 0.9241\n",
      "\n",
      "Epoch 01254: loss did not improve from 0.17447\n",
      "Epoch 1255/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1705 - accuracy: 0.9312\n",
      "\n",
      "Epoch 01255: loss did not improve from 0.17447\n",
      "Epoch 1256/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1703 - accuracy: 0.9286\n",
      "\n",
      "Epoch 01256: loss did not improve from 0.17447\n",
      "Epoch 1257/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1704 - accuracy: 0.9307\n",
      "\n",
      "Epoch 01257: loss did not improve from 0.17447\n",
      "Epoch 1258/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1792 - accuracy: 0.9201\n",
      "\n",
      "Epoch 01258: loss did not improve from 0.17447\n",
      "Epoch 1259/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1841 - accuracy: 0.9198\n",
      "\n",
      "Epoch 01259: loss did not improve from 0.17447\n",
      "Epoch 1260/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1690 - accuracy: 0.9244\n",
      "\n",
      "Epoch 01260: loss did not improve from 0.17447\n",
      "Epoch 1261/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1709 - accuracy: 0.9270\n",
      "\n",
      "Epoch 01261: loss did not improve from 0.17447\n",
      "Epoch 1262/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1840 - accuracy: 0.9194\n",
      "\n",
      "Epoch 01262: loss did not improve from 0.17447\n",
      "Epoch 1263/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1730 - accuracy: 0.9251\n",
      "\n",
      "Epoch 01263: loss did not improve from 0.17447\n",
      "Epoch 1264/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1708 - accuracy: 0.9266\n",
      "\n",
      "Epoch 01264: loss did not improve from 0.17447\n",
      "Epoch 1265/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1827 - accuracy: 0.9220\n",
      "\n",
      "Epoch 01265: loss did not improve from 0.17447\n",
      "Epoch 1266/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1808 - accuracy: 0.9313\n",
      "\n",
      "Epoch 01266: loss did not improve from 0.17447\n",
      "Epoch 1267/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1652 - accuracy: 0.9325\n",
      "\n",
      "Epoch 01267: loss did not improve from 0.17447\n",
      "Epoch 1268/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1871 - accuracy: 0.9204\n",
      "\n",
      "Epoch 01268: loss did not improve from 0.17447\n",
      "Epoch 1269/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1701 - accuracy: 0.9253\n",
      "\n",
      "Epoch 01269: loss did not improve from 0.17447\n",
      "Epoch 1270/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1692 - accuracy: 0.9252\n",
      "\n",
      "Epoch 01270: loss did not improve from 0.17447\n",
      "Epoch 1271/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1731 - accuracy: 0.9262\n",
      "\n",
      "Epoch 01271: loss did not improve from 0.17447\n",
      "Epoch 1272/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1817 - accuracy: 0.9218\n",
      "\n",
      "Epoch 01272: loss did not improve from 0.17447\n",
      "Epoch 1273/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1829 - accuracy: 0.9216\n",
      "\n",
      "Epoch 01273: loss did not improve from 0.17447\n",
      "Epoch 1274/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1783 - accuracy: 0.9219\n",
      "\n",
      "Epoch 01274: loss did not improve from 0.17447\n",
      "Epoch 1275/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1756 - accuracy: 0.9256\n",
      "\n",
      "Epoch 01275: loss did not improve from 0.17447\n",
      "Epoch 1276/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1733 - accuracy: 0.9288\n",
      "\n",
      "Epoch 01276: loss did not improve from 0.17447\n",
      "Epoch 1277/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1715 - accuracy: 0.9288\n",
      "\n",
      "Epoch 01277: loss did not improve from 0.17447\n",
      "Epoch 1278/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1772 - accuracy: 0.9269\n",
      "\n",
      "Epoch 01278: loss did not improve from 0.17447\n",
      "Epoch 1279/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1753 - accuracy: 0.9316\n",
      "\n",
      "Epoch 01279: loss did not improve from 0.17447\n",
      "Epoch 1280/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1813 - accuracy: 0.9236\n",
      "\n",
      "Epoch 01280: loss did not improve from 0.17447\n",
      "Epoch 1281/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1978 - accuracy: 0.9192\n",
      "\n",
      "Epoch 01281: loss did not improve from 0.17447\n",
      "Epoch 1282/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1986 - accuracy: 0.9197\n",
      "\n",
      "Epoch 01282: loss did not improve from 0.17447\n",
      "Epoch 1283/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1890 - accuracy: 0.9285\n",
      "\n",
      "Epoch 01283: loss did not improve from 0.17447\n",
      "Epoch 1284/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1939 - accuracy: 0.9236\n",
      "\n",
      "Epoch 01284: loss did not improve from 0.17447\n",
      "Epoch 1285/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2117 - accuracy: 0.9201\n",
      "\n",
      "Epoch 01285: loss did not improve from 0.17447\n",
      "Epoch 1286/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2063 - accuracy: 0.9155\n",
      "\n",
      "Epoch 01286: loss did not improve from 0.17447\n",
      "Epoch 1287/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1982 - accuracy: 0.9212\n",
      "\n",
      "Epoch 01287: loss did not improve from 0.17447\n",
      "Epoch 1288/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1890 - accuracy: 0.9266\n",
      "\n",
      "Epoch 01288: loss did not improve from 0.17447\n",
      "Epoch 1289/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1818 - accuracy: 0.9250\n",
      "\n",
      "Epoch 01289: loss did not improve from 0.17447\n",
      "Epoch 1290/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1825 - accuracy: 0.9261\n",
      "\n",
      "Epoch 01290: loss did not improve from 0.17447\n",
      "Epoch 1291/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1704 - accuracy: 0.9332\n",
      "\n",
      "Epoch 01291: loss did not improve from 0.17447\n",
      "Epoch 1292/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1734 - accuracy: 0.9268\n",
      "\n",
      "Epoch 01292: loss did not improve from 0.17447\n",
      "Epoch 1293/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1824 - accuracy: 0.9201\n",
      "\n",
      "Epoch 01293: loss did not improve from 0.17447\n",
      "Epoch 1294/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1692 - accuracy: 0.9286\n",
      "\n",
      "Epoch 01294: loss improved from 0.17447 to 0.17355, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1295/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1616 - accuracy: 0.9295\n",
      "\n",
      "Epoch 01295: loss did not improve from 0.17355\n",
      "Epoch 1296/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1682 - accuracy: 0.9259\n",
      "\n",
      "Epoch 01296: loss did not improve from 0.17355\n",
      "Epoch 1297/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1748 - accuracy: 0.9259\n",
      "\n",
      "Epoch 01297: loss did not improve from 0.17355\n",
      "Epoch 1298/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1707 - accuracy: 0.9223\n",
      "\n",
      "Epoch 01298: loss did not improve from 0.17355\n",
      "Epoch 1299/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1705 - accuracy: 0.9234\n",
      "\n",
      "Epoch 01299: loss improved from 0.17355 to 0.17278, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1300/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1661 - accuracy: 0.9318\n",
      "\n",
      "Epoch 01300: loss improved from 0.17278 to 0.17253, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1301/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1749 - accuracy: 0.9200\n",
      "\n",
      "Epoch 01301: loss did not improve from 0.17253\n",
      "Epoch 1302/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1659 - accuracy: 0.9292\n",
      "\n",
      "Epoch 01302: loss did not improve from 0.17253\n",
      "Epoch 1303/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1695 - accuracy: 0.9273\n",
      "\n",
      "Epoch 01303: loss did not improve from 0.17253\n",
      "Epoch 1304/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1793 - accuracy: 0.9230\n",
      "\n",
      "Epoch 01304: loss did not improve from 0.17253\n",
      "Epoch 1305/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1675 - accuracy: 0.9283\n",
      "\n",
      "Epoch 01305: loss did not improve from 0.17253\n",
      "Epoch 1306/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1711 - accuracy: 0.9294\n",
      "\n",
      "Epoch 01306: loss did not improve from 0.17253\n",
      "Epoch 1307/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1793 - accuracy: 0.9230\n",
      "\n",
      "Epoch 01307: loss did not improve from 0.17253\n",
      "Epoch 1308/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1732 - accuracy: 0.9291\n",
      "\n",
      "Epoch 01308: loss did not improve from 0.17253\n",
      "Epoch 1309/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1886 - accuracy: 0.9242\n",
      "\n",
      "Epoch 01309: loss did not improve from 0.17253\n",
      "Epoch 1310/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1910 - accuracy: 0.9249\n",
      "\n",
      "Epoch 01310: loss did not improve from 0.17253\n",
      "Epoch 1311/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2075 - accuracy: 0.9194\n",
      "\n",
      "Epoch 01311: loss did not improve from 0.17253\n",
      "Epoch 1312/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2368 - accuracy: 0.9098\n",
      "\n",
      "Epoch 01312: loss did not improve from 0.17253\n",
      "Epoch 1313/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3085 - accuracy: 0.8892\n",
      "\n",
      "Epoch 01313: loss did not improve from 0.17253\n",
      "Epoch 1314/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2805 - accuracy: 0.8960\n",
      "\n",
      "Epoch 01314: loss did not improve from 0.17253\n",
      "Epoch 1315/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2969 - accuracy: 0.8948\n",
      "\n",
      "Epoch 01315: loss did not improve from 0.17253\n",
      "Epoch 1316/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3281 - accuracy: 0.8822\n",
      "\n",
      "Epoch 01316: loss did not improve from 0.17253\n",
      "Epoch 1317/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2582 - accuracy: 0.9027\n",
      "\n",
      "Epoch 01317: loss did not improve from 0.17253\n",
      "Epoch 1318/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2062 - accuracy: 0.9178\n",
      "\n",
      "Epoch 01318: loss did not improve from 0.17253\n",
      "Epoch 1319/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1822 - accuracy: 0.9266\n",
      "\n",
      "Epoch 01319: loss did not improve from 0.17253\n",
      "Epoch 1320/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1808 - accuracy: 0.9258\n",
      "\n",
      "Epoch 01320: loss did not improve from 0.17253\n",
      "Epoch 1321/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1755 - accuracy: 0.9242\n",
      "\n",
      "Epoch 01321: loss did not improve from 0.17253\n",
      "Epoch 1322/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1669 - accuracy: 0.9267\n",
      "\n",
      "Epoch 01322: loss did not improve from 0.17253\n",
      "Epoch 1323/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1664 - accuracy: 0.9265\n",
      "\n",
      "Epoch 01323: loss improved from 0.17253 to 0.17226, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1324/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1677 - accuracy: 0.9306\n",
      "\n",
      "Epoch 01324: loss improved from 0.17226 to 0.16943, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1325/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1591 - accuracy: 0.9318\n",
      "\n",
      "Epoch 01325: loss improved from 0.16943 to 0.16895, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1326/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1585 - accuracy: 0.9296\n",
      "\n",
      "Epoch 01326: loss did not improve from 0.16895\n",
      "Epoch 1327/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1669 - accuracy: 0.9276\n",
      "\n",
      "Epoch 01327: loss did not improve from 0.16895\n",
      "Epoch 1328/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1699 - accuracy: 0.9272\n",
      "\n",
      "Epoch 01328: loss did not improve from 0.16895\n",
      "Epoch 1329/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1665 - accuracy: 0.9250\n",
      "\n",
      "Epoch 01329: loss improved from 0.16895 to 0.16775, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1330/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1534 - accuracy: 0.9349\n",
      "\n",
      "Epoch 01330: loss improved from 0.16775 to 0.16657, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1331/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1625 - accuracy: 0.9258\n",
      "\n",
      "Epoch 01331: loss did not improve from 0.16657\n",
      "Epoch 1332/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1656 - accuracy: 0.9276\n",
      "\n",
      "Epoch 01332: loss did not improve from 0.16657\n",
      "Epoch 1333/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1602 - accuracy: 0.9285\n",
      "\n",
      "Epoch 01333: loss did not improve from 0.16657\n",
      "Epoch 1334/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1678 - accuracy: 0.9229\n",
      "\n",
      "Epoch 01334: loss did not improve from 0.16657\n",
      "Epoch 1335/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1647 - accuracy: 0.9271\n",
      "\n",
      "Epoch 01335: loss did not improve from 0.16657\n",
      "Epoch 1336/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1620 - accuracy: 0.9268\n",
      "\n",
      "Epoch 01336: loss did not improve from 0.16657\n",
      "Epoch 1337/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1605 - accuracy: 0.9253\n",
      "\n",
      "Epoch 01337: loss did not improve from 0.16657\n",
      "Epoch 1338/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1577 - accuracy: 0.9329\n",
      "\n",
      "Epoch 01338: loss did not improve from 0.16657\n",
      "Epoch 1339/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1598 - accuracy: 0.9305\n",
      "\n",
      "Epoch 01339: loss did not improve from 0.16657\n",
      "Epoch 1340/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1733 - accuracy: 0.9204\n",
      "\n",
      "Epoch 01340: loss did not improve from 0.16657\n",
      "Epoch 1341/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1680 - accuracy: 0.9256\n",
      "\n",
      "Epoch 01341: loss did not improve from 0.16657\n",
      "Epoch 1342/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1592 - accuracy: 0.9308\n",
      "\n",
      "Epoch 01342: loss did not improve from 0.16657\n",
      "Epoch 1343/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1678 - accuracy: 0.9263\n",
      "\n",
      "Epoch 01343: loss did not improve from 0.16657\n",
      "Epoch 1344/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1638 - accuracy: 0.9298\n",
      "\n",
      "Epoch 01344: loss did not improve from 0.16657\n",
      "Epoch 1345/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1582 - accuracy: 0.9346\n",
      "\n",
      "Epoch 01345: loss did not improve from 0.16657\n",
      "Epoch 1346/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1677 - accuracy: 0.9295\n",
      "\n",
      "Epoch 01346: loss did not improve from 0.16657\n",
      "Epoch 1347/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1796 - accuracy: 0.9247\n",
      "\n",
      "Epoch 01347: loss did not improve from 0.16657\n",
      "Epoch 1348/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1792 - accuracy: 0.9246\n",
      "\n",
      "Epoch 01348: loss did not improve from 0.16657\n",
      "Epoch 1349/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1836 - accuracy: 0.9244\n",
      "\n",
      "Epoch 01349: loss did not improve from 0.16657\n",
      "Epoch 1350/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1739 - accuracy: 0.9273\n",
      "\n",
      "Epoch 01350: loss did not improve from 0.16657\n",
      "Epoch 1351/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1882 - accuracy: 0.9238\n",
      "\n",
      "Epoch 01351: loss did not improve from 0.16657\n",
      "Epoch 1352/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1900 - accuracy: 0.9197\n",
      "\n",
      "Epoch 01352: loss did not improve from 0.16657\n",
      "Epoch 1353/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2131 - accuracy: 0.9196\n",
      "\n",
      "Epoch 01353: loss did not improve from 0.16657\n",
      "Epoch 1354/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2263 - accuracy: 0.9099\n",
      "\n",
      "Epoch 01354: loss did not improve from 0.16657\n",
      "Epoch 1355/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2253 - accuracy: 0.9123\n",
      "\n",
      "Epoch 01355: loss did not improve from 0.16657\n",
      "Epoch 1356/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2364 - accuracy: 0.9097\n",
      "\n",
      "Epoch 01356: loss did not improve from 0.16657\n",
      "Epoch 1357/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2510 - accuracy: 0.9069\n",
      "\n",
      "Epoch 01357: loss did not improve from 0.16657\n",
      "Epoch 1358/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2238 - accuracy: 0.9134\n",
      "\n",
      "Epoch 01358: loss did not improve from 0.16657\n",
      "Epoch 1359/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1974 - accuracy: 0.9251\n",
      "\n",
      "Epoch 01359: loss did not improve from 0.16657\n",
      "Epoch 1360/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1821 - accuracy: 0.9309\n",
      "\n",
      "Epoch 01360: loss did not improve from 0.16657\n",
      "Epoch 1361/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1871 - accuracy: 0.9251\n",
      "\n",
      "Epoch 01361: loss did not improve from 0.16657\n",
      "Epoch 1362/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1720 - accuracy: 0.9270\n",
      "\n",
      "Epoch 01362: loss did not improve from 0.16657\n",
      "Epoch 1363/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1687 - accuracy: 0.9263\n",
      "\n",
      "Epoch 01363: loss did not improve from 0.16657\n",
      "Epoch 1364/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1618 - accuracy: 0.9278\n",
      "\n",
      "Epoch 01364: loss did not improve from 0.16657\n",
      "Epoch 1365/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1659 - accuracy: 0.9259\n",
      "\n",
      "Epoch 01365: loss did not improve from 0.16657\n",
      "Epoch 1366/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1725 - accuracy: 0.9207\n",
      "\n",
      "Epoch 01366: loss improved from 0.16657 to 0.16551, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1367/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1576 - accuracy: 0.9325\n",
      "\n",
      "Epoch 01367: loss improved from 0.16551 to 0.16435, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1368/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1692 - accuracy: 0.9264\n",
      "\n",
      "Epoch 01368: loss did not improve from 0.16435\n",
      "Epoch 1369/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1622 - accuracy: 0.9303\n",
      "\n",
      "Epoch 01369: loss did not improve from 0.16435\n",
      "Epoch 1370/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1681 - accuracy: 0.9209\n",
      "\n",
      "Epoch 01370: loss improved from 0.16435 to 0.16389, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1371/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1662 - accuracy: 0.9200\n",
      "\n",
      "Epoch 01371: loss improved from 0.16389 to 0.16318, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1372/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1586 - accuracy: 0.9293\n",
      "\n",
      "Epoch 01372: loss did not improve from 0.16318\n",
      "Epoch 1373/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1607 - accuracy: 0.9239\n",
      "\n",
      "Epoch 01373: loss did not improve from 0.16318\n",
      "Epoch 1374/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1594 - accuracy: 0.9327\n",
      "\n",
      "Epoch 01374: loss did not improve from 0.16318\n",
      "Epoch 1375/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1629 - accuracy: 0.9288\n",
      "\n",
      "Epoch 01375: loss did not improve from 0.16318\n",
      "Epoch 1376/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1615 - accuracy: 0.9273\n",
      "\n",
      "Epoch 01376: loss did not improve from 0.16318\n",
      "Epoch 1377/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1630 - accuracy: 0.9261\n",
      "\n",
      "Epoch 01377: loss did not improve from 0.16318\n",
      "Epoch 1378/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1774 - accuracy: 0.9212\n",
      "\n",
      "Epoch 01378: loss did not improve from 0.16318\n",
      "Epoch 1379/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1674 - accuracy: 0.9261\n",
      "\n",
      "Epoch 01379: loss did not improve from 0.16318\n",
      "Epoch 1380/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1673 - accuracy: 0.9229\n",
      "\n",
      "Epoch 01380: loss did not improve from 0.16318\n",
      "Epoch 1381/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1545 - accuracy: 0.9339\n",
      "\n",
      "Epoch 01381: loss did not improve from 0.16318\n",
      "Epoch 1382/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1587 - accuracy: 0.9291\n",
      "\n",
      "Epoch 01382: loss did not improve from 0.16318\n",
      "Epoch 1383/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1610 - accuracy: 0.9284\n",
      "\n",
      "Epoch 01383: loss did not improve from 0.16318\n",
      "Epoch 1384/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1630 - accuracy: 0.9299\n",
      "\n",
      "Epoch 01384: loss did not improve from 0.16318\n",
      "Epoch 1385/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1705 - accuracy: 0.9305\n",
      "\n",
      "Epoch 01385: loss did not improve from 0.16318\n",
      "Epoch 1386/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1624 - accuracy: 0.9311\n",
      "\n",
      "Epoch 01386: loss did not improve from 0.16318\n",
      "Epoch 1387/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1714 - accuracy: 0.9242\n",
      "\n",
      "Epoch 01387: loss did not improve from 0.16318\n",
      "Epoch 1388/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1664 - accuracy: 0.9242\n",
      "\n",
      "Epoch 01388: loss did not improve from 0.16318\n",
      "Epoch 1389/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1581 - accuracy: 0.9283\n",
      "\n",
      "Epoch 01389: loss did not improve from 0.16318\n",
      "Epoch 1390/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1624 - accuracy: 0.9306\n",
      "\n",
      "Epoch 01390: loss did not improve from 0.16318\n",
      "Epoch 1391/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1615 - accuracy: 0.9280\n",
      "\n",
      "Epoch 01391: loss did not improve from 0.16318\n",
      "Epoch 1392/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1601 - accuracy: 0.9243\n",
      "\n",
      "Epoch 01392: loss did not improve from 0.16318\n",
      "Epoch 1393/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1567 - accuracy: 0.9250\n",
      "\n",
      "Epoch 01393: loss improved from 0.16318 to 0.16239, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1394/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1565 - accuracy: 0.9320\n",
      "\n",
      "Epoch 01394: loss improved from 0.16239 to 0.16161, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1395/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1691 - accuracy: 0.9225\n",
      "\n",
      "Epoch 01395: loss did not improve from 0.16161\n",
      "Epoch 1396/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1679 - accuracy: 0.9270\n",
      "\n",
      "Epoch 01396: loss did not improve from 0.16161\n",
      "Epoch 1397/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1733 - accuracy: 0.9285\n",
      "\n",
      "Epoch 01397: loss did not improve from 0.16161\n",
      "Epoch 1398/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1707 - accuracy: 0.9262\n",
      "\n",
      "Epoch 01398: loss did not improve from 0.16161\n",
      "Epoch 1399/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1727 - accuracy: 0.9229\n",
      "\n",
      "Epoch 01399: loss did not improve from 0.16161\n",
      "Epoch 1400/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1779 - accuracy: 0.9269\n",
      "\n",
      "Epoch 01400: loss did not improve from 0.16161\n",
      "Epoch 1401/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1777 - accuracy: 0.9267\n",
      "\n",
      "Epoch 01401: loss did not improve from 0.16161\n",
      "Epoch 1402/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1953 - accuracy: 0.9203\n",
      "\n",
      "Epoch 01402: loss did not improve from 0.16161\n",
      "Epoch 1403/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1856 - accuracy: 0.9250\n",
      "\n",
      "Epoch 01403: loss did not improve from 0.16161\n",
      "Epoch 1404/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2025 - accuracy: 0.9175\n",
      "\n",
      "Epoch 01404: loss did not improve from 0.16161\n",
      "Epoch 1405/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3289 - accuracy: 0.8810\n",
      "\n",
      "Epoch 01405: loss did not improve from 0.16161\n",
      "Epoch 1406/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3247 - accuracy: 0.8827\n",
      "\n",
      "Epoch 01406: loss did not improve from 0.16161\n",
      "Epoch 1407/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3696 - accuracy: 0.8745\n",
      "\n",
      "Epoch 01407: loss did not improve from 0.16161\n",
      "Epoch 1408/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3303 - accuracy: 0.8812\n",
      "\n",
      "Epoch 01408: loss did not improve from 0.16161\n",
      "Epoch 1409/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2575 - accuracy: 0.8968\n",
      "\n",
      "Epoch 01409: loss did not improve from 0.16161\n",
      "Epoch 1410/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2126 - accuracy: 0.9143\n",
      "\n",
      "Epoch 01410: loss did not improve from 0.16161\n",
      "Epoch 1411/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2017 - accuracy: 0.9224\n",
      "\n",
      "Epoch 01411: loss did not improve from 0.16161\n",
      "Epoch 1412/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1802 - accuracy: 0.9270\n",
      "\n",
      "Epoch 01412: loss did not improve from 0.16161\n",
      "Epoch 1413/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1587 - accuracy: 0.9333\n",
      "\n",
      "Epoch 01413: loss did not improve from 0.16161\n",
      "Epoch 1414/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1729 - accuracy: 0.9223\n",
      "\n",
      "Epoch 01414: loss did not improve from 0.16161\n",
      "Epoch 1415/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1619 - accuracy: 0.9269\n",
      "\n",
      "Epoch 01415: loss did not improve from 0.16161\n",
      "Epoch 1416/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1613 - accuracy: 0.9288\n",
      "\n",
      "Epoch 01416: loss did not improve from 0.16161\n",
      "Epoch 1417/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1532 - accuracy: 0.9317\n",
      "\n",
      "Epoch 01417: loss did not improve from 0.16161\n",
      "Epoch 1418/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1569 - accuracy: 0.9328\n",
      "\n",
      "Epoch 01418: loss did not improve from 0.16161\n",
      "Epoch 1419/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1560 - accuracy: 0.9315\n",
      "\n",
      "Epoch 01419: loss improved from 0.16161 to 0.16094, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1420/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1590 - accuracy: 0.9281\n",
      "\n",
      "Epoch 01420: loss improved from 0.16094 to 0.15982, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1421/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1547 - accuracy: 0.9265\n",
      "\n",
      "Epoch 01421: loss improved from 0.15982 to 0.15979, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1422/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1493 - accuracy: 0.9298\n",
      "\n",
      "Epoch 01422: loss improved from 0.15979 to 0.15964, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1423/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1573 - accuracy: 0.9303\n",
      "\n",
      "Epoch 01423: loss did not improve from 0.15964\n",
      "Epoch 1424/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1575 - accuracy: 0.9300\n",
      "\n",
      "Epoch 01424: loss improved from 0.15964 to 0.15908, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1425/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1540 - accuracy: 0.9269\n",
      "\n",
      "Epoch 01425: loss did not improve from 0.15908\n",
      "Epoch 1426/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1530 - accuracy: 0.9267\n",
      "\n",
      "Epoch 01426: loss did not improve from 0.15908\n",
      "Epoch 1427/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1570 - accuracy: 0.9283\n",
      "\n",
      "Epoch 01427: loss did not improve from 0.15908\n",
      "Epoch 1428/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1572 - accuracy: 0.9283\n",
      "\n",
      "Epoch 01428: loss did not improve from 0.15908\n",
      "Epoch 1429/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1576 - accuracy: 0.9279\n",
      "\n",
      "Epoch 01429: loss did not improve from 0.15908\n",
      "Epoch 1430/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1578 - accuracy: 0.9280\n",
      "\n",
      "Epoch 01430: loss improved from 0.15908 to 0.15783, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1431/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1543 - accuracy: 0.9290\n",
      "\n",
      "Epoch 01431: loss improved from 0.15783 to 0.15645, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1432/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1469 - accuracy: 0.9343\n",
      "\n",
      "Epoch 01432: loss did not improve from 0.15645\n",
      "Epoch 1433/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1586 - accuracy: 0.9259\n",
      "\n",
      "Epoch 01433: loss did not improve from 0.15645\n",
      "Epoch 1434/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1401 - accuracy: 0.9401\n",
      "\n",
      "Epoch 01434: loss did not improve from 0.15645\n",
      "Epoch 1435/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1595 - accuracy: 0.9303\n",
      "\n",
      "Epoch 01435: loss did not improve from 0.15645\n",
      "Epoch 1436/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1540 - accuracy: 0.9283\n",
      "\n",
      "Epoch 01436: loss did not improve from 0.15645\n",
      "Epoch 1437/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1488 - accuracy: 0.9340\n",
      "\n",
      "Epoch 01437: loss did not improve from 0.15645\n",
      "Epoch 1438/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1559 - accuracy: 0.9260\n",
      "\n",
      "Epoch 01438: loss did not improve from 0.15645\n",
      "Epoch 1439/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1522 - accuracy: 0.9317\n",
      "\n",
      "Epoch 01439: loss did not improve from 0.15645\n",
      "Epoch 1440/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1580 - accuracy: 0.9263\n",
      "\n",
      "Epoch 01440: loss did not improve from 0.15645\n",
      "Epoch 1441/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1585 - accuracy: 0.9298\n",
      "\n",
      "Epoch 01441: loss did not improve from 0.15645\n",
      "Epoch 1442/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1595 - accuracy: 0.9283\n",
      "\n",
      "Epoch 01442: loss did not improve from 0.15645\n",
      "Epoch 1443/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1612 - accuracy: 0.9290\n",
      "\n",
      "Epoch 01443: loss did not improve from 0.15645\n",
      "Epoch 1444/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1622 - accuracy: 0.9261\n",
      "\n",
      "Epoch 01444: loss did not improve from 0.15645\n",
      "Epoch 1445/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1597 - accuracy: 0.9291\n",
      "\n",
      "Epoch 01445: loss did not improve from 0.15645\n",
      "Epoch 1446/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1746 - accuracy: 0.9281\n",
      "\n",
      "Epoch 01446: loss did not improve from 0.15645\n",
      "Epoch 1447/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1855 - accuracy: 0.9238\n",
      "\n",
      "Epoch 01447: loss did not improve from 0.15645\n",
      "Epoch 1448/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1928 - accuracy: 0.9243\n",
      "\n",
      "Epoch 01448: loss did not improve from 0.15645\n",
      "Epoch 1449/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2089 - accuracy: 0.9169\n",
      "\n",
      "Epoch 01449: loss did not improve from 0.15645\n",
      "Epoch 1450/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1907 - accuracy: 0.9213\n",
      "\n",
      "Epoch 01450: loss did not improve from 0.15645\n",
      "Epoch 1451/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1880 - accuracy: 0.9165\n",
      "\n",
      "Epoch 01451: loss did not improve from 0.15645\n",
      "Epoch 1452/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1743 - accuracy: 0.9239\n",
      "\n",
      "Epoch 01452: loss did not improve from 0.15645\n",
      "Epoch 1453/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1595 - accuracy: 0.9276\n",
      "\n",
      "Epoch 01453: loss did not improve from 0.15645\n",
      "Epoch 1454/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1640 - accuracy: 0.9265\n",
      "\n",
      "Epoch 01454: loss did not improve from 0.15645\n",
      "Epoch 1455/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1614 - accuracy: 0.9277\n",
      "\n",
      "Epoch 01455: loss did not improve from 0.15645\n",
      "Epoch 1456/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1479 - accuracy: 0.9332\n",
      "\n",
      "Epoch 01456: loss did not improve from 0.15645\n",
      "Epoch 1457/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1608 - accuracy: 0.9235\n",
      "\n",
      "Epoch 01457: loss did not improve from 0.15645\n",
      "Epoch 1458/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1497 - accuracy: 0.9311\n",
      "\n",
      "Epoch 01458: loss did not improve from 0.15645\n",
      "Epoch 1459/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1624 - accuracy: 0.9269\n",
      "\n",
      "Epoch 01459: loss did not improve from 0.15645\n",
      "Epoch 1460/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1514 - accuracy: 0.9293\n",
      "\n",
      "Epoch 01460: loss did not improve from 0.15645\n",
      "Epoch 1461/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1626 - accuracy: 0.9245\n",
      "\n",
      "Epoch 01461: loss did not improve from 0.15645\n",
      "Epoch 1462/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1481 - accuracy: 0.9351\n",
      "\n",
      "Epoch 01462: loss improved from 0.15645 to 0.15575, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1463/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1538 - accuracy: 0.9317\n",
      "\n",
      "Epoch 01463: loss did not improve from 0.15575\n",
      "Epoch 1464/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1434 - accuracy: 0.9330\n",
      "\n",
      "Epoch 01464: loss did not improve from 0.15575\n",
      "Epoch 1465/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1468 - accuracy: 0.9337\n",
      "\n",
      "Epoch 01465: loss improved from 0.15575 to 0.15435, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1466/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1488 - accuracy: 0.9306\n",
      "\n",
      "Epoch 01466: loss did not improve from 0.15435\n",
      "Epoch 1467/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1509 - accuracy: 0.9287\n",
      "\n",
      "Epoch 01467: loss did not improve from 0.15435\n",
      "Epoch 1468/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1463 - accuracy: 0.9330\n",
      "\n",
      "Epoch 01468: loss did not improve from 0.15435\n",
      "Epoch 1469/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1581 - accuracy: 0.9260\n",
      "\n",
      "Epoch 01469: loss did not improve from 0.15435\n",
      "Epoch 1470/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1502 - accuracy: 0.9258\n",
      "\n",
      "Epoch 01470: loss did not improve from 0.15435\n",
      "Epoch 1471/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1554 - accuracy: 0.9299\n",
      "\n",
      "Epoch 01471: loss did not improve from 0.15435\n",
      "Epoch 1472/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1520 - accuracy: 0.9283\n",
      "\n",
      "Epoch 01472: loss did not improve from 0.15435\n",
      "Epoch 1473/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1686 - accuracy: 0.9270\n",
      "\n",
      "Epoch 01473: loss did not improve from 0.15435\n",
      "Epoch 1474/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1832 - accuracy: 0.9213\n",
      "\n",
      "Epoch 01474: loss did not improve from 0.15435\n",
      "Epoch 1475/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1982 - accuracy: 0.9209\n",
      "\n",
      "Epoch 01475: loss did not improve from 0.15435\n",
      "Epoch 1476/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2040 - accuracy: 0.9176\n",
      "\n",
      "Epoch 01476: loss did not improve from 0.15435\n",
      "Epoch 1477/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2038 - accuracy: 0.9161\n",
      "\n",
      "Epoch 01477: loss did not improve from 0.15435\n",
      "Epoch 1478/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1966 - accuracy: 0.9152\n",
      "\n",
      "Epoch 01478: loss did not improve from 0.15435\n",
      "Epoch 1479/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2078 - accuracy: 0.9148\n",
      "\n",
      "Epoch 01479: loss did not improve from 0.15435\n",
      "Epoch 1480/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2160 - accuracy: 0.9109\n",
      "\n",
      "Epoch 01480: loss did not improve from 0.15435\n",
      "Epoch 1481/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1986 - accuracy: 0.9157\n",
      "\n",
      "Epoch 01481: loss did not improve from 0.15435\n",
      "Epoch 1482/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1776 - accuracy: 0.9263\n",
      "\n",
      "Epoch 01482: loss did not improve from 0.15435\n",
      "Epoch 1483/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1581 - accuracy: 0.9331\n",
      "\n",
      "Epoch 01483: loss did not improve from 0.15435\n",
      "Epoch 1484/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1594 - accuracy: 0.9316\n",
      "\n",
      "Epoch 01484: loss did not improve from 0.15435\n",
      "Epoch 1485/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1539 - accuracy: 0.9306\n",
      "\n",
      "Epoch 01485: loss did not improve from 0.15435\n",
      "Epoch 1486/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1554 - accuracy: 0.9278\n",
      "\n",
      "Epoch 01486: loss did not improve from 0.15435\n",
      "Epoch 1487/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1548 - accuracy: 0.9307\n",
      "\n",
      "Epoch 01487: loss did not improve from 0.15435\n",
      "Epoch 1488/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1454 - accuracy: 0.9388\n",
      "\n",
      "Epoch 01488: loss did not improve from 0.15435\n",
      "Epoch 1489/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1594 - accuracy: 0.9266\n",
      "\n",
      "Epoch 01489: loss did not improve from 0.15435\n",
      "Epoch 1490/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1468 - accuracy: 0.9308\n",
      "\n",
      "Epoch 01490: loss did not improve from 0.15435\n",
      "Epoch 1491/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1571 - accuracy: 0.9276\n",
      "\n",
      "Epoch 01491: loss did not improve from 0.15435\n",
      "Epoch 1492/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1534 - accuracy: 0.9261\n",
      "\n",
      "Epoch 01492: loss improved from 0.15435 to 0.15375, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1493/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1530 - accuracy: 0.9306\n",
      "\n",
      "Epoch 01493: loss did not improve from 0.15375\n",
      "Epoch 1494/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1560 - accuracy: 0.9298\n",
      "\n",
      "Epoch 01494: loss did not improve from 0.15375\n",
      "Epoch 1495/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1383 - accuracy: 0.9378\n",
      "\n",
      "Epoch 01495: loss did not improve from 0.15375\n",
      "Epoch 1496/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1554 - accuracy: 0.9248\n",
      "\n",
      "Epoch 01496: loss did not improve from 0.15375\n",
      "Epoch 1497/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1595 - accuracy: 0.9283\n",
      "\n",
      "Epoch 01497: loss did not improve from 0.15375\n",
      "Epoch 1498/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1567 - accuracy: 0.9274\n",
      "\n",
      "Epoch 01498: loss did not improve from 0.15375\n",
      "Epoch 1499/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1869 - accuracy: 0.9204\n",
      "\n",
      "Epoch 01499: loss did not improve from 0.15375\n",
      "Epoch 1500/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1969 - accuracy: 0.9183\n",
      "\n",
      "Epoch 01500: loss did not improve from 0.15375\n",
      "Epoch 1501/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2095 - accuracy: 0.9135\n",
      "\n",
      "Epoch 01501: loss did not improve from 0.15375\n",
      "Epoch 1502/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1965 - accuracy: 0.9193\n",
      "\n",
      "Epoch 01502: loss did not improve from 0.15375\n",
      "Epoch 1503/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1879 - accuracy: 0.9261\n",
      "\n",
      "Epoch 01503: loss did not improve from 0.15375\n",
      "Epoch 1504/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2136 - accuracy: 0.9151\n",
      "\n",
      "Epoch 01504: loss did not improve from 0.15375\n",
      "Epoch 1505/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2036 - accuracy: 0.9170\n",
      "\n",
      "Epoch 01505: loss did not improve from 0.15375\n",
      "Epoch 1506/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1889 - accuracy: 0.9192\n",
      "\n",
      "Epoch 01506: loss did not improve from 0.15375\n",
      "Epoch 1507/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1803 - accuracy: 0.9235\n",
      "\n",
      "Epoch 01507: loss did not improve from 0.15375\n",
      "Epoch 1508/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1562 - accuracy: 0.9311\n",
      "\n",
      "Epoch 01508: loss did not improve from 0.15375\n",
      "Epoch 1509/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1585 - accuracy: 0.9256\n",
      "\n",
      "Epoch 01509: loss did not improve from 0.15375\n",
      "Epoch 1510/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1489 - accuracy: 0.9298\n",
      "\n",
      "Epoch 01510: loss did not improve from 0.15375\n",
      "Epoch 1511/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1583 - accuracy: 0.9225\n",
      "\n",
      "Epoch 01511: loss did not improve from 0.15375\n",
      "Epoch 1512/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1507 - accuracy: 0.9267\n",
      "\n",
      "Epoch 01512: loss improved from 0.15375 to 0.15286, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1513/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1552 - accuracy: 0.9270\n",
      "\n",
      "Epoch 01513: loss improved from 0.15286 to 0.15184, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1514/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1433 - accuracy: 0.9323\n",
      "\n",
      "Epoch 01514: loss improved from 0.15184 to 0.15174, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1515/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1506 - accuracy: 0.9322\n",
      "\n",
      "Epoch 01515: loss did not improve from 0.15174\n",
      "Epoch 1516/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1544 - accuracy: 0.9284\n",
      "\n",
      "Epoch 01516: loss did not improve from 0.15174\n",
      "Epoch 1517/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1509 - accuracy: 0.9292\n",
      "\n",
      "Epoch 01517: loss did not improve from 0.15174\n",
      "Epoch 1518/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1432 - accuracy: 0.9369\n",
      "\n",
      "Epoch 01518: loss did not improve from 0.15174\n",
      "Epoch 1519/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1529 - accuracy: 0.9293\n",
      "\n",
      "Epoch 01519: loss did not improve from 0.15174\n",
      "Epoch 1520/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1460 - accuracy: 0.9308\n",
      "\n",
      "Epoch 01520: loss did not improve from 0.15174\n",
      "Epoch 1521/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1552 - accuracy: 0.9262\n",
      "\n",
      "Epoch 01521: loss did not improve from 0.15174\n",
      "Epoch 1522/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1551 - accuracy: 0.9306\n",
      "\n",
      "Epoch 01522: loss improved from 0.15174 to 0.15137, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1523/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1480 - accuracy: 0.9337\n",
      "\n",
      "Epoch 01523: loss did not improve from 0.15137\n",
      "Epoch 1524/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1440 - accuracy: 0.9330\n",
      "\n",
      "Epoch 01524: loss did not improve from 0.15137\n",
      "Epoch 1525/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1562 - accuracy: 0.9270\n",
      "\n",
      "Epoch 01525: loss did not improve from 0.15137\n",
      "Epoch 1526/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1561 - accuracy: 0.9267\n",
      "\n",
      "Epoch 01526: loss did not improve from 0.15137\n",
      "Epoch 1527/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1581 - accuracy: 0.9237\n",
      "\n",
      "Epoch 01527: loss did not improve from 0.15137\n",
      "Epoch 1528/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1478 - accuracy: 0.9302\n",
      "\n",
      "Epoch 01528: loss did not improve from 0.15137\n",
      "Epoch 1529/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1438 - accuracy: 0.9279\n",
      "\n",
      "Epoch 01529: loss did not improve from 0.15137\n",
      "Epoch 1530/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1523 - accuracy: 0.9331\n",
      "\n",
      "Epoch 01530: loss did not improve from 0.15137\n",
      "Epoch 1531/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1617 - accuracy: 0.9279\n",
      "\n",
      "Epoch 01531: loss did not improve from 0.15137\n",
      "Epoch 1532/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1564 - accuracy: 0.9326\n",
      "\n",
      "Epoch 01532: loss did not improve from 0.15137\n",
      "Epoch 1533/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1505 - accuracy: 0.9357\n",
      "\n",
      "Epoch 01533: loss did not improve from 0.15137\n",
      "Epoch 1534/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1738 - accuracy: 0.9254\n",
      "\n",
      "Epoch 01534: loss did not improve from 0.15137\n",
      "Epoch 1535/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2041 - accuracy: 0.9126\n",
      "\n",
      "Epoch 01535: loss did not improve from 0.15137\n",
      "Epoch 1536/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2045 - accuracy: 0.9186\n",
      "\n",
      "Epoch 01536: loss did not improve from 0.15137\n",
      "Epoch 1537/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3182 - accuracy: 0.8859\n",
      "\n",
      "Epoch 01537: loss did not improve from 0.15137\n",
      "Epoch 1538/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3917 - accuracy: 0.8701\n",
      "\n",
      "Epoch 01538: loss did not improve from 0.15137\n",
      "Epoch 1539/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3591 - accuracy: 0.8753\n",
      "\n",
      "Epoch 01539: loss did not improve from 0.15137\n",
      "Epoch 1540/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2855 - accuracy: 0.8916\n",
      "\n",
      "Epoch 01540: loss did not improve from 0.15137\n",
      "Epoch 1541/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2156 - accuracy: 0.9144\n",
      "\n",
      "Epoch 01541: loss did not improve from 0.15137\n",
      "Epoch 1542/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1805 - accuracy: 0.9226\n",
      "\n",
      "Epoch 01542: loss did not improve from 0.15137\n",
      "Epoch 1543/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1591 - accuracy: 0.9315\n",
      "\n",
      "Epoch 01543: loss did not improve from 0.15137\n",
      "Epoch 1544/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1506 - accuracy: 0.9349\n",
      "\n",
      "Epoch 01544: loss did not improve from 0.15137\n",
      "Epoch 1545/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1430 - accuracy: 0.9338\n",
      "\n",
      "Epoch 01545: loss did not improve from 0.15137\n",
      "Epoch 1546/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1502 - accuracy: 0.9310\n",
      "\n",
      "Epoch 01546: loss did not improve from 0.15137\n",
      "Epoch 1547/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1454 - accuracy: 0.9323\n",
      "\n",
      "Epoch 01547: loss did not improve from 0.15137\n",
      "Epoch 1548/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1478 - accuracy: 0.9325\n",
      "\n",
      "Epoch 01548: loss did not improve from 0.15137\n",
      "Epoch 1549/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1484 - accuracy: 0.9331\n",
      "\n",
      "Epoch 01549: loss did not improve from 0.15137\n",
      "Epoch 1550/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1502 - accuracy: 0.9250\n",
      "\n",
      "Epoch 01550: loss improved from 0.15137 to 0.15041, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1551/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1415 - accuracy: 0.9328\n",
      "\n",
      "Epoch 01551: loss improved from 0.15041 to 0.15003, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1552/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1445 - accuracy: 0.9297\n",
      "\n",
      "Epoch 01552: loss improved from 0.15003 to 0.14964, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1553/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1450 - accuracy: 0.9283\n",
      "\n",
      "Epoch 01553: loss did not improve from 0.14964\n",
      "Epoch 1554/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1437 - accuracy: 0.9367\n",
      "\n",
      "Epoch 01554: loss improved from 0.14964 to 0.14940, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1555/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1538 - accuracy: 0.9271\n",
      "\n",
      "Epoch 01555: loss did not improve from 0.14940\n",
      "Epoch 1556/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1392 - accuracy: 0.9351\n",
      "\n",
      "Epoch 01556: loss did not improve from 0.14940\n",
      "Epoch 1557/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1415 - accuracy: 0.9357\n",
      "\n",
      "Epoch 01557: loss improved from 0.14940 to 0.14853, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1558/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1471 - accuracy: 0.9259\n",
      "\n",
      "Epoch 01558: loss did not improve from 0.14853\n",
      "Epoch 1559/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1329 - accuracy: 0.9366\n",
      "\n",
      "Epoch 01559: loss improved from 0.14853 to 0.14713, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1560/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1350 - accuracy: 0.9345\n",
      "\n",
      "Epoch 01560: loss did not improve from 0.14713\n",
      "Epoch 1561/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1428 - accuracy: 0.9308\n",
      "\n",
      "Epoch 01561: loss did not improve from 0.14713\n",
      "Epoch 1562/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1443 - accuracy: 0.9307\n",
      "\n",
      "Epoch 01562: loss did not improve from 0.14713\n",
      "Epoch 1563/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1438 - accuracy: 0.9298\n",
      "\n",
      "Epoch 01563: loss improved from 0.14713 to 0.14653, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1564/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1369 - accuracy: 0.9328\n",
      "\n",
      "Epoch 01564: loss improved from 0.14653 to 0.14566, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1565/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1412 - accuracy: 0.9291\n",
      "\n",
      "Epoch 01565: loss did not improve from 0.14566\n",
      "Epoch 1566/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1431 - accuracy: 0.9327\n",
      "\n",
      "Epoch 01566: loss did not improve from 0.14566\n",
      "Epoch 1567/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1455 - accuracy: 0.9318\n",
      "\n",
      "Epoch 01567: loss did not improve from 0.14566\n",
      "Epoch 1568/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1354 - accuracy: 0.9342\n",
      "\n",
      "Epoch 01568: loss did not improve from 0.14566\n",
      "Epoch 1569/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1430 - accuracy: 0.9295\n",
      "\n",
      "Epoch 01569: loss did not improve from 0.14566\n",
      "Epoch 1570/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1419 - accuracy: 0.9302\n",
      "\n",
      "Epoch 01570: loss did not improve from 0.14566\n",
      "Epoch 1571/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1414 - accuracy: 0.9349\n",
      "\n",
      "Epoch 01571: loss did not improve from 0.14566\n",
      "Epoch 1572/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1374 - accuracy: 0.9337\n",
      "\n",
      "Epoch 01572: loss did not improve from 0.14566\n",
      "Epoch 1573/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1466 - accuracy: 0.9269\n",
      "\n",
      "Epoch 01573: loss did not improve from 0.14566\n",
      "Epoch 1574/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1447 - accuracy: 0.9331\n",
      "\n",
      "Epoch 01574: loss did not improve from 0.14566\n",
      "Epoch 1575/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1416 - accuracy: 0.9336\n",
      "\n",
      "Epoch 01575: loss did not improve from 0.14566\n",
      "Epoch 1576/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1447 - accuracy: 0.9306\n",
      "\n",
      "Epoch 01576: loss did not improve from 0.14566\n",
      "Epoch 1577/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1427 - accuracy: 0.9322\n",
      "\n",
      "Epoch 01577: loss did not improve from 0.14566\n",
      "Epoch 1578/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1588 - accuracy: 0.9260\n",
      "\n",
      "Epoch 01578: loss did not improve from 0.14566\n",
      "Epoch 1579/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1529 - accuracy: 0.9294\n",
      "\n",
      "Epoch 01579: loss did not improve from 0.14566\n",
      "Epoch 1580/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1498 - accuracy: 0.9292\n",
      "\n",
      "Epoch 01580: loss did not improve from 0.14566\n",
      "Epoch 1581/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1407 - accuracy: 0.9346\n",
      "\n",
      "Epoch 01581: loss did not improve from 0.14566\n",
      "Epoch 1582/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1528 - accuracy: 0.9259\n",
      "\n",
      "Epoch 01582: loss did not improve from 0.14566\n",
      "Epoch 1583/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1377 - accuracy: 0.9347\n",
      "\n",
      "Epoch 01583: loss did not improve from 0.14566\n",
      "Epoch 1584/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1384 - accuracy: 0.9358\n",
      "\n",
      "Epoch 01584: loss did not improve from 0.14566\n",
      "Epoch 1585/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1474 - accuracy: 0.9291\n",
      "\n",
      "Epoch 01585: loss did not improve from 0.14566\n",
      "Epoch 1586/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1484 - accuracy: 0.9293\n",
      "\n",
      "Epoch 01586: loss did not improve from 0.14566\n",
      "Epoch 1587/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1476 - accuracy: 0.9329\n",
      "\n",
      "Epoch 01587: loss did not improve from 0.14566\n",
      "Epoch 1588/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1423 - accuracy: 0.9330\n",
      "\n",
      "Epoch 01588: loss did not improve from 0.14566\n",
      "Epoch 1589/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1521 - accuracy: 0.9262\n",
      "\n",
      "Epoch 01589: loss did not improve from 0.14566\n",
      "Epoch 1590/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1513 - accuracy: 0.9252\n",
      "\n",
      "Epoch 01590: loss did not improve from 0.14566\n",
      "Epoch 1591/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1501 - accuracy: 0.9280\n",
      "\n",
      "Epoch 01591: loss did not improve from 0.14566\n",
      "Epoch 1592/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1457 - accuracy: 0.9296\n",
      "\n",
      "Epoch 01592: loss did not improve from 0.14566\n",
      "Epoch 1593/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1413 - accuracy: 0.9295\n",
      "\n",
      "Epoch 01593: loss did not improve from 0.14566\n",
      "Epoch 1594/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1440 - accuracy: 0.9307\n",
      "\n",
      "Epoch 01594: loss did not improve from 0.14566\n",
      "Epoch 1595/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1488 - accuracy: 0.9286\n",
      "\n",
      "Epoch 01595: loss did not improve from 0.14566\n",
      "Epoch 1596/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1377 - accuracy: 0.9322\n",
      "\n",
      "Epoch 01596: loss did not improve from 0.14566\n",
      "Epoch 1597/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1496 - accuracy: 0.9272\n",
      "\n",
      "Epoch 01597: loss did not improve from 0.14566\n",
      "Epoch 1598/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1582 - accuracy: 0.9238\n",
      "\n",
      "Epoch 01598: loss did not improve from 0.14566\n",
      "Epoch 1599/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1520 - accuracy: 0.9325\n",
      "\n",
      "Epoch 01599: loss did not improve from 0.14566\n",
      "Epoch 1600/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1662 - accuracy: 0.9258\n",
      "\n",
      "Epoch 01600: loss did not improve from 0.14566\n",
      "Epoch 1601/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1860 - accuracy: 0.9239\n",
      "\n",
      "Epoch 01601: loss did not improve from 0.14566\n",
      "Epoch 1602/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2019 - accuracy: 0.9149\n",
      "\n",
      "Epoch 01602: loss did not improve from 0.14566\n",
      "Epoch 1603/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2206 - accuracy: 0.9127\n",
      "\n",
      "Epoch 01603: loss did not improve from 0.14566\n",
      "Epoch 1604/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2436 - accuracy: 0.9033\n",
      "\n",
      "Epoch 01604: loss did not improve from 0.14566\n",
      "Epoch 1605/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2329 - accuracy: 0.9098\n",
      "\n",
      "Epoch 01605: loss did not improve from 0.14566\n",
      "Epoch 1606/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2566 - accuracy: 0.9012\n",
      "\n",
      "Epoch 01606: loss did not improve from 0.14566\n",
      "Epoch 1607/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2857 - accuracy: 0.8859\n",
      "\n",
      "Epoch 01607: loss did not improve from 0.14566\n",
      "Epoch 1608/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2288 - accuracy: 0.9114\n",
      "\n",
      "Epoch 01608: loss did not improve from 0.14566\n",
      "Epoch 1609/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2186 - accuracy: 0.9057\n",
      "\n",
      "Epoch 01609: loss did not improve from 0.14566\n",
      "Epoch 1610/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1869 - accuracy: 0.9241\n",
      "\n",
      "Epoch 01610: loss did not improve from 0.14566\n",
      "Epoch 1611/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1734 - accuracy: 0.9287\n",
      "\n",
      "Epoch 01611: loss did not improve from 0.14566\n",
      "Epoch 1612/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1750 - accuracy: 0.9257\n",
      "\n",
      "Epoch 01612: loss did not improve from 0.14566\n",
      "Epoch 1613/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1635 - accuracy: 0.9250\n",
      "\n",
      "Epoch 01613: loss did not improve from 0.14566\n",
      "Epoch 1614/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1421 - accuracy: 0.9326\n",
      "\n",
      "Epoch 01614: loss did not improve from 0.14566\n",
      "Epoch 1615/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1424 - accuracy: 0.9346\n",
      "\n",
      "Epoch 01615: loss did not improve from 0.14566\n",
      "Epoch 1616/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1428 - accuracy: 0.9323\n",
      "\n",
      "Epoch 01616: loss did not improve from 0.14566\n",
      "Epoch 1617/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1428 - accuracy: 0.9345\n",
      "\n",
      "Epoch 01617: loss did not improve from 0.14566\n",
      "Epoch 1618/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1472 - accuracy: 0.9325\n",
      "\n",
      "Epoch 01618: loss did not improve from 0.14566\n",
      "Epoch 1619/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1397 - accuracy: 0.9339\n",
      "\n",
      "Epoch 01619: loss did not improve from 0.14566\n",
      "Epoch 1620/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1387 - accuracy: 0.9307\n",
      "\n",
      "Epoch 01620: loss improved from 0.14566 to 0.14510, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1621/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1405 - accuracy: 0.9296\n",
      "\n",
      "Epoch 01621: loss improved from 0.14510 to 0.14408, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1622/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1364 - accuracy: 0.9317\n",
      "\n",
      "Epoch 01622: loss did not improve from 0.14408\n",
      "Epoch 1623/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1356 - accuracy: 0.9337\n",
      "\n",
      "Epoch 01623: loss improved from 0.14408 to 0.14362, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1624/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1287 - accuracy: 0.9409\n",
      "\n",
      "Epoch 01624: loss did not improve from 0.14362\n",
      "Epoch 1625/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1466 - accuracy: 0.9260\n",
      "\n",
      "Epoch 01625: loss improved from 0.14362 to 0.14268, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1626/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1401 - accuracy: 0.9308\n",
      "\n",
      "Epoch 01626: loss did not improve from 0.14268\n",
      "Epoch 1627/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1495 - accuracy: 0.9251\n",
      "\n",
      "Epoch 01627: loss did not improve from 0.14268\n",
      "Epoch 1628/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1391 - accuracy: 0.9341\n",
      "\n",
      "Epoch 01628: loss did not improve from 0.14268\n",
      "Epoch 1629/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1323 - accuracy: 0.9336\n",
      "\n",
      "Epoch 01629: loss did not improve from 0.14268\n",
      "Epoch 1630/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1355 - accuracy: 0.9341\n",
      "\n",
      "Epoch 01630: loss did not improve from 0.14268\n",
      "Epoch 1631/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1358 - accuracy: 0.9333\n",
      "\n",
      "Epoch 01631: loss did not improve from 0.14268\n",
      "Epoch 1632/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1454 - accuracy: 0.9279\n",
      "\n",
      "Epoch 01632: loss did not improve from 0.14268\n",
      "Epoch 1633/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1443 - accuracy: 0.9271\n",
      "\n",
      "Epoch 01633: loss did not improve from 0.14268\n",
      "Epoch 1634/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1365 - accuracy: 0.9348\n",
      "\n",
      "Epoch 01634: loss did not improve from 0.14268\n",
      "Epoch 1635/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1434 - accuracy: 0.9274\n",
      "\n",
      "Epoch 01635: loss did not improve from 0.14268\n",
      "Epoch 1636/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1336 - accuracy: 0.9336\n",
      "\n",
      "Epoch 01636: loss did not improve from 0.14268\n",
      "Epoch 1637/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1456 - accuracy: 0.9302\n",
      "\n",
      "Epoch 01637: loss did not improve from 0.14268\n",
      "Epoch 1638/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1439 - accuracy: 0.9333\n",
      "\n",
      "Epoch 01638: loss did not improve from 0.14268\n",
      "Epoch 1639/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1388 - accuracy: 0.9346\n",
      "\n",
      "Epoch 01639: loss did not improve from 0.14268\n",
      "Epoch 1640/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1496 - accuracy: 0.9242\n",
      "\n",
      "Epoch 01640: loss did not improve from 0.14268\n",
      "Epoch 1641/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1452 - accuracy: 0.9267\n",
      "\n",
      "Epoch 01641: loss did not improve from 0.14268\n",
      "Epoch 1642/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1495 - accuracy: 0.9254\n",
      "\n",
      "Epoch 01642: loss did not improve from 0.14268\n",
      "Epoch 1643/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1418 - accuracy: 0.9335\n",
      "\n",
      "Epoch 01643: loss did not improve from 0.14268\n",
      "Epoch 1644/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1417 - accuracy: 0.9304\n",
      "\n",
      "Epoch 01644: loss did not improve from 0.14268\n",
      "Epoch 1645/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1474 - accuracy: 0.9320\n",
      "\n",
      "Epoch 01645: loss did not improve from 0.14268\n",
      "Epoch 1646/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1413 - accuracy: 0.9322\n",
      "\n",
      "Epoch 01646: loss did not improve from 0.14268\n",
      "Epoch 1647/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1420 - accuracy: 0.9298\n",
      "\n",
      "Epoch 01647: loss did not improve from 0.14268\n",
      "Epoch 1648/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1410 - accuracy: 0.9343\n",
      "\n",
      "Epoch 01648: loss did not improve from 0.14268\n",
      "Epoch 1649/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1467 - accuracy: 0.9245\n",
      "\n",
      "Epoch 01649: loss did not improve from 0.14268\n",
      "Epoch 1650/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1526 - accuracy: 0.9193\n",
      "\n",
      "Epoch 01650: loss did not improve from 0.14268\n",
      "Epoch 1651/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1450 - accuracy: 0.9305\n",
      "\n",
      "Epoch 01651: loss did not improve from 0.14268\n",
      "Epoch 1652/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1456 - accuracy: 0.9314\n",
      "\n",
      "Epoch 01652: loss did not improve from 0.14268\n",
      "Epoch 1653/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1347 - accuracy: 0.9335\n",
      "\n",
      "Epoch 01653: loss did not improve from 0.14268\n",
      "Epoch 1654/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1365 - accuracy: 0.9335\n",
      "\n",
      "Epoch 01654: loss did not improve from 0.14268\n",
      "Epoch 1655/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1437 - accuracy: 0.9314\n",
      "\n",
      "Epoch 01655: loss did not improve from 0.14268\n",
      "Epoch 1656/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1420 - accuracy: 0.9308\n",
      "\n",
      "Epoch 01656: loss did not improve from 0.14268\n",
      "Epoch 1657/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1399 - accuracy: 0.9322\n",
      "\n",
      "Epoch 01657: loss did not improve from 0.14268\n",
      "Epoch 1658/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1487 - accuracy: 0.9276\n",
      "\n",
      "Epoch 01658: loss did not improve from 0.14268\n",
      "Epoch 1659/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1484 - accuracy: 0.9324\n",
      "\n",
      "Epoch 01659: loss did not improve from 0.14268\n",
      "Epoch 1660/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1461 - accuracy: 0.9310\n",
      "\n",
      "Epoch 01660: loss did not improve from 0.14268\n",
      "Epoch 1661/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1568 - accuracy: 0.9308\n",
      "\n",
      "Epoch 01661: loss did not improve from 0.14268\n",
      "Epoch 1662/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1645 - accuracy: 0.9299\n",
      "\n",
      "Epoch 01662: loss did not improve from 0.14268\n",
      "Epoch 1663/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2506 - accuracy: 0.9028\n",
      "\n",
      "Epoch 01663: loss did not improve from 0.14268\n",
      "Epoch 1664/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.3226 - accuracy: 0.8817\n",
      "\n",
      "Epoch 01664: loss did not improve from 0.14268\n",
      "Epoch 1665/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3201 - accuracy: 0.8819\n",
      "\n",
      "Epoch 01665: loss did not improve from 0.14268\n",
      "Epoch 1666/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3540 - accuracy: 0.8701\n",
      "\n",
      "Epoch 01666: loss did not improve from 0.14268\n",
      "Epoch 1667/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3026 - accuracy: 0.8858\n",
      "\n",
      "Epoch 01667: loss did not improve from 0.14268\n",
      "Epoch 1668/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2732 - accuracy: 0.8980\n",
      "\n",
      "Epoch 01668: loss did not improve from 0.14268\n",
      "Epoch 1669/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2128 - accuracy: 0.9130\n",
      "\n",
      "Epoch 01669: loss did not improve from 0.14268\n",
      "Epoch 1670/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1736 - accuracy: 0.9243\n",
      "\n",
      "Epoch 01670: loss did not improve from 0.14268\n",
      "Epoch 1671/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1634 - accuracy: 0.9303\n",
      "\n",
      "Epoch 01671: loss did not improve from 0.14268\n",
      "Epoch 1672/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1449 - accuracy: 0.9336\n",
      "\n",
      "Epoch 01672: loss did not improve from 0.14268\n",
      "Epoch 1673/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1384 - accuracy: 0.9337\n",
      "\n",
      "Epoch 01673: loss did not improve from 0.14268\n",
      "Epoch 1674/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1317 - accuracy: 0.9400\n",
      "\n",
      "Epoch 01674: loss did not improve from 0.14268\n",
      "Epoch 1675/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1404 - accuracy: 0.9314\n",
      "\n",
      "Epoch 01675: loss did not improve from 0.14268\n",
      "Epoch 1676/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1428 - accuracy: 0.9295\n",
      "\n",
      "Epoch 01676: loss did not improve from 0.14268\n",
      "Epoch 1677/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1482 - accuracy: 0.9287\n",
      "\n",
      "Epoch 01677: loss did not improve from 0.14268\n",
      "Epoch 1678/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1417 - accuracy: 0.9297\n",
      "\n",
      "Epoch 01678: loss did not improve from 0.14268\n",
      "Epoch 1679/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1318 - accuracy: 0.9354\n",
      "\n",
      "Epoch 01679: loss improved from 0.14268 to 0.14226, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1680/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1422 - accuracy: 0.9303\n",
      "\n",
      "Epoch 01680: loss improved from 0.14226 to 0.14096, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1681/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1424 - accuracy: 0.9262\n",
      "\n",
      "Epoch 01681: loss did not improve from 0.14096\n",
      "Epoch 1682/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1320 - accuracy: 0.9356\n",
      "\n",
      "Epoch 01682: loss did not improve from 0.14096\n",
      "Epoch 1683/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1410 - accuracy: 0.9266\n",
      "\n",
      "Epoch 01683: loss improved from 0.14096 to 0.14052, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1684/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1306 - accuracy: 0.9360\n",
      "\n",
      "Epoch 01684: loss did not improve from 0.14052\n",
      "Epoch 1685/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1387 - accuracy: 0.9253\n",
      "\n",
      "Epoch 01685: loss improved from 0.14052 to 0.14049, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1686/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1316 - accuracy: 0.9342\n",
      "\n",
      "Epoch 01686: loss improved from 0.14049 to 0.13953, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1687/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1372 - accuracy: 0.9362\n",
      "\n",
      "Epoch 01687: loss did not improve from 0.13953\n",
      "Epoch 1688/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1316 - accuracy: 0.9348\n",
      "\n",
      "Epoch 01688: loss did not improve from 0.13953\n",
      "Epoch 1689/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1399 - accuracy: 0.9304\n",
      "\n",
      "Epoch 01689: loss did not improve from 0.13953\n",
      "Epoch 1690/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1299 - accuracy: 0.9341\n",
      "\n",
      "Epoch 01690: loss did not improve from 0.13953\n",
      "Epoch 1691/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1354 - accuracy: 0.9320\n",
      "\n",
      "Epoch 01691: loss did not improve from 0.13953\n",
      "Epoch 1692/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1305 - accuracy: 0.9366\n",
      "\n",
      "Epoch 01692: loss did not improve from 0.13953\n",
      "Epoch 1693/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1357 - accuracy: 0.9328\n",
      "\n",
      "Epoch 01693: loss did not improve from 0.13953\n",
      "Epoch 1694/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1303 - accuracy: 0.9346\n",
      "\n",
      "Epoch 01694: loss did not improve from 0.13953\n",
      "Epoch 1695/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1376 - accuracy: 0.9289\n",
      "\n",
      "Epoch 01695: loss did not improve from 0.13953\n",
      "Epoch 1696/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1398 - accuracy: 0.9321\n",
      "\n",
      "Epoch 01696: loss did not improve from 0.13953\n",
      "Epoch 1697/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1317 - accuracy: 0.9361\n",
      "\n",
      "Epoch 01697: loss did not improve from 0.13953\n",
      "Epoch 1698/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1352 - accuracy: 0.9333\n",
      "\n",
      "Epoch 01698: loss did not improve from 0.13953\n",
      "Epoch 1699/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1366 - accuracy: 0.9351\n",
      "\n",
      "Epoch 01699: loss did not improve from 0.13953\n",
      "Epoch 1700/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1351 - accuracy: 0.9313\n",
      "\n",
      "Epoch 01700: loss did not improve from 0.13953\n",
      "Epoch 1701/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1419 - accuracy: 0.9271\n",
      "\n",
      "Epoch 01701: loss did not improve from 0.13953\n",
      "Epoch 1702/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1547 - accuracy: 0.9187\n",
      "\n",
      "Epoch 01702: loss did not improve from 0.13953\n",
      "Epoch 1703/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1361 - accuracy: 0.9287\n",
      "\n",
      "Epoch 01703: loss did not improve from 0.13953\n",
      "Epoch 1704/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1339 - accuracy: 0.9340\n",
      "\n",
      "Epoch 01704: loss did not improve from 0.13953\n",
      "Epoch 1705/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1395 - accuracy: 0.9306\n",
      "\n",
      "Epoch 01705: loss did not improve from 0.13953\n",
      "Epoch 1706/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1404 - accuracy: 0.9282\n",
      "\n",
      "Epoch 01706: loss did not improve from 0.13953\n",
      "Epoch 1707/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1429 - accuracy: 0.9237\n",
      "\n",
      "Epoch 01707: loss did not improve from 0.13953\n",
      "Epoch 1708/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1396 - accuracy: 0.9318\n",
      "\n",
      "Epoch 01708: loss did not improve from 0.13953\n",
      "Epoch 1709/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1467 - accuracy: 0.9323\n",
      "\n",
      "Epoch 01709: loss did not improve from 0.13953\n",
      "Epoch 1710/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1381 - accuracy: 0.9326\n",
      "\n",
      "Epoch 01710: loss did not improve from 0.13953\n",
      "Epoch 1711/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1451 - accuracy: 0.9249\n",
      "\n",
      "Epoch 01711: loss did not improve from 0.13953\n",
      "Epoch 1712/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1285 - accuracy: 0.9383\n",
      "\n",
      "Epoch 01712: loss did not improve from 0.13953\n",
      "Epoch 1713/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1384 - accuracy: 0.9337\n",
      "\n",
      "Epoch 01713: loss did not improve from 0.13953\n",
      "Epoch 1714/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1407 - accuracy: 0.9304\n",
      "\n",
      "Epoch 01714: loss did not improve from 0.13953\n",
      "Epoch 1715/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1413 - accuracy: 0.9280\n",
      "\n",
      "Epoch 01715: loss did not improve from 0.13953\n",
      "Epoch 1716/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1465 - accuracy: 0.9282\n",
      "\n",
      "Epoch 01716: loss did not improve from 0.13953\n",
      "Epoch 1717/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1357 - accuracy: 0.9353\n",
      "\n",
      "Epoch 01717: loss did not improve from 0.13953\n",
      "Epoch 1718/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1448 - accuracy: 0.9309\n",
      "\n",
      "Epoch 01718: loss did not improve from 0.13953\n",
      "Epoch 1719/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1319 - accuracy: 0.9398\n",
      "\n",
      "Epoch 01719: loss did not improve from 0.13953\n",
      "Epoch 1720/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1403 - accuracy: 0.9326\n",
      "\n",
      "Epoch 01720: loss did not improve from 0.13953\n",
      "Epoch 1721/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1399 - accuracy: 0.9287\n",
      "\n",
      "Epoch 01721: loss did not improve from 0.13953\n",
      "Epoch 1722/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1451 - accuracy: 0.9244\n",
      "\n",
      "Epoch 01722: loss did not improve from 0.13953\n",
      "Epoch 1723/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1439 - accuracy: 0.9322\n",
      "\n",
      "Epoch 01723: loss did not improve from 0.13953\n",
      "Epoch 1724/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1505 - accuracy: 0.9250\n",
      "\n",
      "Epoch 01724: loss did not improve from 0.13953\n",
      "Epoch 1725/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1429 - accuracy: 0.9270\n",
      "\n",
      "Epoch 01725: loss did not improve from 0.13953\n",
      "Epoch 1726/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1405 - accuracy: 0.9317\n",
      "\n",
      "Epoch 01726: loss did not improve from 0.13953\n",
      "Epoch 1727/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1517 - accuracy: 0.9274\n",
      "\n",
      "Epoch 01727: loss did not improve from 0.13953\n",
      "Epoch 1728/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1665 - accuracy: 0.9246\n",
      "\n",
      "Epoch 01728: loss did not improve from 0.13953\n",
      "Epoch 1729/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2056 - accuracy: 0.9195\n",
      "\n",
      "Epoch 01729: loss did not improve from 0.13953\n",
      "Epoch 1730/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2025 - accuracy: 0.9141\n",
      "\n",
      "Epoch 01730: loss did not improve from 0.13953\n",
      "Epoch 1731/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2413 - accuracy: 0.9017\n",
      "\n",
      "Epoch 01731: loss did not improve from 0.13953\n",
      "Epoch 1732/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2564 - accuracy: 0.9003\n",
      "\n",
      "Epoch 01732: loss did not improve from 0.13953\n",
      "Epoch 1733/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2171 - accuracy: 0.9110\n",
      "\n",
      "Epoch 01733: loss did not improve from 0.13953\n",
      "Epoch 1734/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1828 - accuracy: 0.9219\n",
      "\n",
      "Epoch 01734: loss did not improve from 0.13953\n",
      "Epoch 1735/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1693 - accuracy: 0.9296\n",
      "\n",
      "Epoch 01735: loss did not improve from 0.13953\n",
      "Epoch 1736/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1679 - accuracy: 0.9250\n",
      "\n",
      "Epoch 01736: loss did not improve from 0.13953\n",
      "Epoch 1737/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1440 - accuracy: 0.9301\n",
      "\n",
      "Epoch 01737: loss did not improve from 0.13953\n",
      "Epoch 1738/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1429 - accuracy: 0.9321\n",
      "\n",
      "Epoch 01738: loss did not improve from 0.13953\n",
      "Epoch 1739/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1391 - accuracy: 0.9327\n",
      "\n",
      "Epoch 01739: loss did not improve from 0.13953\n",
      "Epoch 1740/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1403 - accuracy: 0.9319\n",
      "\n",
      "Epoch 01740: loss did not improve from 0.13953\n",
      "Epoch 1741/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1418 - accuracy: 0.9271\n",
      "\n",
      "Epoch 01741: loss did not improve from 0.13953\n",
      "Epoch 1742/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1387 - accuracy: 0.9269\n",
      "\n",
      "Epoch 01742: loss did not improve from 0.13953\n",
      "Epoch 1743/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1347 - accuracy: 0.9322\n",
      "\n",
      "Epoch 01743: loss did not improve from 0.13953\n",
      "Epoch 1744/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1314 - accuracy: 0.9355\n",
      "\n",
      "Epoch 01744: loss did not improve from 0.13953\n",
      "Epoch 1745/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1451 - accuracy: 0.9279\n",
      "\n",
      "Epoch 01745: loss improved from 0.13953 to 0.13871, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1746/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1305 - accuracy: 0.9364\n",
      "\n",
      "Epoch 01746: loss did not improve from 0.13871\n",
      "Epoch 1747/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1361 - accuracy: 0.9320\n",
      "\n",
      "Epoch 01747: loss did not improve from 0.13871\n",
      "Epoch 1748/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1377 - accuracy: 0.9307\n",
      "\n",
      "Epoch 01748: loss did not improve from 0.13871\n",
      "Epoch 1749/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1366 - accuracy: 0.9338\n",
      "\n",
      "Epoch 01749: loss did not improve from 0.13871\n",
      "Epoch 1750/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1315 - accuracy: 0.9316\n",
      "\n",
      "Epoch 01750: loss did not improve from 0.13871\n",
      "Epoch 1751/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1336 - accuracy: 0.9313\n",
      "\n",
      "Epoch 01751: loss improved from 0.13871 to 0.13868, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1752/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1383 - accuracy: 0.9333\n",
      "\n",
      "Epoch 01752: loss did not improve from 0.13868\n",
      "Epoch 1753/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1260 - accuracy: 0.9374\n",
      "\n",
      "Epoch 01753: loss did not improve from 0.13868\n",
      "Epoch 1754/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1407 - accuracy: 0.9279\n",
      "\n",
      "Epoch 01754: loss did not improve from 0.13868\n",
      "Epoch 1755/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1395 - accuracy: 0.9296\n",
      "\n",
      "Epoch 01755: loss did not improve from 0.13868\n",
      "Epoch 1756/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1314 - accuracy: 0.9311\n",
      "\n",
      "Epoch 01756: loss did not improve from 0.13868\n",
      "Epoch 1757/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1374 - accuracy: 0.9322\n",
      "\n",
      "Epoch 01757: loss improved from 0.13868 to 0.13855, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1758/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1329 - accuracy: 0.9277\n",
      "\n",
      "Epoch 01758: loss improved from 0.13855 to 0.13782, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1759/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1281 - accuracy: 0.9338\n",
      "\n",
      "Epoch 01759: loss did not improve from 0.13782\n",
      "Epoch 1760/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1348 - accuracy: 0.9321\n",
      "\n",
      "Epoch 01760: loss did not improve from 0.13782\n",
      "Epoch 1761/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1419 - accuracy: 0.9231\n",
      "\n",
      "Epoch 01761: loss did not improve from 0.13782\n",
      "Epoch 1762/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1350 - accuracy: 0.9315\n",
      "\n",
      "Epoch 01762: loss did not improve from 0.13782\n",
      "Epoch 1763/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1306 - accuracy: 0.9374\n",
      "\n",
      "Epoch 01763: loss did not improve from 0.13782\n",
      "Epoch 1764/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1430 - accuracy: 0.9220\n",
      "\n",
      "Epoch 01764: loss did not improve from 0.13782\n",
      "Epoch 1765/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1327 - accuracy: 0.9324\n",
      "\n",
      "Epoch 01765: loss improved from 0.13782 to 0.13734, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1766/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1323 - accuracy: 0.9326\n",
      "\n",
      "Epoch 01766: loss did not improve from 0.13734\n",
      "Epoch 1767/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1385 - accuracy: 0.9323\n",
      "\n",
      "Epoch 01767: loss did not improve from 0.13734\n",
      "Epoch 1768/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1365 - accuracy: 0.9313\n",
      "\n",
      "Epoch 01768: loss did not improve from 0.13734\n",
      "Epoch 1769/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1384 - accuracy: 0.9349\n",
      "\n",
      "Epoch 01769: loss did not improve from 0.13734\n",
      "Epoch 1770/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1420 - accuracy: 0.9356\n",
      "\n",
      "Epoch 01770: loss did not improve from 0.13734\n",
      "Epoch 1771/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1342 - accuracy: 0.9357\n",
      "\n",
      "Epoch 01771: loss did not improve from 0.13734\n",
      "Epoch 1772/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1549 - accuracy: 0.9309\n",
      "\n",
      "Epoch 01772: loss did not improve from 0.13734\n",
      "Epoch 1773/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1844 - accuracy: 0.9210\n",
      "\n",
      "Epoch 01773: loss did not improve from 0.13734\n",
      "Epoch 1774/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2410 - accuracy: 0.9094\n",
      "\n",
      "Epoch 01774: loss did not improve from 0.13734\n",
      "Epoch 1775/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3307 - accuracy: 0.8807\n",
      "\n",
      "Epoch 01775: loss did not improve from 0.13734\n",
      "Epoch 1776/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3888 - accuracy: 0.8641\n",
      "\n",
      "Epoch 01776: loss did not improve from 0.13734\n",
      "Epoch 1777/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3468 - accuracy: 0.8742\n",
      "\n",
      "Epoch 01777: loss did not improve from 0.13734\n",
      "Epoch 1778/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2836 - accuracy: 0.8922\n",
      "\n",
      "Epoch 01778: loss did not improve from 0.13734\n",
      "Epoch 1779/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2194 - accuracy: 0.9135\n",
      "\n",
      "Epoch 01779: loss did not improve from 0.13734\n",
      "Epoch 1780/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1774 - accuracy: 0.9205\n",
      "\n",
      "Epoch 01780: loss did not improve from 0.13734\n",
      "Epoch 1781/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1485 - accuracy: 0.9338\n",
      "\n",
      "Epoch 01781: loss did not improve from 0.13734\n",
      "Epoch 1782/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1306 - accuracy: 0.9422\n",
      "\n",
      "Epoch 01782: loss did not improve from 0.13734\n",
      "Epoch 1783/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1480 - accuracy: 0.9302\n",
      "\n",
      "Epoch 01783: loss did not improve from 0.13734\n",
      "Epoch 1784/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1420 - accuracy: 0.9304\n",
      "\n",
      "Epoch 01784: loss did not improve from 0.13734\n",
      "Epoch 1785/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1341 - accuracy: 0.9329\n",
      "\n",
      "Epoch 01785: loss did not improve from 0.13734\n",
      "Epoch 1786/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1375 - accuracy: 0.9316\n",
      "\n",
      "Epoch 01786: loss did not improve from 0.13734\n",
      "Epoch 1787/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1271 - accuracy: 0.9387\n",
      "\n",
      "Epoch 01787: loss did not improve from 0.13734\n",
      "Epoch 1788/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1295 - accuracy: 0.9323\n",
      "\n",
      "Epoch 01788: loss did not improve from 0.13734\n",
      "Epoch 1789/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1299 - accuracy: 0.9376\n",
      "\n",
      "Epoch 01789: loss did not improve from 0.13734\n",
      "Epoch 1790/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1389 - accuracy: 0.9298\n",
      "\n",
      "Epoch 01790: loss did not improve from 0.13734\n",
      "Epoch 1791/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1340 - accuracy: 0.9347\n",
      "\n",
      "Epoch 01791: loss improved from 0.13734 to 0.13675, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1792/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1341 - accuracy: 0.9319\n",
      "\n",
      "Epoch 01792: loss did not improve from 0.13675\n",
      "Epoch 1793/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1280 - accuracy: 0.9319\n",
      "\n",
      "Epoch 01793: loss improved from 0.13675 to 0.13563, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1794/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1323 - accuracy: 0.9329\n",
      "\n",
      "Epoch 01794: loss did not improve from 0.13563\n",
      "Epoch 1795/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1394 - accuracy: 0.9308\n",
      "\n",
      "Epoch 01795: loss did not improve from 0.13563\n",
      "Epoch 1796/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1306 - accuracy: 0.9361\n",
      "\n",
      "Epoch 01796: loss improved from 0.13563 to 0.13526, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1797/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1288 - accuracy: 0.9311\n",
      "\n",
      "Epoch 01797: loss did not improve from 0.13526\n",
      "Epoch 1798/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1330 - accuracy: 0.9324\n",
      "\n",
      "Epoch 01798: loss did not improve from 0.13526\n",
      "Epoch 1799/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1357 - accuracy: 0.9300\n",
      "\n",
      "Epoch 01799: loss did not improve from 0.13526\n",
      "Epoch 1800/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1369 - accuracy: 0.9257\n",
      "\n",
      "Epoch 01800: loss improved from 0.13526 to 0.13490, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1801/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1316 - accuracy: 0.9333\n",
      "\n",
      "Epoch 01801: loss improved from 0.13490 to 0.13470, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1802/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1338 - accuracy: 0.9265\n",
      "\n",
      "Epoch 01802: loss did not improve from 0.13470\n",
      "Epoch 1803/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1259 - accuracy: 0.9341\n",
      "\n",
      "Epoch 01803: loss did not improve from 0.13470\n",
      "Epoch 1804/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1283 - accuracy: 0.9340\n",
      "\n",
      "Epoch 01804: loss did not improve from 0.13470\n",
      "Epoch 1805/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1259 - accuracy: 0.9373\n",
      "\n",
      "Epoch 01805: loss improved from 0.13470 to 0.13436, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1806/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1324 - accuracy: 0.9318\n",
      "\n",
      "Epoch 01806: loss did not improve from 0.13436\n",
      "Epoch 1807/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1222 - accuracy: 0.9381\n",
      "\n",
      "Epoch 01807: loss did not improve from 0.13436\n",
      "Epoch 1808/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1326 - accuracy: 0.9332\n",
      "\n",
      "Epoch 01808: loss did not improve from 0.13436\n",
      "Epoch 1809/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1312 - accuracy: 0.9315\n",
      "\n",
      "Epoch 01809: loss did not improve from 0.13436\n",
      "Epoch 1810/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1265 - accuracy: 0.9360\n",
      "\n",
      "Epoch 01810: loss improved from 0.13436 to 0.13399, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1811/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1343 - accuracy: 0.9286\n",
      "\n",
      "Epoch 01811: loss did not improve from 0.13399\n",
      "Epoch 1812/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1249 - accuracy: 0.9376\n",
      "\n",
      "Epoch 01812: loss improved from 0.13399 to 0.13332, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1813/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1350 - accuracy: 0.9295\n",
      "\n",
      "Epoch 01813: loss did not improve from 0.13332\n",
      "Epoch 1814/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1346 - accuracy: 0.9277\n",
      "\n",
      "Epoch 01814: loss did not improve from 0.13332\n",
      "Epoch 1815/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1304 - accuracy: 0.9386\n",
      "\n",
      "Epoch 01815: loss did not improve from 0.13332\n",
      "Epoch 1816/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1344 - accuracy: 0.9325\n",
      "\n",
      "Epoch 01816: loss did not improve from 0.13332\n",
      "Epoch 1817/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1294 - accuracy: 0.9344\n",
      "\n",
      "Epoch 01817: loss did not improve from 0.13332\n",
      "Epoch 1818/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1291 - accuracy: 0.9350\n",
      "\n",
      "Epoch 01818: loss did not improve from 0.13332\n",
      "Epoch 1819/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1360 - accuracy: 0.9263\n",
      "\n",
      "Epoch 01819: loss did not improve from 0.13332\n",
      "Epoch 1820/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1312 - accuracy: 0.9366\n",
      "\n",
      "Epoch 01820: loss did not improve from 0.13332\n",
      "Epoch 1821/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1350 - accuracy: 0.9351\n",
      "\n",
      "Epoch 01821: loss did not improve from 0.13332\n",
      "Epoch 1822/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1416 - accuracy: 0.9300\n",
      "\n",
      "Epoch 01822: loss did not improve from 0.13332\n",
      "Epoch 1823/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1504 - accuracy: 0.9166\n",
      "\n",
      "Epoch 01823: loss did not improve from 0.13332\n",
      "Epoch 1824/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1350 - accuracy: 0.9359\n",
      "\n",
      "Epoch 01824: loss did not improve from 0.13332\n",
      "Epoch 1825/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1353 - accuracy: 0.9281\n",
      "\n",
      "Epoch 01825: loss did not improve from 0.13332\n",
      "Epoch 1826/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1453 - accuracy: 0.9259\n",
      "\n",
      "Epoch 01826: loss did not improve from 0.13332\n",
      "Epoch 1827/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1503 - accuracy: 0.9329\n",
      "\n",
      "Epoch 01827: loss did not improve from 0.13332\n",
      "Epoch 1828/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1398 - accuracy: 0.9330\n",
      "\n",
      "Epoch 01828: loss did not improve from 0.13332\n",
      "Epoch 1829/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1546 - accuracy: 0.9285\n",
      "\n",
      "Epoch 01829: loss did not improve from 0.13332\n",
      "Epoch 1830/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1717 - accuracy: 0.9230\n",
      "\n",
      "Epoch 01830: loss did not improve from 0.13332\n",
      "Epoch 1831/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2400 - accuracy: 0.9008\n",
      "\n",
      "Epoch 01831: loss did not improve from 0.13332\n",
      "Epoch 1832/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2388 - accuracy: 0.9053\n",
      "\n",
      "Epoch 01832: loss did not improve from 0.13332\n",
      "Epoch 1833/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2022 - accuracy: 0.9160\n",
      "\n",
      "Epoch 01833: loss did not improve from 0.13332\n",
      "Epoch 1834/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1879 - accuracy: 0.9252\n",
      "\n",
      "Epoch 01834: loss did not improve from 0.13332\n",
      "Epoch 1835/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1780 - accuracy: 0.9175\n",
      "\n",
      "Epoch 01835: loss did not improve from 0.13332\n",
      "Epoch 1836/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1549 - accuracy: 0.9291\n",
      "\n",
      "Epoch 01836: loss did not improve from 0.13332\n",
      "Epoch 1837/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1486 - accuracy: 0.9321\n",
      "\n",
      "Epoch 01837: loss did not improve from 0.13332\n",
      "Epoch 1838/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1377 - accuracy: 0.9335\n",
      "\n",
      "Epoch 01838: loss did not improve from 0.13332\n",
      "Epoch 1839/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1385 - accuracy: 0.9316\n",
      "\n",
      "Epoch 01839: loss did not improve from 0.13332\n",
      "Epoch 1840/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1342 - accuracy: 0.9306\n",
      "\n",
      "Epoch 01840: loss did not improve from 0.13332\n",
      "Epoch 1841/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1259 - accuracy: 0.9333\n",
      "\n",
      "Epoch 01841: loss did not improve from 0.13332\n",
      "Epoch 1842/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1351 - accuracy: 0.9299\n",
      "\n",
      "Epoch 01842: loss did not improve from 0.13332\n",
      "Epoch 1843/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1377 - accuracy: 0.9290\n",
      "\n",
      "Epoch 01843: loss did not improve from 0.13332\n",
      "Epoch 1844/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1369 - accuracy: 0.9288\n",
      "\n",
      "Epoch 01844: loss did not improve from 0.13332\n",
      "Epoch 1845/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1269 - accuracy: 0.9369\n",
      "\n",
      "Epoch 01845: loss did not improve from 0.13332\n",
      "Epoch 1846/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1253 - accuracy: 0.9354\n",
      "\n",
      "Epoch 01846: loss improved from 0.13332 to 0.13309, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1847/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1269 - accuracy: 0.9339\n",
      "\n",
      "Epoch 01847: loss did not improve from 0.13309\n",
      "Epoch 1848/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1277 - accuracy: 0.9384\n",
      "\n",
      "Epoch 01848: loss did not improve from 0.13309\n",
      "Epoch 1849/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1439 - accuracy: 0.9306\n",
      "\n",
      "Epoch 01849: loss did not improve from 0.13309\n",
      "Epoch 1850/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1366 - accuracy: 0.9293\n",
      "\n",
      "Epoch 01850: loss did not improve from 0.13309\n",
      "Epoch 1851/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1369 - accuracy: 0.9306\n",
      "\n",
      "Epoch 01851: loss did not improve from 0.13309\n",
      "Epoch 1852/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1332 - accuracy: 0.9313\n",
      "\n",
      "Epoch 01852: loss did not improve from 0.13309\n",
      "Epoch 1853/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1314 - accuracy: 0.9330\n",
      "\n",
      "Epoch 01853: loss did not improve from 0.13309\n",
      "Epoch 1854/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1243 - accuracy: 0.9376\n",
      "\n",
      "Epoch 01854: loss improved from 0.13309 to 0.13296, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1855/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1314 - accuracy: 0.9321\n",
      "\n",
      "Epoch 01855: loss did not improve from 0.13296\n",
      "Epoch 1856/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1279 - accuracy: 0.9338\n",
      "\n",
      "Epoch 01856: loss did not improve from 0.13296\n",
      "Epoch 1857/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1353 - accuracy: 0.9305\n",
      "\n",
      "Epoch 01857: loss did not improve from 0.13296\n",
      "Epoch 1858/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1272 - accuracy: 0.9353\n",
      "\n",
      "Epoch 01858: loss did not improve from 0.13296\n",
      "Epoch 1859/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1285 - accuracy: 0.9300\n",
      "\n",
      "Epoch 01859: loss did not improve from 0.13296\n",
      "Epoch 1860/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1336 - accuracy: 0.9300\n",
      "\n",
      "Epoch 01860: loss did not improve from 0.13296\n",
      "Epoch 1861/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1318 - accuracy: 0.9378\n",
      "\n",
      "Epoch 01861: loss did not improve from 0.13296\n",
      "Epoch 1862/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1967 - accuracy: 0.9123\n",
      "\n",
      "Epoch 01862: loss did not improve from 0.13296\n",
      "Epoch 1863/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1924 - accuracy: 0.9187\n",
      "\n",
      "Epoch 01863: loss did not improve from 0.13296\n",
      "Epoch 1864/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1653 - accuracy: 0.9250\n",
      "\n",
      "Epoch 01864: loss did not improve from 0.13296\n",
      "Epoch 1865/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1743 - accuracy: 0.9240\n",
      "\n",
      "Epoch 01865: loss did not improve from 0.13296\n",
      "Epoch 1866/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1717 - accuracy: 0.9243\n",
      "\n",
      "Epoch 01866: loss did not improve from 0.13296\n",
      "Epoch 1867/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1781 - accuracy: 0.9231\n",
      "\n",
      "Epoch 01867: loss did not improve from 0.13296\n",
      "Epoch 1868/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1796 - accuracy: 0.9194\n",
      "\n",
      "Epoch 01868: loss did not improve from 0.13296\n",
      "Epoch 1869/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1620 - accuracy: 0.9293\n",
      "\n",
      "Epoch 01869: loss did not improve from 0.13296\n",
      "Epoch 1870/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1419 - accuracy: 0.9344\n",
      "\n",
      "Epoch 01870: loss did not improve from 0.13296\n",
      "Epoch 1871/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1409 - accuracy: 0.9306\n",
      "\n",
      "Epoch 01871: loss did not improve from 0.13296\n",
      "Epoch 1872/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1339 - accuracy: 0.9304\n",
      "\n",
      "Epoch 01872: loss did not improve from 0.13296\n",
      "Epoch 1873/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1306 - accuracy: 0.9306\n",
      "\n",
      "Epoch 01873: loss did not improve from 0.13296\n",
      "Epoch 1874/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1359 - accuracy: 0.9295\n",
      "\n",
      "Epoch 01874: loss did not improve from 0.13296\n",
      "Epoch 1875/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1353 - accuracy: 0.9284\n",
      "\n",
      "Epoch 01875: loss did not improve from 0.13296\n",
      "Epoch 1876/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1312 - accuracy: 0.9286\n",
      "\n",
      "Epoch 01876: loss did not improve from 0.13296\n",
      "Epoch 1877/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1279 - accuracy: 0.9334\n",
      "\n",
      "Epoch 01877: loss did not improve from 0.13296\n",
      "Epoch 1878/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1303 - accuracy: 0.9347\n",
      "\n",
      "Epoch 01878: loss did not improve from 0.13296\n",
      "Epoch 1879/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1384 - accuracy: 0.9299\n",
      "\n",
      "Epoch 01879: loss did not improve from 0.13296\n",
      "Epoch 1880/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1250 - accuracy: 0.9371\n",
      "\n",
      "Epoch 01880: loss improved from 0.13296 to 0.13180, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1881/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1259 - accuracy: 0.9351\n",
      "\n",
      "Epoch 01881: loss did not improve from 0.13180\n",
      "Epoch 1882/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1325 - accuracy: 0.9320\n",
      "\n",
      "Epoch 01882: loss did not improve from 0.13180\n",
      "Epoch 1883/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1302 - accuracy: 0.9335\n",
      "\n",
      "Epoch 01883: loss did not improve from 0.13180\n",
      "Epoch 1884/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1303 - accuracy: 0.9331\n",
      "\n",
      "Epoch 01884: loss did not improve from 0.13180\n",
      "Epoch 1885/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1279 - accuracy: 0.9362\n",
      "\n",
      "Epoch 01885: loss did not improve from 0.13180\n",
      "Epoch 1886/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1341 - accuracy: 0.9295\n",
      "\n",
      "Epoch 01886: loss did not improve from 0.13180\n",
      "Epoch 1887/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1279 - accuracy: 0.9325\n",
      "\n",
      "Epoch 01887: loss did not improve from 0.13180\n",
      "Epoch 1888/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1303 - accuracy: 0.9291\n",
      "\n",
      "Epoch 01888: loss did not improve from 0.13180\n",
      "Epoch 1889/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1237 - accuracy: 0.9313\n",
      "\n",
      "Epoch 01889: loss did not improve from 0.13180\n",
      "Epoch 1890/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1335 - accuracy: 0.9305\n",
      "\n",
      "Epoch 01890: loss did not improve from 0.13180\n",
      "Epoch 1891/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1311 - accuracy: 0.9292\n",
      "\n",
      "Epoch 01891: loss did not improve from 0.13180\n",
      "Epoch 1892/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1220 - accuracy: 0.9398\n",
      "\n",
      "Epoch 01892: loss did not improve from 0.13180\n",
      "Epoch 1893/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1288 - accuracy: 0.9316\n",
      "\n",
      "Epoch 01893: loss did not improve from 0.13180\n",
      "Epoch 1894/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1235 - accuracy: 0.9334\n",
      "\n",
      "Epoch 01894: loss did not improve from 0.13180\n",
      "Epoch 1895/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1320 - accuracy: 0.9273\n",
      "\n",
      "Epoch 01895: loss did not improve from 0.13180\n",
      "Epoch 1896/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1296 - accuracy: 0.9343\n",
      "\n",
      "Epoch 01896: loss did not improve from 0.13180\n",
      "Epoch 1897/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1344 - accuracy: 0.9270\n",
      "\n",
      "Epoch 01897: loss did not improve from 0.13180\n",
      "Epoch 1898/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1271 - accuracy: 0.9371\n",
      "\n",
      "Epoch 01898: loss did not improve from 0.13180\n",
      "Epoch 1899/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1345 - accuracy: 0.9322\n",
      "\n",
      "Epoch 01899: loss did not improve from 0.13180\n",
      "Epoch 1900/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1335 - accuracy: 0.9314\n",
      "\n",
      "Epoch 01900: loss did not improve from 0.13180\n",
      "Epoch 1901/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1247 - accuracy: 0.9321\n",
      "\n",
      "Epoch 01901: loss did not improve from 0.13180\n",
      "Epoch 1902/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1327 - accuracy: 0.9357\n",
      "\n",
      "Epoch 01902: loss did not improve from 0.13180\n",
      "Epoch 1903/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1473 - accuracy: 0.9268\n",
      "\n",
      "Epoch 01903: loss did not improve from 0.13180\n",
      "Epoch 1904/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1427 - accuracy: 0.9292\n",
      "\n",
      "Epoch 01904: loss did not improve from 0.13180\n",
      "Epoch 1905/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1394 - accuracy: 0.9356\n",
      "\n",
      "Epoch 01905: loss did not improve from 0.13180\n",
      "Epoch 1906/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2049 - accuracy: 0.9137\n",
      "\n",
      "Epoch 01906: loss did not improve from 0.13180\n",
      "Epoch 1907/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3146 - accuracy: 0.8815\n",
      "\n",
      "Epoch 01907: loss did not improve from 0.13180\n",
      "Epoch 1908/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3742 - accuracy: 0.8622\n",
      "\n",
      "Epoch 01908: loss did not improve from 0.13180\n",
      "Epoch 1909/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2907 - accuracy: 0.8869\n",
      "\n",
      "Epoch 01909: loss did not improve from 0.13180\n",
      "Epoch 1910/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2570 - accuracy: 0.8980\n",
      "\n",
      "Epoch 01910: loss did not improve from 0.13180\n",
      "Epoch 1911/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2101 - accuracy: 0.9151\n",
      "\n",
      "Epoch 01911: loss did not improve from 0.13180\n",
      "Epoch 1912/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1598 - accuracy: 0.9274\n",
      "\n",
      "Epoch 01912: loss did not improve from 0.13180\n",
      "Epoch 1913/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1516 - accuracy: 0.9280\n",
      "\n",
      "Epoch 01913: loss did not improve from 0.13180\n",
      "Epoch 1914/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1386 - accuracy: 0.9337\n",
      "\n",
      "Epoch 01914: loss did not improve from 0.13180\n",
      "Epoch 1915/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1321 - accuracy: 0.9327\n",
      "\n",
      "Epoch 01915: loss did not improve from 0.13180\n",
      "Epoch 1916/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1249 - accuracy: 0.9393\n",
      "\n",
      "Epoch 01916: loss did not improve from 0.13180\n",
      "Epoch 1917/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1229 - accuracy: 0.9389\n",
      "\n",
      "Epoch 01917: loss did not improve from 0.13180\n",
      "Epoch 1918/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1313 - accuracy: 0.9306\n",
      "\n",
      "Epoch 01918: loss did not improve from 0.13180\n",
      "Epoch 1919/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1300 - accuracy: 0.9305\n",
      "\n",
      "Epoch 01919: loss did not improve from 0.13180\n",
      "Epoch 1920/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1241 - accuracy: 0.9358\n",
      "\n",
      "Epoch 01920: loss did not improve from 0.13180\n",
      "Epoch 1921/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1311 - accuracy: 0.9302\n",
      "\n",
      "Epoch 01921: loss improved from 0.13180 to 0.13158, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1922/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1275 - accuracy: 0.9314\n",
      "\n",
      "Epoch 01922: loss did not improve from 0.13158\n",
      "Epoch 1923/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1270 - accuracy: 0.9331\n",
      "\n",
      "Epoch 01923: loss improved from 0.13158 to 0.13121, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1924/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1225 - accuracy: 0.9354\n",
      "\n",
      "Epoch 01924: loss did not improve from 0.13121\n",
      "Epoch 1925/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1295 - accuracy: 0.9349\n",
      "\n",
      "Epoch 01925: loss did not improve from 0.13121\n",
      "Epoch 1926/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1304 - accuracy: 0.9337\n",
      "\n",
      "Epoch 01926: loss did not improve from 0.13121\n",
      "Epoch 1927/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1301 - accuracy: 0.9303\n",
      "\n",
      "Epoch 01927: loss improved from 0.13121 to 0.13083, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1928/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1227 - accuracy: 0.9341\n",
      "\n",
      "Epoch 01928: loss improved from 0.13083 to 0.13037, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1929/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1223 - accuracy: 0.9342\n",
      "\n",
      "Epoch 01929: loss did not improve from 0.13037\n",
      "Epoch 1930/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1255 - accuracy: 0.9331\n",
      "\n",
      "Epoch 01930: loss improved from 0.13037 to 0.12978, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1931/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1275 - accuracy: 0.9317\n",
      "\n",
      "Epoch 01931: loss did not improve from 0.12978\n",
      "Epoch 1932/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1271 - accuracy: 0.9322\n",
      "\n",
      "Epoch 01932: loss did not improve from 0.12978\n",
      "Epoch 1933/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1240 - accuracy: 0.9342\n",
      "\n",
      "Epoch 01933: loss did not improve from 0.12978\n",
      "Epoch 1934/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1197 - accuracy: 0.9363\n",
      "\n",
      "Epoch 01934: loss improved from 0.12978 to 0.12943, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1935/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1297 - accuracy: 0.9299\n",
      "\n",
      "Epoch 01935: loss did not improve from 0.12943\n",
      "Epoch 1936/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1299 - accuracy: 0.9323\n",
      "\n",
      "Epoch 01936: loss did not improve from 0.12943\n",
      "Epoch 1937/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1261 - accuracy: 0.9317\n",
      "\n",
      "Epoch 01937: loss did not improve from 0.12943\n",
      "Epoch 1938/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1273 - accuracy: 0.9349\n",
      "\n",
      "Epoch 01938: loss did not improve from 0.12943\n",
      "Epoch 1939/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1308 - accuracy: 0.9275\n",
      "\n",
      "Epoch 01939: loss did not improve from 0.12943\n",
      "Epoch 1940/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1249 - accuracy: 0.9324\n",
      "\n",
      "Epoch 01940: loss did not improve from 0.12943\n",
      "Epoch 1941/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1349 - accuracy: 0.9303\n",
      "\n",
      "Epoch 01941: loss did not improve from 0.12943\n",
      "Epoch 1942/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1294 - accuracy: 0.9293\n",
      "\n",
      "Epoch 01942: loss did not improve from 0.12943\n",
      "Epoch 1943/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1206 - accuracy: 0.9343\n",
      "\n",
      "Epoch 01943: loss improved from 0.12943 to 0.12830, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1944/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1328 - accuracy: 0.9291\n",
      "\n",
      "Epoch 01944: loss did not improve from 0.12830\n",
      "Epoch 1945/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1268 - accuracy: 0.9311\n",
      "\n",
      "Epoch 01945: loss did not improve from 0.12830\n",
      "Epoch 1946/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1262 - accuracy: 0.9333\n",
      "\n",
      "Epoch 01946: loss did not improve from 0.12830\n",
      "Epoch 1947/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1279 - accuracy: 0.9319\n",
      "\n",
      "Epoch 01947: loss did not improve from 0.12830\n",
      "Epoch 1948/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1332 - accuracy: 0.9260\n",
      "\n",
      "Epoch 01948: loss did not improve from 0.12830\n",
      "Epoch 1949/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1287 - accuracy: 0.9334\n",
      "\n",
      "Epoch 01949: loss did not improve from 0.12830\n",
      "Epoch 1950/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1280 - accuracy: 0.9302\n",
      "\n",
      "Epoch 01950: loss did not improve from 0.12830\n",
      "Epoch 1951/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1252 - accuracy: 0.9355\n",
      "\n",
      "Epoch 01951: loss did not improve from 0.12830\n",
      "Epoch 1952/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1260 - accuracy: 0.9373\n",
      "\n",
      "Epoch 01952: loss did not improve from 0.12830\n",
      "Epoch 1953/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1240 - accuracy: 0.9355\n",
      "\n",
      "Epoch 01953: loss did not improve from 0.12830\n",
      "Epoch 1954/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1267 - accuracy: 0.9357\n",
      "\n",
      "Epoch 01954: loss did not improve from 0.12830\n",
      "Epoch 1955/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1333 - accuracy: 0.9326\n",
      "\n",
      "Epoch 01955: loss did not improve from 0.12830\n",
      "Epoch 1956/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1368 - accuracy: 0.9277\n",
      "\n",
      "Epoch 01956: loss did not improve from 0.12830\n",
      "Epoch 1957/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1367 - accuracy: 0.9295\n",
      "\n",
      "Epoch 01957: loss did not improve from 0.12830\n",
      "Epoch 1958/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1312 - accuracy: 0.9285\n",
      "\n",
      "Epoch 01958: loss did not improve from 0.12830\n",
      "Epoch 1959/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1339 - accuracy: 0.9303\n",
      "\n",
      "Epoch 01959: loss did not improve from 0.12830\n",
      "Epoch 1960/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1339 - accuracy: 0.9306\n",
      "\n",
      "Epoch 01960: loss did not improve from 0.12830\n",
      "Epoch 1961/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1477 - accuracy: 0.9311\n",
      "\n",
      "Epoch 01961: loss did not improve from 0.12830\n",
      "Epoch 1962/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1728 - accuracy: 0.9228\n",
      "\n",
      "Epoch 01962: loss did not improve from 0.12830\n",
      "Epoch 1963/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2018 - accuracy: 0.9117\n",
      "\n",
      "Epoch 01963: loss did not improve from 0.12830\n",
      "Epoch 1964/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3732 - accuracy: 0.8695\n",
      "\n",
      "Epoch 01964: loss did not improve from 0.12830\n",
      "Epoch 1965/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.4293 - accuracy: 0.8556\n",
      "\n",
      "Epoch 01965: loss did not improve from 0.12830\n",
      "Epoch 1966/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.3254 - accuracy: 0.8815\n",
      "\n",
      "Epoch 01966: loss did not improve from 0.12830\n",
      "Epoch 1967/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.2586 - accuracy: 0.9004\n",
      "\n",
      "Epoch 01967: loss did not improve from 0.12830\n",
      "Epoch 1968/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.2123 - accuracy: 0.9124\n",
      "\n",
      "Epoch 01968: loss did not improve from 0.12830\n",
      "Epoch 1969/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1653 - accuracy: 0.9252\n",
      "\n",
      "Epoch 01969: loss did not improve from 0.12830\n",
      "Epoch 1970/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1552 - accuracy: 0.9263\n",
      "\n",
      "Epoch 01970: loss did not improve from 0.12830\n",
      "Epoch 1971/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1400 - accuracy: 0.9277\n",
      "\n",
      "Epoch 01971: loss did not improve from 0.12830\n",
      "Epoch 1972/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1378 - accuracy: 0.9304\n",
      "\n",
      "Epoch 01972: loss did not improve from 0.12830\n",
      "Epoch 1973/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1308 - accuracy: 0.9349\n",
      "\n",
      "Epoch 01973: loss did not improve from 0.12830\n",
      "Epoch 1974/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1268 - accuracy: 0.9355\n",
      "\n",
      "Epoch 01974: loss did not improve from 0.12830\n",
      "Epoch 1975/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1268 - accuracy: 0.9332\n",
      "\n",
      "Epoch 01975: loss did not improve from 0.12830\n",
      "Epoch 1976/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1280 - accuracy: 0.9310\n",
      "\n",
      "Epoch 01976: loss did not improve from 0.12830\n",
      "Epoch 1977/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1214 - accuracy: 0.9397\n",
      "\n",
      "Epoch 01977: loss did not improve from 0.12830\n",
      "Epoch 1978/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1298 - accuracy: 0.9299\n",
      "\n",
      "Epoch 01978: loss did not improve from 0.12830\n",
      "Epoch 1979/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1263 - accuracy: 0.9368\n",
      "\n",
      "Epoch 01979: loss did not improve from 0.12830\n",
      "Epoch 1980/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1297 - accuracy: 0.9340\n",
      "\n",
      "Epoch 01980: loss did not improve from 0.12830\n",
      "Epoch 1981/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1203 - accuracy: 0.9349\n",
      "\n",
      "Epoch 01981: loss did not improve from 0.12830\n",
      "Epoch 1982/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1259 - accuracy: 0.9334\n",
      "\n",
      "Epoch 01982: loss did not improve from 0.12830\n",
      "Epoch 1983/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1347 - accuracy: 0.9312\n",
      "\n",
      "Epoch 01983: loss did not improve from 0.12830\n",
      "Epoch 1984/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1262 - accuracy: 0.9315\n",
      "\n",
      "Epoch 01984: loss did not improve from 0.12830\n",
      "Epoch 1985/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1198 - accuracy: 0.9336\n",
      "\n",
      "Epoch 01985: loss did not improve from 0.12830\n",
      "Epoch 1986/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1315 - accuracy: 0.9302\n",
      "\n",
      "Epoch 01986: loss did not improve from 0.12830\n",
      "Epoch 1987/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1198 - accuracy: 0.9368\n",
      "\n",
      "Epoch 01987: loss did not improve from 0.12830\n",
      "Epoch 1988/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1233 - accuracy: 0.9337\n",
      "\n",
      "Epoch 01988: loss did not improve from 0.12830\n",
      "Epoch 1989/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1253 - accuracy: 0.9305\n",
      "\n",
      "Epoch 01989: loss improved from 0.12830 to 0.12780, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1990/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1253 - accuracy: 0.9329\n",
      "\n",
      "Epoch 01990: loss improved from 0.12780 to 0.12746, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1991/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1225 - accuracy: 0.9346\n",
      "\n",
      "Epoch 01991: loss improved from 0.12746 to 0.12694, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1992/2000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 0.1231 - accuracy: 0.9345\n",
      "\n",
      "Epoch 01992: loss did not improve from 0.12694\n",
      "Epoch 1993/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1200 - accuracy: 0.9366\n",
      "\n",
      "Epoch 01993: loss did not improve from 0.12694\n",
      "Epoch 1994/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1279 - accuracy: 0.9288\n",
      "\n",
      "Epoch 01994: loss did not improve from 0.12694\n",
      "Epoch 1995/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1207 - accuracy: 0.9368\n",
      "\n",
      "Epoch 01995: loss did not improve from 0.12694\n",
      "Epoch 1996/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1201 - accuracy: 0.9372\n",
      "\n",
      "Epoch 01996: loss did not improve from 0.12694\n",
      "Epoch 1997/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1244 - accuracy: 0.9343\n",
      "\n",
      "Epoch 01997: loss did not improve from 0.12694\n",
      "Epoch 1998/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1321 - accuracy: 0.9284\n",
      "\n",
      "Epoch 01998: loss did not improve from 0.12694\n",
      "Epoch 1999/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1245 - accuracy: 0.9323\n",
      "\n",
      "Epoch 01999: loss did not improve from 0.12694\n",
      "Epoch 2000/2000\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.1234 - accuracy: 0.9341\n",
      "\n",
      "Epoch 02000: loss did not improve from 0.12694\n"
     ]
    }
   ],
   "source": [
    "### Model checkpoint\n",
    "ModelSaveSameName = save_path+'10LetterWordM.hdf5'\n",
    "ModelSave = ModelCheckpoint(filepath=ModelSaveSameName, monitor='loss', verbose=1, save_best_only=True)\n",
    "\n",
    "### Model Early stop\n",
    "EarlyStop = EarlyStopping(monitor='loss', patience=100)\n",
    "\n",
    "WordsM = WordsModel()\n",
    "WordsM.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam', metrics =['accuracy'])\n",
    "WordsMHist = WordsM.fit(InpData, TargetData,  epochs=2000, batch_size=100,  verbose=1, callbacks=[ModelSave, EarlyStop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed39eb0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "589abdf6",
   "metadata": {},
   "source": [
    "### Model weight load and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50900133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading weights\n",
      "9/9 [==============================] - 1s 3ms/step - loss: 3.2578 - accuracy: 0.0406\n",
      "[3.257826089859009, 0.038841258734464645]\n",
      "\n",
      "After loading weights\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1203 - accuracy: 0.9378\n",
      "[0.12028944492340088, 0.9377760291099548]\n"
     ]
    }
   ],
   "source": [
    "WordsM = WordsModel()\n",
    "WordsM.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam', metrics =['accuracy'])\n",
    "print('Before loading weights')\n",
    "print(WordsM.evaluate(InpData, TargetData, batch_size=300 ))\n",
    "print()\n",
    "\n",
    "WordsM.load_weights(ModelSaveSameName)\n",
    "print('After loading weights')\n",
    "print(WordsM.evaluate(InpData, TargetData, batch_size=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d23e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91ff0d80",
   "metadata": {},
   "source": [
    "### Plot loss graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dd9bee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(WordsMHist.history['loss'], label='Tr', marker='.')\n",
    "plt.legend() \n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010cb5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c957692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abbreviate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abdication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aberration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abjuration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abnegation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0  abbreviate\n",
       "1  abdication\n",
       "2  aberration\n",
       "3  abjuration\n",
       "4  abnegation"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa14661",
   "metadata": {},
   "source": [
    "### Prediction example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cb9c90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ascdfxz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AlphaInp = input().upper()\n",
    "assert len(AlphaInp)==7, 'Please enter exactly three alphabets'\n",
    "AlphaInpEncod = [AlphOneHot.values[char_to_int[i]][None] for i in AlphaInp ]\n",
    "AlphaInpEncod = np.concatenate(AlphaInpEncod, axis=0)\n",
    "AlphaPred = WordsM.predict(AlphaInpEncod[None])\n",
    "\n",
    "plt.figure(figsize=(18,4))    \n",
    "sns.heatmap(AlphaPred[0], xticklabels=[i for i in char_to_int], annot=np.round(AlphaPred[0], 2),linewidths=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1a07d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
