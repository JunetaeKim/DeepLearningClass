{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c788ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Input, Dense, TimeDistributed, RepeatVector, SimpleRNN\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98df7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d5c7ba1",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afef7ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/JunetaeKim/DeepLearningClass/main/Dataset/10LetterWords.txt'\n",
    "Dataset = pd.read_csv(url, header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12de1196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abbreviate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abdication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aberration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abjuration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abnegation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>wrongdoing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>xenophobia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>xerography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>yesteryear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>zoological</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "0     abbreviate\n",
       "1     abdication\n",
       "2     aberration\n",
       "3     abjuration\n",
       "4     abnegation\n",
       "...          ...\n",
       "2561  wrongdoing\n",
       "2562  xenophobia\n",
       "2563  xerography\n",
       "2564  yesteryear\n",
       "2565  zoological\n",
       "\n",
       "[2566 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79237a80",
   "metadata": {},
   "source": [
    "### Alphabet table generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58f69527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# one-hot encoding for alphabets\n",
    "AlphTab = pd.DataFrame(char_to_int,index = [0]).T\n",
    "AlphOneHot =  pd.get_dummies(AlphTab[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac585a37",
   "metadata": {},
   "source": [
    "### Data encoding; alphabet to numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad1da166",
   "metadata": {},
   "outputs": [],
   "source": [
    "AlphaEncode = []\n",
    "for word in Dataset.values:\n",
    "    SubVec = []\n",
    "    for char in word[0]:\n",
    "        CharIDX = char_to_int[char.upper()]\n",
    "        SubVec.append(AlphOneHot.iloc[CharIDX].values[None])\n",
    "    SubVec = np.concatenate(SubVec, axis=0)\n",
    "    AlphaEncode.append(SubVec[None])\n",
    "AlphaEncode = np.concatenate(AlphaEncode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf35522",
   "metadata": {},
   "source": [
    "### Data selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d11e87e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data selection for input and output \n",
    "InpData = AlphaEncode[:,:7, :] # Five input alphabets\n",
    "TargetData = AlphaEncode[:,7:, :] # Five output alphabets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118551e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0d19771",
   "metadata": {},
   "source": [
    "### Model save directory setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7a0443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './Results/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7898a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1008604",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c795f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordsModel ():\n",
    "    InpL = Input(shape=(InpData.shape[1], InpData.shape[2]))\n",
    "    LSTML = LSTM(30, return_sequences=True)(InpL)\n",
    "    LSTMOut = LSTM(30, return_sequences=False)(LSTML)\n",
    "\n",
    "    LSTMOut = RepeatVector(TargetData.shape[1])(LSTMOut)\n",
    "    Output = TimeDistributed(Dense(30))(LSTMOut)\n",
    "    Output = LSTM(30, return_sequences=True)(Output)\n",
    "    Output = Dense(InpData.shape[2], activation='softmax')(Output)\n",
    "    return Model(InpL,Output)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ace4d56",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "26/26 [==============================] - 3s 7ms/step - loss: 3.2361 - accuracy: 0.0957\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.20217, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 2/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.9308 - accuracy: 0.1222\n",
      "\n",
      "Epoch 00002: loss improved from 3.20217 to 2.86335, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 3/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.7206 - accuracy: 0.2123\n",
      "\n",
      "Epoch 00003: loss improved from 2.86335 to 2.70496, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 4/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 2.6558 - accuracy: 0.2271\n",
      "\n",
      "Epoch 00004: loss improved from 2.70496 to 2.62585, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 5/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 2.5904 - accuracy: 0.2287\n",
      "\n",
      "Epoch 00005: loss improved from 2.62585 to 2.57487, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 6/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 2.5581 - accuracy: 0.2333\n",
      "\n",
      "Epoch 00006: loss improved from 2.57487 to 2.54446, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 7/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.5212 - accuracy: 0.2320\n",
      "\n",
      "Epoch 00007: loss improved from 2.54446 to 2.52268, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 8/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.5033 - accuracy: 0.2353\n",
      "\n",
      "Epoch 00008: loss improved from 2.52268 to 2.50724, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 9/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.5039 - accuracy: 0.2353\n",
      "\n",
      "Epoch 00009: loss improved from 2.50724 to 2.49321, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 10/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.4929 - accuracy: 0.2380\n",
      "\n",
      "Epoch 00010: loss improved from 2.49321 to 2.48494, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 11/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.4779 - accuracy: 0.2395\n",
      "\n",
      "Epoch 00011: loss improved from 2.48494 to 2.47852, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 12/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 2.4791 - accuracy: 0.2375\n",
      "\n",
      "Epoch 00012: loss improved from 2.47852 to 2.47294, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 13/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.4732 - accuracy: 0.2381\n",
      "\n",
      "Epoch 00013: loss improved from 2.47294 to 2.46977, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 14/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 2.4659 - accuracy: 0.2434\n",
      "\n",
      "Epoch 00014: loss improved from 2.46977 to 2.46175, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 15/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 2.4553 - accuracy: 0.2422\n",
      "\n",
      "Epoch 00015: loss improved from 2.46175 to 2.45702, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 16/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 2.4548 - accuracy: 0.2440\n",
      "\n",
      "Epoch 00016: loss improved from 2.45702 to 2.44958, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 17/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 2.4416 - accuracy: 0.2550\n",
      "\n",
      "Epoch 00017: loss improved from 2.44958 to 2.44561, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 18/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.4357 - accuracy: 0.2604\n",
      "\n",
      "Epoch 00018: loss improved from 2.44561 to 2.43088, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 19/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 2.4140 - accuracy: 0.2693\n",
      "\n",
      "Epoch 00019: loss improved from 2.43088 to 2.41611, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 20/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.4245 - accuracy: 0.2704\n",
      "\n",
      "Epoch 00020: loss improved from 2.41611 to 2.40111, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 21/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.3980 - accuracy: 0.2907\n",
      "\n",
      "Epoch 00021: loss improved from 2.40111 to 2.38892, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 22/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.3909 - accuracy: 0.2842\n",
      "\n",
      "Epoch 00022: loss improved from 2.38892 to 2.37405, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 23/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 2.3564 - accuracy: 0.2931\n",
      "\n",
      "Epoch 00023: loss improved from 2.37405 to 2.36293, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 24/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.3530 - accuracy: 0.2944\n",
      "\n",
      "Epoch 00024: loss improved from 2.36293 to 2.35151, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 25/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.3421 - accuracy: 0.3057\n",
      "\n",
      "Epoch 00025: loss improved from 2.35151 to 2.34534, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 26/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.3194 - accuracy: 0.3093\n",
      "\n",
      "Epoch 00026: loss improved from 2.34534 to 2.33409, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 27/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.3167 - accuracy: 0.2985\n",
      "\n",
      "Epoch 00027: loss improved from 2.33409 to 2.32282, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 28/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 2.3000 - accuracy: 0.3042\n",
      "\n",
      "Epoch 00028: loss improved from 2.32282 to 2.31622, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 29/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 2.2968 - accuracy: 0.3054\n",
      "\n",
      "Epoch 00029: loss improved from 2.31622 to 2.30207, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 30/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.2831 - accuracy: 0.3054\n",
      "\n",
      "Epoch 00030: loss improved from 2.30207 to 2.29027, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 31/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.2876 - accuracy: 0.3045\n",
      "\n",
      "Epoch 00031: loss improved from 2.29027 to 2.28069, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 32/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 2.2708 - accuracy: 0.3052\n",
      "\n",
      "Epoch 00032: loss improved from 2.28069 to 2.26856, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 33/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.2841 - accuracy: 0.2959\n",
      "\n",
      "Epoch 00033: loss improved from 2.26856 to 2.25507, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 34/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.2306 - accuracy: 0.3126\n",
      "\n",
      "Epoch 00034: loss improved from 2.25507 to 2.24603, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 35/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.2311 - accuracy: 0.3185\n",
      "\n",
      "Epoch 00035: loss improved from 2.24603 to 2.22770, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 36/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.2204 - accuracy: 0.3207\n",
      "\n",
      "Epoch 00036: loss improved from 2.22770 to 2.22016, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 37/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.2032 - accuracy: 0.3237\n",
      "\n",
      "Epoch 00037: loss improved from 2.22016 to 2.21241, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 38/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.1863 - accuracy: 0.3355\n",
      "\n",
      "Epoch 00038: loss improved from 2.21241 to 2.19939, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 39/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.2074 - accuracy: 0.3272\n",
      "\n",
      "Epoch 00039: loss improved from 2.19939 to 2.18972, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 40/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.1717 - accuracy: 0.3340\n",
      "\n",
      "Epoch 00040: loss improved from 2.18972 to 2.17893, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 41/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.1789 - accuracy: 0.3306\n",
      "\n",
      "Epoch 00041: loss improved from 2.17893 to 2.17090, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 42/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 2.1812 - accuracy: 0.3348\n",
      "\n",
      "Epoch 00042: loss improved from 2.17090 to 2.16560, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 43/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.1435 - accuracy: 0.3476\n",
      "\n",
      "Epoch 00043: loss improved from 2.16560 to 2.16147, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 44/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 2.1440 - accuracy: 0.3370\n",
      "\n",
      "Epoch 00044: loss improved from 2.16147 to 2.15119, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 45/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.1523 - accuracy: 0.3325\n",
      "\n",
      "Epoch 00045: loss improved from 2.15119 to 2.14420, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 46/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.1253 - accuracy: 0.3470\n",
      "\n",
      "Epoch 00046: loss improved from 2.14420 to 2.14130, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 47/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 2.1427 - accuracy: 0.3425\n",
      "\n",
      "Epoch 00047: loss improved from 2.14130 to 2.13258, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 48/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.1100 - accuracy: 0.3474\n",
      "\n",
      "Epoch 00048: loss improved from 2.13258 to 2.12799, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 49/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 2.1266 - accuracy: 0.3408\n",
      "\n",
      "Epoch 00049: loss did not improve from 2.12799\n",
      "Epoch 50/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 2.0768 - accuracy: 0.3581\n",
      "\n",
      "Epoch 00050: loss improved from 2.12799 to 2.11961, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 51/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 2.1017 - accuracy: 0.3453\n",
      "\n",
      "Epoch 00051: loss improved from 2.11961 to 2.11437, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 52/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.1195 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00052: loss improved from 2.11437 to 2.10810, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 53/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 2.0932 - accuracy: 0.3552\n",
      "\n",
      "Epoch 00053: loss improved from 2.10810 to 2.10447, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 54/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 2.0983 - accuracy: 0.3521\n",
      "\n",
      "Epoch 00054: loss improved from 2.10447 to 2.09689, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 55/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.1062 - accuracy: 0.3457\n",
      "\n",
      "Epoch 00055: loss improved from 2.09689 to 2.09249, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 56/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0742 - accuracy: 0.3527\n",
      "\n",
      "Epoch 00056: loss improved from 2.09249 to 2.08854, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 57/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0678 - accuracy: 0.3517\n",
      "\n",
      "Epoch 00057: loss improved from 2.08854 to 2.08420, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 58/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0912 - accuracy: 0.3465\n",
      "\n",
      "Epoch 00058: loss improved from 2.08420 to 2.08258, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 59/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0731 - accuracy: 0.3520\n",
      "\n",
      "Epoch 00059: loss improved from 2.08258 to 2.07421, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 60/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0829 - accuracy: 0.3504\n",
      "\n",
      "Epoch 00060: loss improved from 2.07421 to 2.07295, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 61/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0828 - accuracy: 0.3472\n",
      "\n",
      "Epoch 00061: loss improved from 2.07295 to 2.06557, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 62/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0685 - accuracy: 0.3525\n",
      "\n",
      "Epoch 00062: loss did not improve from 2.06557\n",
      "Epoch 63/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0699 - accuracy: 0.3531\n",
      "\n",
      "Epoch 00063: loss improved from 2.06557 to 2.05667, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 64/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0456 - accuracy: 0.3628\n",
      "\n",
      "Epoch 00064: loss improved from 2.05667 to 2.05356, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 65/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0626 - accuracy: 0.3621\n",
      "\n",
      "Epoch 00065: loss improved from 2.05356 to 2.05201, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 66/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0096 - accuracy: 0.3722\n",
      "\n",
      "Epoch 00066: loss improved from 2.05201 to 2.04547, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 67/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0729 - accuracy: 0.3540\n",
      "\n",
      "Epoch 00067: loss improved from 2.04547 to 2.03997, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 68/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0386 - accuracy: 0.3676\n",
      "\n",
      "Epoch 00068: loss improved from 2.03997 to 2.03848, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 69/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0007 - accuracy: 0.3750\n",
      "\n",
      "Epoch 00069: loss improved from 2.03848 to 2.03324, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 70/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0455 - accuracy: 0.3661\n",
      "\n",
      "Epoch 00070: loss improved from 2.03324 to 2.02982, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 71/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0257 - accuracy: 0.3762\n",
      "\n",
      "Epoch 00071: loss improved from 2.02982 to 2.02427, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 72/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0454 - accuracy: 0.3624\n",
      "\n",
      "Epoch 00072: loss improved from 2.02427 to 2.02272, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 73/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0292 - accuracy: 0.3652\n",
      "\n",
      "Epoch 00073: loss improved from 2.02272 to 2.01568, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 74/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0174 - accuracy: 0.3759\n",
      "\n",
      "Epoch 00074: loss improved from 2.01568 to 2.01269, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 75/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0136 - accuracy: 0.3741\n",
      "\n",
      "Epoch 00075: loss improved from 2.01269 to 2.01028, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 76/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9795 - accuracy: 0.3893\n",
      "\n",
      "Epoch 00076: loss improved from 2.01028 to 2.00486, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 77/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9630 - accuracy: 0.3845\n",
      "\n",
      "Epoch 00077: loss improved from 2.00486 to 2.00449, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 78/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9899 - accuracy: 0.3792\n",
      "\n",
      "Epoch 00078: loss improved from 2.00449 to 2.00133, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 79/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0062 - accuracy: 0.3751\n",
      "\n",
      "Epoch 00079: loss improved from 2.00133 to 1.99393, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 80/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9720 - accuracy: 0.3813\n",
      "\n",
      "Epoch 00080: loss improved from 1.99393 to 1.99158, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 81/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0044 - accuracy: 0.3744\n",
      "\n",
      "Epoch 00081: loss improved from 1.99158 to 1.99073, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 82/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9660 - accuracy: 0.3852\n",
      "\n",
      "Epoch 00082: loss improved from 1.99073 to 1.98085, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 83/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9928 - accuracy: 0.3881\n",
      "\n",
      "Epoch 00083: loss improved from 1.98085 to 1.97951, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 84/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.0032 - accuracy: 0.3802\n",
      "\n",
      "Epoch 00084: loss improved from 1.97951 to 1.97622, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 85/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9640 - accuracy: 0.3915\n",
      "\n",
      "Epoch 00085: loss improved from 1.97622 to 1.96396, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 86/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9838 - accuracy: 0.3783\n",
      "\n",
      "Epoch 00086: loss did not improve from 1.96396\n",
      "Epoch 87/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9555 - accuracy: 0.3945\n",
      "\n",
      "Epoch 00087: loss improved from 1.96396 to 1.95957, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 88/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9512 - accuracy: 0.3985\n",
      "\n",
      "Epoch 00088: loss improved from 1.95957 to 1.95135, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 89/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9478 - accuracy: 0.3975\n",
      "\n",
      "Epoch 00089: loss improved from 1.95135 to 1.94447, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 90/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9278 - accuracy: 0.4026\n",
      "\n",
      "Epoch 00090: loss improved from 1.94447 to 1.94200, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 91/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9343 - accuracy: 0.3986\n",
      "\n",
      "Epoch 00091: loss improved from 1.94200 to 1.93770, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 92/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9261 - accuracy: 0.4052\n",
      "\n",
      "Epoch 00092: loss improved from 1.93770 to 1.93272, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 93/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9561 - accuracy: 0.3958\n",
      "\n",
      "Epoch 00093: loss improved from 1.93272 to 1.92623, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 94/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9258 - accuracy: 0.3996\n",
      "\n",
      "Epoch 00094: loss improved from 1.92623 to 1.92529, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 95/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8946 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00095: loss improved from 1.92529 to 1.91539, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 96/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9036 - accuracy: 0.4084\n",
      "\n",
      "Epoch 00096: loss improved from 1.91539 to 1.91176, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 97/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8831 - accuracy: 0.4100\n",
      "\n",
      "Epoch 00097: loss improved from 1.91176 to 1.90519, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 98/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8997 - accuracy: 0.4100\n",
      "\n",
      "Epoch 00098: loss improved from 1.90519 to 1.90024, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 99/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9104 - accuracy: 0.4005\n",
      "\n",
      "Epoch 00099: loss improved from 1.90024 to 1.89689, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 100/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9020 - accuracy: 0.4088\n",
      "\n",
      "Epoch 00100: loss improved from 1.89689 to 1.89562, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 101/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8628 - accuracy: 0.4212\n",
      "\n",
      "Epoch 00101: loss improved from 1.89562 to 1.88629, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 102/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8929 - accuracy: 0.4181\n",
      "\n",
      "Epoch 00102: loss improved from 1.88629 to 1.88260, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 103/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8568 - accuracy: 0.4274\n",
      "\n",
      "Epoch 00103: loss improved from 1.88260 to 1.87411, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 104/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8842 - accuracy: 0.4156\n",
      "\n",
      "Epoch 00104: loss did not improve from 1.87411\n",
      "Epoch 105/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9016 - accuracy: 0.4084\n",
      "\n",
      "Epoch 00105: loss improved from 1.87411 to 1.86990, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 106/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8549 - accuracy: 0.4316\n",
      "\n",
      "Epoch 00106: loss improved from 1.86990 to 1.86159, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 107/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8276 - accuracy: 0.4374\n",
      "\n",
      "Epoch 00107: loss improved from 1.86159 to 1.85929, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 108/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8752 - accuracy: 0.4140\n",
      "\n",
      "Epoch 00108: loss improved from 1.85929 to 1.85740, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 109/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8497 - accuracy: 0.4315\n",
      "\n",
      "Epoch 00109: loss improved from 1.85740 to 1.85174, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 110/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8459 - accuracy: 0.4302\n",
      "\n",
      "Epoch 00110: loss improved from 1.85174 to 1.84335, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 111/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8468 - accuracy: 0.4323\n",
      "\n",
      "Epoch 00111: loss improved from 1.84335 to 1.84087, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 112/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8286 - accuracy: 0.4372\n",
      "\n",
      "Epoch 00112: loss improved from 1.84087 to 1.83353, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 113/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8409 - accuracy: 0.4386\n",
      "\n",
      "Epoch 00113: loss improved from 1.83353 to 1.83141, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 114/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8281 - accuracy: 0.4384\n",
      "\n",
      "Epoch 00114: loss improved from 1.83141 to 1.82704, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 115/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8098 - accuracy: 0.4462\n",
      "\n",
      "Epoch 00115: loss improved from 1.82704 to 1.81967, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 116/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8143 - accuracy: 0.4467\n",
      "\n",
      "Epoch 00116: loss improved from 1.81967 to 1.81575, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 117/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.7878 - accuracy: 0.4547\n",
      "\n",
      "Epoch 00117: loss improved from 1.81575 to 1.81174, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 118/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8133 - accuracy: 0.4453\n",
      "\n",
      "Epoch 00118: loss improved from 1.81174 to 1.80587, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 119/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8124 - accuracy: 0.4414\n",
      "\n",
      "Epoch 00119: loss improved from 1.80587 to 1.80493, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 120/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.8007 - accuracy: 0.4489\n",
      "\n",
      "Epoch 00120: loss improved from 1.80493 to 1.79225, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 121/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.7710 - accuracy: 0.4543\n",
      "\n",
      "Epoch 00121: loss improved from 1.79225 to 1.78845, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 122/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.7852 - accuracy: 0.4531\n",
      "\n",
      "Epoch 00122: loss improved from 1.78845 to 1.78669, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 123/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.7605 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00123: loss improved from 1.78669 to 1.78521, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 124/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.7762 - accuracy: 0.4555\n",
      "\n",
      "Epoch 00124: loss improved from 1.78521 to 1.77472, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 125/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.7514 - accuracy: 0.4622\n",
      "\n",
      "Epoch 00125: loss improved from 1.77472 to 1.77255, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 126/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.7781 - accuracy: 0.4496\n",
      "\n",
      "Epoch 00126: loss improved from 1.77255 to 1.76781, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 127/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.7566 - accuracy: 0.4648\n",
      "\n",
      "Epoch 00127: loss improved from 1.76781 to 1.76298, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 128/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.7753 - accuracy: 0.4512\n",
      "\n",
      "Epoch 00128: loss improved from 1.76298 to 1.75843, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 129/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.7538 - accuracy: 0.4641\n",
      "\n",
      "Epoch 00129: loss improved from 1.75843 to 1.74978, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 130/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.7453 - accuracy: 0.4672\n",
      "\n",
      "Epoch 00130: loss improved from 1.74978 to 1.74364, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 131/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.7239 - accuracy: 0.4753\n",
      "\n",
      "Epoch 00131: loss improved from 1.74364 to 1.73539, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 132/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.7717 - accuracy: 0.4576\n",
      "\n",
      "Epoch 00132: loss improved from 1.73539 to 1.73517, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 133/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.7278 - accuracy: 0.4742\n",
      "\n",
      "Epoch 00133: loss improved from 1.73517 to 1.72733, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 134/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.7414 - accuracy: 0.4707\n",
      "\n",
      "Epoch 00134: loss did not improve from 1.72733\n",
      "Epoch 135/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6847 - accuracy: 0.4823\n",
      "\n",
      "Epoch 00135: loss improved from 1.72733 to 1.72713, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 136/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6850 - accuracy: 0.4940\n",
      "\n",
      "Epoch 00136: loss improved from 1.72713 to 1.71681, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 137/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.7103 - accuracy: 0.4721\n",
      "\n",
      "Epoch 00137: loss improved from 1.71681 to 1.71112, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 138/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6974 - accuracy: 0.4874\n",
      "\n",
      "Epoch 00138: loss improved from 1.71112 to 1.70663, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 139/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6781 - accuracy: 0.4864\n",
      "\n",
      "Epoch 00139: loss improved from 1.70663 to 1.69616, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 140/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6784 - accuracy: 0.4874\n",
      "\n",
      "Epoch 00140: loss did not improve from 1.69616\n",
      "Epoch 141/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6802 - accuracy: 0.4878\n",
      "\n",
      "Epoch 00141: loss improved from 1.69616 to 1.69331, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 142/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6991 - accuracy: 0.4773\n",
      "\n",
      "Epoch 00142: loss improved from 1.69331 to 1.68917, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 143/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6762 - accuracy: 0.4859\n",
      "\n",
      "Epoch 00143: loss improved from 1.68917 to 1.68494, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 144/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6737 - accuracy: 0.4822\n",
      "\n",
      "Epoch 00144: loss improved from 1.68494 to 1.67973, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 145/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6512 - accuracy: 0.5034\n",
      "\n",
      "Epoch 00145: loss improved from 1.67973 to 1.66697, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 146/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6355 - accuracy: 0.4971\n",
      "\n",
      "Epoch 00146: loss did not improve from 1.66697\n",
      "Epoch 147/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6600 - accuracy: 0.4878\n",
      "\n",
      "Epoch 00147: loss improved from 1.66697 to 1.65930, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 148/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6701 - accuracy: 0.4868\n",
      "\n",
      "Epoch 00148: loss improved from 1.65930 to 1.65134, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 149/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6551 - accuracy: 0.4967\n",
      "\n",
      "Epoch 00149: loss improved from 1.65134 to 1.64693, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 150/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6569 - accuracy: 0.4899\n",
      "\n",
      "Epoch 00150: loss improved from 1.64693 to 1.64579, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 151/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6198 - accuracy: 0.5077\n",
      "\n",
      "Epoch 00151: loss improved from 1.64579 to 1.63657, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 152/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6137 - accuracy: 0.5038\n",
      "\n",
      "Epoch 00152: loss improved from 1.63657 to 1.63395, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 153/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6246 - accuracy: 0.5059\n",
      "\n",
      "Epoch 00153: loss improved from 1.63395 to 1.62859, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 154/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6147 - accuracy: 0.5095\n",
      "\n",
      "Epoch 00154: loss improved from 1.62859 to 1.62392, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 155/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6164 - accuracy: 0.5120\n",
      "\n",
      "Epoch 00155: loss improved from 1.62392 to 1.62030, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 156/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6078 - accuracy: 0.5147\n",
      "\n",
      "Epoch 00156: loss improved from 1.62030 to 1.61693, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 157/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6211 - accuracy: 0.5058\n",
      "\n",
      "Epoch 00157: loss improved from 1.61693 to 1.61628, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 158/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5803 - accuracy: 0.5216\n",
      "\n",
      "Epoch 00158: loss improved from 1.61628 to 1.60858, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 159/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6229 - accuracy: 0.5104\n",
      "\n",
      "Epoch 00159: loss improved from 1.60858 to 1.60544, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 160/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.6007 - accuracy: 0.5165\n",
      "\n",
      "Epoch 00160: loss improved from 1.60544 to 1.59687, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 161/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5780 - accuracy: 0.5140\n",
      "\n",
      "Epoch 00161: loss improved from 1.59687 to 1.59253, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 162/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5861 - accuracy: 0.5203\n",
      "\n",
      "Epoch 00162: loss improved from 1.59253 to 1.58869, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 163/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5729 - accuracy: 0.5260\n",
      "\n",
      "Epoch 00163: loss improved from 1.58869 to 1.58534, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 164/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5946 - accuracy: 0.5120\n",
      "\n",
      "Epoch 00164: loss improved from 1.58534 to 1.58401, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 165/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5992 - accuracy: 0.5106\n",
      "\n",
      "Epoch 00165: loss improved from 1.58401 to 1.57958, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 166/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5515 - accuracy: 0.5300\n",
      "\n",
      "Epoch 00166: loss improved from 1.57958 to 1.57357, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 167/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5661 - accuracy: 0.5186\n",
      "\n",
      "Epoch 00167: loss improved from 1.57357 to 1.57246, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 168/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5493 - accuracy: 0.5301\n",
      "\n",
      "Epoch 00168: loss improved from 1.57246 to 1.56293, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 169/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5495 - accuracy: 0.5215\n",
      "\n",
      "Epoch 00169: loss improved from 1.56293 to 1.56162, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 170/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5486 - accuracy: 0.5312\n",
      "\n",
      "Epoch 00170: loss improved from 1.56162 to 1.55148, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 171/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5457 - accuracy: 0.5319\n",
      "\n",
      "Epoch 00171: loss did not improve from 1.55148\n",
      "Epoch 172/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5587 - accuracy: 0.5249\n",
      "\n",
      "Epoch 00172: loss did not improve from 1.55148\n",
      "Epoch 173/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5229 - accuracy: 0.5436\n",
      "\n",
      "Epoch 00173: loss improved from 1.55148 to 1.54406, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 174/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5516 - accuracy: 0.5342\n",
      "\n",
      "Epoch 00174: loss improved from 1.54406 to 1.53588, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 175/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5154 - accuracy: 0.5401\n",
      "\n",
      "Epoch 00175: loss improved from 1.53588 to 1.53269, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 176/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5431 - accuracy: 0.5284\n",
      "\n",
      "Epoch 00176: loss did not improve from 1.53269\n",
      "Epoch 177/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5323 - accuracy: 0.5368\n",
      "\n",
      "Epoch 00177: loss improved from 1.53269 to 1.53118, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 178/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5296 - accuracy: 0.5402\n",
      "\n",
      "Epoch 00178: loss improved from 1.53118 to 1.52226, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 179/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.4814 - accuracy: 0.5566\n",
      "\n",
      "Epoch 00179: loss improved from 1.52226 to 1.51864, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 180/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5354 - accuracy: 0.5334\n",
      "\n",
      "Epoch 00180: loss improved from 1.51864 to 1.51352, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 181/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5229 - accuracy: 0.5360\n",
      "\n",
      "Epoch 00181: loss improved from 1.51352 to 1.51229, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 182/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5030 - accuracy: 0.5360\n",
      "\n",
      "Epoch 00182: loss improved from 1.51229 to 1.50810, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 183/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5077 - accuracy: 0.5379\n",
      "\n",
      "Epoch 00183: loss improved from 1.50810 to 1.50535, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 184/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5352 - accuracy: 0.5350\n",
      "\n",
      "Epoch 00184: loss improved from 1.50535 to 1.50046, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 185/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.4961 - accuracy: 0.5437\n",
      "\n",
      "Epoch 00185: loss did not improve from 1.50046\n",
      "Epoch 186/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.4875 - accuracy: 0.5409\n",
      "\n",
      "Epoch 00186: loss improved from 1.50046 to 1.49237, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 187/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.4920 - accuracy: 0.5477\n",
      "\n",
      "Epoch 00187: loss improved from 1.49237 to 1.48402, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 188/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.4930 - accuracy: 0.5484\n",
      "\n",
      "Epoch 00188: loss improved from 1.48402 to 1.48012, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 189/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.4361 - accuracy: 0.5674\n",
      "\n",
      "Epoch 00189: loss improved from 1.48012 to 1.47991, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 190/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.4532 - accuracy: 0.5591\n",
      "\n",
      "Epoch 00190: loss improved from 1.47991 to 1.47194, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 191/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.4856 - accuracy: 0.5443\n",
      "\n",
      "Epoch 00191: loss improved from 1.47194 to 1.46995, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 192/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.4500 - accuracy: 0.5584\n",
      "\n",
      "Epoch 00192: loss improved from 1.46995 to 1.46176, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 193/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.4599 - accuracy: 0.5607\n",
      "\n",
      "Epoch 00193: loss did not improve from 1.46176\n",
      "Epoch 194/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.4579 - accuracy: 0.5631\n",
      "\n",
      "Epoch 00194: loss did not improve from 1.46176\n",
      "Epoch 195/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.4273 - accuracy: 0.5627\n",
      "\n",
      "Epoch 00195: loss improved from 1.46176 to 1.45107, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 196/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.4597 - accuracy: 0.5626\n",
      "\n",
      "Epoch 00196: loss improved from 1.45107 to 1.44750, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 197/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.4386 - accuracy: 0.5623\n",
      "\n",
      "Epoch 00197: loss improved from 1.44750 to 1.44114, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 198/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.4296 - accuracy: 0.5667\n",
      "\n",
      "Epoch 00198: loss improved from 1.44114 to 1.43713, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 199/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3836 - accuracy: 0.5908\n",
      "\n",
      "Epoch 00199: loss improved from 1.43713 to 1.43321, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 200/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.4369 - accuracy: 0.5674\n",
      "\n",
      "Epoch 00200: loss improved from 1.43321 to 1.42847, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 201/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.4062 - accuracy: 0.5753\n",
      "\n",
      "Epoch 00201: loss improved from 1.42847 to 1.42376, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 202/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.4097 - accuracy: 0.5716\n",
      "\n",
      "Epoch 00202: loss improved from 1.42376 to 1.42210, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 203/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3945 - accuracy: 0.5736\n",
      "\n",
      "Epoch 00203: loss improved from 1.42210 to 1.41557, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 204/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3909 - accuracy: 0.5717\n",
      "\n",
      "Epoch 00204: loss did not improve from 1.41557\n",
      "Epoch 205/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3997 - accuracy: 0.5753\n",
      "\n",
      "Epoch 00205: loss improved from 1.41557 to 1.40945, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 206/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3866 - accuracy: 0.5769\n",
      "\n",
      "Epoch 00206: loss improved from 1.40945 to 1.40406, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 207/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3858 - accuracy: 0.5788\n",
      "\n",
      "Epoch 00207: loss improved from 1.40406 to 1.40167, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 208/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.4189 - accuracy: 0.5670\n",
      "\n",
      "Epoch 00208: loss did not improve from 1.40167\n",
      "Epoch 209/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3828 - accuracy: 0.5799\n",
      "\n",
      "Epoch 00209: loss improved from 1.40167 to 1.39585, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 210/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.4004 - accuracy: 0.5841\n",
      "\n",
      "Epoch 00210: loss improved from 1.39585 to 1.39095, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 211/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3600 - accuracy: 0.5902\n",
      "\n",
      "Epoch 00211: loss improved from 1.39095 to 1.38535, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 212/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.4125 - accuracy: 0.5714\n",
      "\n",
      "Epoch 00212: loss improved from 1.38535 to 1.38309, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 213/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.4419 - accuracy: 0.5587\n",
      "\n",
      "Epoch 00213: loss did not improve from 1.38309\n",
      "Epoch 214/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3815 - accuracy: 0.5846\n",
      "\n",
      "Epoch 00214: loss improved from 1.38309 to 1.37536, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 215/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3706 - accuracy: 0.5906\n",
      "\n",
      "Epoch 00215: loss did not improve from 1.37536\n",
      "Epoch 216/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3713 - accuracy: 0.5876\n",
      "\n",
      "Epoch 00216: loss improved from 1.37536 to 1.37009, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 217/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3665 - accuracy: 0.5876\n",
      "\n",
      "Epoch 00217: loss improved from 1.37009 to 1.35911, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 218/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3365 - accuracy: 0.6037\n",
      "\n",
      "Epoch 00218: loss improved from 1.35911 to 1.35776, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 219/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3676 - accuracy: 0.5825\n",
      "\n",
      "Epoch 00219: loss improved from 1.35776 to 1.35467, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 220/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3368 - accuracy: 0.5955\n",
      "\n",
      "Epoch 00220: loss improved from 1.35467 to 1.35315, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 221/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3307 - accuracy: 0.5969\n",
      "\n",
      "Epoch 00221: loss improved from 1.35315 to 1.34462, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 222/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3328 - accuracy: 0.5983\n",
      "\n",
      "Epoch 00222: loss improved from 1.34462 to 1.34215, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 223/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3204 - accuracy: 0.5967\n",
      "\n",
      "Epoch 00223: loss improved from 1.34215 to 1.34063, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 224/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3362 - accuracy: 0.5954\n",
      "\n",
      "Epoch 00224: loss improved from 1.34063 to 1.33309, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 225/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3336 - accuracy: 0.5989\n",
      "\n",
      "Epoch 00225: loss did not improve from 1.33309\n",
      "Epoch 226/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3548 - accuracy: 0.5918\n",
      "\n",
      "Epoch 00226: loss improved from 1.33309 to 1.32815, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 227/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3085 - accuracy: 0.6083\n",
      "\n",
      "Epoch 00227: loss improved from 1.32815 to 1.32615, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 228/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3434 - accuracy: 0.5921\n",
      "\n",
      "Epoch 00228: loss did not improve from 1.32615\n",
      "Epoch 229/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3312 - accuracy: 0.6002\n",
      "\n",
      "Epoch 00229: loss improved from 1.32615 to 1.31935, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 230/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3016 - accuracy: 0.6157\n",
      "\n",
      "Epoch 00230: loss improved from 1.31935 to 1.31642, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 231/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3079 - accuracy: 0.6050\n",
      "\n",
      "Epoch 00231: loss improved from 1.31642 to 1.31396, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 232/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3152 - accuracy: 0.5975\n",
      "\n",
      "Epoch 00232: loss improved from 1.31396 to 1.30608, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 233/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3047 - accuracy: 0.6060\n",
      "\n",
      "Epoch 00233: loss improved from 1.30608 to 1.30103, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 234/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2878 - accuracy: 0.6108\n",
      "\n",
      "Epoch 00234: loss did not improve from 1.30103\n",
      "Epoch 235/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2936 - accuracy: 0.6086\n",
      "\n",
      "Epoch 00235: loss improved from 1.30103 to 1.29479, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 236/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2834 - accuracy: 0.6126\n",
      "\n",
      "Epoch 00236: loss did not improve from 1.29479\n",
      "Epoch 237/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2863 - accuracy: 0.6123\n",
      "\n",
      "Epoch 00237: loss improved from 1.29479 to 1.29397, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 238/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2817 - accuracy: 0.6155\n",
      "\n",
      "Epoch 00238: loss improved from 1.29397 to 1.28888, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 239/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2665 - accuracy: 0.6166\n",
      "\n",
      "Epoch 00239: loss improved from 1.28888 to 1.28582, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 240/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3013 - accuracy: 0.6174\n",
      "\n",
      "Epoch 00240: loss improved from 1.28582 to 1.27866, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 241/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2704 - accuracy: 0.6200\n",
      "\n",
      "Epoch 00241: loss improved from 1.27866 to 1.27249, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 242/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2506 - accuracy: 0.6243\n",
      "\n",
      "Epoch 00242: loss did not improve from 1.27249\n",
      "Epoch 243/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2507 - accuracy: 0.6226\n",
      "\n",
      "Epoch 00243: loss improved from 1.27249 to 1.26937, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 244/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2443 - accuracy: 0.6244\n",
      "\n",
      "Epoch 00244: loss improved from 1.26937 to 1.26593, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 245/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2805 - accuracy: 0.6139\n",
      "\n",
      "Epoch 00245: loss did not improve from 1.26593\n",
      "Epoch 246/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2683 - accuracy: 0.6175\n",
      "\n",
      "Epoch 00246: loss improved from 1.26593 to 1.25866, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 247/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2545 - accuracy: 0.6294\n",
      "\n",
      "Epoch 00247: loss improved from 1.25866 to 1.25257, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 248/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2644 - accuracy: 0.6140\n",
      "\n",
      "Epoch 00248: loss did not improve from 1.25257\n",
      "Epoch 249/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2636 - accuracy: 0.6200\n",
      "\n",
      "Epoch 00249: loss improved from 1.25257 to 1.24864, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 250/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2161 - accuracy: 0.6370\n",
      "\n",
      "Epoch 00250: loss improved from 1.24864 to 1.24705, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 251/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2037 - accuracy: 0.6406\n",
      "\n",
      "Epoch 00251: loss improved from 1.24705 to 1.24218, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 252/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2621 - accuracy: 0.6189\n",
      "\n",
      "Epoch 00252: loss improved from 1.24218 to 1.24089, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 253/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2314 - accuracy: 0.6260\n",
      "\n",
      "Epoch 00253: loss did not improve from 1.24089\n",
      "Epoch 254/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2455 - accuracy: 0.6231\n",
      "\n",
      "Epoch 00254: loss improved from 1.24089 to 1.23418, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 255/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2386 - accuracy: 0.6232\n",
      "\n",
      "Epoch 00255: loss improved from 1.23418 to 1.23059, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 256/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2220 - accuracy: 0.6229\n",
      "\n",
      "Epoch 00256: loss improved from 1.23059 to 1.22650, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 257/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1998 - accuracy: 0.6311\n",
      "\n",
      "Epoch 00257: loss improved from 1.22650 to 1.22216, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 258/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2129 - accuracy: 0.6310\n",
      "\n",
      "Epoch 00258: loss did not improve from 1.22216\n",
      "Epoch 259/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2337 - accuracy: 0.6295\n",
      "\n",
      "Epoch 00259: loss did not improve from 1.22216\n",
      "Epoch 260/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2156 - accuracy: 0.6376\n",
      "\n",
      "Epoch 00260: loss improved from 1.22216 to 1.21690, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 261/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2006 - accuracy: 0.6350\n",
      "\n",
      "Epoch 00261: loss improved from 1.21690 to 1.21577, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 262/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2063 - accuracy: 0.6385\n",
      "\n",
      "Epoch 00262: loss improved from 1.21577 to 1.21000, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 263/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1764 - accuracy: 0.6437\n",
      "\n",
      "Epoch 00263: loss did not improve from 1.21000\n",
      "Epoch 264/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1766 - accuracy: 0.6484\n",
      "\n",
      "Epoch 00264: loss improved from 1.21000 to 1.20648, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 265/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2020 - accuracy: 0.6406\n",
      "\n",
      "Epoch 00265: loss improved from 1.20648 to 1.20561, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 266/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1825 - accuracy: 0.6416\n",
      "\n",
      "Epoch 00266: loss improved from 1.20561 to 1.19916, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 267/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2117 - accuracy: 0.6324\n",
      "\n",
      "Epoch 00267: loss improved from 1.19916 to 1.19870, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 268/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1923 - accuracy: 0.6405\n",
      "\n",
      "Epoch 00268: loss improved from 1.19870 to 1.19775, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 269/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1767 - accuracy: 0.6492\n",
      "\n",
      "Epoch 00269: loss improved from 1.19775 to 1.19471, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 270/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1711 - accuracy: 0.6508\n",
      "\n",
      "Epoch 00270: loss improved from 1.19471 to 1.19340, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 271/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2009 - accuracy: 0.6306\n",
      "\n",
      "Epoch 00271: loss improved from 1.19340 to 1.18466, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 272/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1593 - accuracy: 0.6569\n",
      "\n",
      "Epoch 00272: loss improved from 1.18466 to 1.17821, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 273/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1663 - accuracy: 0.6525\n",
      "\n",
      "Epoch 00273: loss improved from 1.17821 to 1.17533, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 274/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1589 - accuracy: 0.6480\n",
      "\n",
      "Epoch 00274: loss improved from 1.17533 to 1.17281, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 275/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1617 - accuracy: 0.6545\n",
      "\n",
      "Epoch 00275: loss did not improve from 1.17281\n",
      "Epoch 276/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1802 - accuracy: 0.6386\n",
      "\n",
      "Epoch 00276: loss did not improve from 1.17281\n",
      "Epoch 277/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1611 - accuracy: 0.6559\n",
      "\n",
      "Epoch 00277: loss improved from 1.17281 to 1.16540, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 278/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1371 - accuracy: 0.6496\n",
      "\n",
      "Epoch 00278: loss improved from 1.16540 to 1.16482, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 279/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1492 - accuracy: 0.6497\n",
      "\n",
      "Epoch 00279: loss did not improve from 1.16482\n",
      "Epoch 280/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1353 - accuracy: 0.6576\n",
      "\n",
      "Epoch 00280: loss improved from 1.16482 to 1.15568, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 281/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1788 - accuracy: 0.6465\n",
      "\n",
      "Epoch 00281: loss did not improve from 1.15568\n",
      "Epoch 282/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1371 - accuracy: 0.6572\n",
      "\n",
      "Epoch 00282: loss did not improve from 1.15568\n",
      "Epoch 283/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1634 - accuracy: 0.6518\n",
      "\n",
      "Epoch 00283: loss improved from 1.15568 to 1.15424, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 284/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1427 - accuracy: 0.6494\n",
      "\n",
      "Epoch 00284: loss improved from 1.15424 to 1.14676, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 285/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1116 - accuracy: 0.6670\n",
      "\n",
      "Epoch 00285: loss improved from 1.14676 to 1.13844, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 286/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1482 - accuracy: 0.6510\n",
      "\n",
      "Epoch 00286: loss improved from 1.13844 to 1.13474, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 287/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1233 - accuracy: 0.6582\n",
      "\n",
      "Epoch 00287: loss did not improve from 1.13474\n",
      "Epoch 288/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1363 - accuracy: 0.6601\n",
      "\n",
      "Epoch 00288: loss improved from 1.13474 to 1.13382, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 289/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1585 - accuracy: 0.6502\n",
      "\n",
      "Epoch 00289: loss improved from 1.13382 to 1.13083, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 290/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1241 - accuracy: 0.6642\n",
      "\n",
      "Epoch 00290: loss improved from 1.13083 to 1.12755, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 291/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1430 - accuracy: 0.6543\n",
      "\n",
      "Epoch 00291: loss did not improve from 1.12755\n",
      "Epoch 292/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0961 - accuracy: 0.6680\n",
      "\n",
      "Epoch 00292: loss improved from 1.12755 to 1.12090, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 293/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1397 - accuracy: 0.6562\n",
      "\n",
      "Epoch 00293: loss did not improve from 1.12090\n",
      "Epoch 294/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1337 - accuracy: 0.6631\n",
      "\n",
      "Epoch 00294: loss improved from 1.12090 to 1.11993, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 295/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1538 - accuracy: 0.6499\n",
      "\n",
      "Epoch 00295: loss did not improve from 1.11993\n",
      "Epoch 296/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1020 - accuracy: 0.6684\n",
      "\n",
      "Epoch 00296: loss did not improve from 1.11993\n",
      "Epoch 297/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1318 - accuracy: 0.6506\n",
      "\n",
      "Epoch 00297: loss improved from 1.11993 to 1.11368, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 298/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1287 - accuracy: 0.6550\n",
      "\n",
      "Epoch 00298: loss improved from 1.11368 to 1.10792, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 299/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1060 - accuracy: 0.6618\n",
      "\n",
      "Epoch 00299: loss improved from 1.10792 to 1.10353, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 300/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0895 - accuracy: 0.6739\n",
      "\n",
      "Epoch 00300: loss did not improve from 1.10353\n",
      "Epoch 301/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0754 - accuracy: 0.6818\n",
      "\n",
      "Epoch 00301: loss improved from 1.10353 to 1.10221, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 302/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0954 - accuracy: 0.6721\n",
      "\n",
      "Epoch 00302: loss improved from 1.10221 to 1.09515, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 303/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0828 - accuracy: 0.6740\n",
      "\n",
      "Epoch 00303: loss improved from 1.09515 to 1.09372, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 304/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0720 - accuracy: 0.6700\n",
      "\n",
      "Epoch 00304: loss improved from 1.09372 to 1.09109, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 305/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1033 - accuracy: 0.6591\n",
      "\n",
      "Epoch 00305: loss improved from 1.09109 to 1.08701, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 306/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0860 - accuracy: 0.6703\n",
      "\n",
      "Epoch 00306: loss improved from 1.08701 to 1.08512, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 307/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1020 - accuracy: 0.6669\n",
      "\n",
      "Epoch 00307: loss improved from 1.08512 to 1.08283, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 308/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0772 - accuracy: 0.6771\n",
      "\n",
      "Epoch 00308: loss improved from 1.08283 to 1.07923, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 309/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0610 - accuracy: 0.6756\n",
      "\n",
      "Epoch 00309: loss improved from 1.07923 to 1.07241, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 310/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0879 - accuracy: 0.6748\n",
      "\n",
      "Epoch 00310: loss improved from 1.07241 to 1.07080, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 311/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0499 - accuracy: 0.6859\n",
      "\n",
      "Epoch 00311: loss improved from 1.07080 to 1.06985, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 312/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0455 - accuracy: 0.6831\n",
      "\n",
      "Epoch 00312: loss did not improve from 1.06985\n",
      "Epoch 313/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0660 - accuracy: 0.6755\n",
      "\n",
      "Epoch 00313: loss did not improve from 1.06985\n",
      "Epoch 314/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0757 - accuracy: 0.6725\n",
      "\n",
      "Epoch 00314: loss did not improve from 1.06985\n",
      "Epoch 315/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0589 - accuracy: 0.6781\n",
      "\n",
      "Epoch 00315: loss did not improve from 1.06985\n",
      "Epoch 316/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0644 - accuracy: 0.6785\n",
      "\n",
      "Epoch 00316: loss improved from 1.06985 to 1.06459, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 317/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0617 - accuracy: 0.6803\n",
      "\n",
      "Epoch 00317: loss improved from 1.06459 to 1.05577, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 318/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0687 - accuracy: 0.6790\n",
      "\n",
      "Epoch 00318: loss did not improve from 1.05577\n",
      "Epoch 319/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0629 - accuracy: 0.6723\n",
      "\n",
      "Epoch 00319: loss improved from 1.05577 to 1.05060, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 320/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0540 - accuracy: 0.6747\n",
      "\n",
      "Epoch 00320: loss did not improve from 1.05060\n",
      "Epoch 321/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0602 - accuracy: 0.6810\n",
      "\n",
      "Epoch 00321: loss improved from 1.05060 to 1.04352, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 322/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0459 - accuracy: 0.6883\n",
      "\n",
      "Epoch 00322: loss improved from 1.04352 to 1.03835, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 323/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0654 - accuracy: 0.6691\n",
      "\n",
      "Epoch 00323: loss did not improve from 1.03835\n",
      "Epoch 324/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0630 - accuracy: 0.6748\n",
      "\n",
      "Epoch 00324: loss did not improve from 1.03835\n",
      "Epoch 325/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0486 - accuracy: 0.6829\n",
      "\n",
      "Epoch 00325: loss improved from 1.03835 to 1.03490, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 326/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0534 - accuracy: 0.6836\n",
      "\n",
      "Epoch 00326: loss improved from 1.03490 to 1.03382, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 327/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0148 - accuracy: 0.6953\n",
      "\n",
      "Epoch 00327: loss improved from 1.03382 to 1.03250, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 328/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0168 - accuracy: 0.6890\n",
      "\n",
      "Epoch 00328: loss did not improve from 1.03250\n",
      "Epoch 329/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0502 - accuracy: 0.6860\n",
      "\n",
      "Epoch 00329: loss improved from 1.03250 to 1.03237, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 330/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0398 - accuracy: 0.6855\n",
      "\n",
      "Epoch 00330: loss did not improve from 1.03237\n",
      "Epoch 331/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9957 - accuracy: 0.6947\n",
      "\n",
      "Epoch 00331: loss improved from 1.03237 to 1.02864, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 332/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0029 - accuracy: 0.7005\n",
      "\n",
      "Epoch 00332: loss improved from 1.02864 to 1.02405, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 333/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0142 - accuracy: 0.6920\n",
      "\n",
      "Epoch 00333: loss improved from 1.02405 to 1.01937, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 334/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0246 - accuracy: 0.6884\n",
      "\n",
      "Epoch 00334: loss improved from 1.01937 to 1.01240, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 335/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9974 - accuracy: 0.6980\n",
      "\n",
      "Epoch 00335: loss did not improve from 1.01240\n",
      "Epoch 336/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0047 - accuracy: 0.6990\n",
      "\n",
      "Epoch 00336: loss improved from 1.01240 to 1.01182, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 337/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9941 - accuracy: 0.6970\n",
      "\n",
      "Epoch 00337: loss improved from 1.01182 to 1.00645, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 338/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9913 - accuracy: 0.6970\n",
      "\n",
      "Epoch 00338: loss improved from 1.00645 to 1.00249, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 339/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9825 - accuracy: 0.7047\n",
      "\n",
      "Epoch 00339: loss improved from 1.00249 to 0.99900, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 340/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0024 - accuracy: 0.7007\n",
      "\n",
      "Epoch 00340: loss did not improve from 0.99900\n",
      "Epoch 341/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9822 - accuracy: 0.7039\n",
      "\n",
      "Epoch 00341: loss did not improve from 0.99900\n",
      "Epoch 342/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9868 - accuracy: 0.7044\n",
      "\n",
      "Epoch 00342: loss did not improve from 0.99900\n",
      "Epoch 343/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9737 - accuracy: 0.7089\n",
      "\n",
      "Epoch 00343: loss did not improve from 0.99900\n",
      "Epoch 344/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9885 - accuracy: 0.6944\n",
      "\n",
      "Epoch 00344: loss improved from 0.99900 to 0.99509, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 345/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0038 - accuracy: 0.6918\n",
      "\n",
      "Epoch 00345: loss did not improve from 0.99509\n",
      "Epoch 346/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0151 - accuracy: 0.6946\n",
      "\n",
      "Epoch 00346: loss did not improve from 0.99509\n",
      "Epoch 347/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9767 - accuracy: 0.7089\n",
      "\n",
      "Epoch 00347: loss improved from 0.99509 to 0.98803, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 348/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9961 - accuracy: 0.6921\n",
      "\n",
      "Epoch 00348: loss improved from 0.98803 to 0.97834, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 349/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9782 - accuracy: 0.7068\n",
      "\n",
      "Epoch 00349: loss improved from 0.97834 to 0.97791, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 350/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9919 - accuracy: 0.7021\n",
      "\n",
      "Epoch 00350: loss improved from 0.97791 to 0.97482, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 351/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9626 - accuracy: 0.7113\n",
      "\n",
      "Epoch 00351: loss improved from 0.97482 to 0.97206, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 352/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9408 - accuracy: 0.7194\n",
      "\n",
      "Epoch 00352: loss improved from 0.97206 to 0.96725, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 353/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9585 - accuracy: 0.7096\n",
      "\n",
      "Epoch 00353: loss did not improve from 0.96725\n",
      "Epoch 354/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9929 - accuracy: 0.7051\n",
      "\n",
      "Epoch 00354: loss improved from 0.96725 to 0.96703, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 355/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9390 - accuracy: 0.7119\n",
      "\n",
      "Epoch 00355: loss improved from 0.96703 to 0.96635, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 356/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9580 - accuracy: 0.7004\n",
      "\n",
      "Epoch 00356: loss did not improve from 0.96635\n",
      "Epoch 357/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9366 - accuracy: 0.7170\n",
      "\n",
      "Epoch 00357: loss improved from 0.96635 to 0.96459, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 358/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9564 - accuracy: 0.7080\n",
      "\n",
      "Epoch 00358: loss improved from 0.96459 to 0.95904, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 359/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9591 - accuracy: 0.6996\n",
      "\n",
      "Epoch 00359: loss improved from 0.95904 to 0.95792, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 360/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9477 - accuracy: 0.7111\n",
      "\n",
      "Epoch 00360: loss improved from 0.95792 to 0.95602, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 361/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9368 - accuracy: 0.7240\n",
      "\n",
      "Epoch 00361: loss improved from 0.95602 to 0.95154, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 362/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9216 - accuracy: 0.7180\n",
      "\n",
      "Epoch 00362: loss improved from 0.95154 to 0.94803, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 363/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9275 - accuracy: 0.7219\n",
      "\n",
      "Epoch 00363: loss improved from 0.94803 to 0.94211, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 364/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9482 - accuracy: 0.7138\n",
      "\n",
      "Epoch 00364: loss did not improve from 0.94211\n",
      "Epoch 365/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9260 - accuracy: 0.7274\n",
      "\n",
      "Epoch 00365: loss improved from 0.94211 to 0.94073, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 366/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9342 - accuracy: 0.7176\n",
      "\n",
      "Epoch 00366: loss improved from 0.94073 to 0.94031, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 367/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9306 - accuracy: 0.7214\n",
      "\n",
      "Epoch 00367: loss improved from 0.94031 to 0.93774, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 368/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9411 - accuracy: 0.7165\n",
      "\n",
      "Epoch 00368: loss did not improve from 0.93774\n",
      "Epoch 369/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9411 - accuracy: 0.7137\n",
      "\n",
      "Epoch 00369: loss did not improve from 0.93774\n",
      "Epoch 370/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9429 - accuracy: 0.7073\n",
      "\n",
      "Epoch 00370: loss improved from 0.93774 to 0.93578, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 371/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9317 - accuracy: 0.7263\n",
      "\n",
      "Epoch 00371: loss improved from 0.93578 to 0.93091, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 372/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9190 - accuracy: 0.7190\n",
      "\n",
      "Epoch 00372: loss improved from 0.93091 to 0.92349, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 373/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8977 - accuracy: 0.7303\n",
      "\n",
      "Epoch 00373: loss improved from 0.92349 to 0.91777, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 374/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8999 - accuracy: 0.7311\n",
      "\n",
      "Epoch 00374: loss did not improve from 0.91777\n",
      "Epoch 375/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9405 - accuracy: 0.7167\n",
      "\n",
      "Epoch 00375: loss did not improve from 0.91777\n",
      "Epoch 376/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9024 - accuracy: 0.7319\n",
      "\n",
      "Epoch 00376: loss did not improve from 0.91777\n",
      "Epoch 377/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9344 - accuracy: 0.7159\n",
      "\n",
      "Epoch 00377: loss did not improve from 0.91777\n",
      "Epoch 378/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9206 - accuracy: 0.7225\n",
      "\n",
      "Epoch 00378: loss did not improve from 0.91777\n",
      "Epoch 379/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9125 - accuracy: 0.7180\n",
      "\n",
      "Epoch 00379: loss did not improve from 0.91777\n",
      "Epoch 380/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9317 - accuracy: 0.7178\n",
      "\n",
      "Epoch 00380: loss improved from 0.91777 to 0.91265, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 381/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9065 - accuracy: 0.7268\n",
      "\n",
      "Epoch 00381: loss improved from 0.91265 to 0.90943, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 382/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9212 - accuracy: 0.7178\n",
      "\n",
      "Epoch 00382: loss did not improve from 0.90943\n",
      "Epoch 383/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8957 - accuracy: 0.7298\n",
      "\n",
      "Epoch 00383: loss improved from 0.90943 to 0.90493, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 384/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9029 - accuracy: 0.7282\n",
      "\n",
      "Epoch 00384: loss did not improve from 0.90493\n",
      "Epoch 385/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9001 - accuracy: 0.7367\n",
      "\n",
      "Epoch 00385: loss did not improve from 0.90493\n",
      "Epoch 386/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9143 - accuracy: 0.7196\n",
      "\n",
      "Epoch 00386: loss did not improve from 0.90493\n",
      "Epoch 387/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8978 - accuracy: 0.7279\n",
      "\n",
      "Epoch 00387: loss did not improve from 0.90493\n",
      "Epoch 388/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9321 - accuracy: 0.7134\n",
      "\n",
      "Epoch 00388: loss did not improve from 0.90493\n",
      "Epoch 389/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9034 - accuracy: 0.7221\n",
      "\n",
      "Epoch 00389: loss improved from 0.90493 to 0.90090, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 390/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8990 - accuracy: 0.7293\n",
      "\n",
      "Epoch 00390: loss improved from 0.90090 to 0.89152, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 391/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.9002 - accuracy: 0.7323\n",
      "\n",
      "Epoch 00391: loss improved from 0.89152 to 0.88551, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 392/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8804 - accuracy: 0.7318\n",
      "\n",
      "Epoch 00392: loss did not improve from 0.88551\n",
      "Epoch 393/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8811 - accuracy: 0.7320\n",
      "\n",
      "Epoch 00393: loss did not improve from 0.88551\n",
      "Epoch 394/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8679 - accuracy: 0.7380\n",
      "\n",
      "Epoch 00394: loss did not improve from 0.88551\n",
      "Epoch 395/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8819 - accuracy: 0.7396\n",
      "\n",
      "Epoch 00395: loss did not improve from 0.88551\n",
      "Epoch 396/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8746 - accuracy: 0.7294\n",
      "\n",
      "Epoch 00396: loss improved from 0.88551 to 0.88188, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 397/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8471 - accuracy: 0.7421\n",
      "\n",
      "Epoch 00397: loss improved from 0.88188 to 0.87629, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 398/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8983 - accuracy: 0.7293\n",
      "\n",
      "Epoch 00398: loss did not improve from 0.87629\n",
      "Epoch 399/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8646 - accuracy: 0.7440\n",
      "\n",
      "Epoch 00399: loss improved from 0.87629 to 0.87237, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 400/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8821 - accuracy: 0.7385\n",
      "\n",
      "Epoch 00400: loss did not improve from 0.87237\n",
      "Epoch 401/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8619 - accuracy: 0.7398\n",
      "\n",
      "Epoch 00401: loss did not improve from 0.87237\n",
      "Epoch 402/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8575 - accuracy: 0.7346\n",
      "\n",
      "Epoch 00402: loss did not improve from 0.87237\n",
      "Epoch 403/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8656 - accuracy: 0.7420\n",
      "\n",
      "Epoch 00403: loss improved from 0.87237 to 0.86790, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 404/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8659 - accuracy: 0.7413\n",
      "\n",
      "Epoch 00404: loss improved from 0.86790 to 0.86262, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 405/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8559 - accuracy: 0.7417\n",
      "\n",
      "Epoch 00405: loss did not improve from 0.86262\n",
      "Epoch 406/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8942 - accuracy: 0.7215\n",
      "\n",
      "Epoch 00406: loss did not improve from 0.86262\n",
      "Epoch 407/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8672 - accuracy: 0.7395\n",
      "\n",
      "Epoch 00407: loss improved from 0.86262 to 0.86134, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 408/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8489 - accuracy: 0.7409\n",
      "\n",
      "Epoch 00408: loss improved from 0.86134 to 0.85696, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 409/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8573 - accuracy: 0.7430\n",
      "\n",
      "Epoch 00409: loss did not improve from 0.85696\n",
      "Epoch 410/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8428 - accuracy: 0.7424\n",
      "\n",
      "Epoch 00410: loss improved from 0.85696 to 0.85020, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 411/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8374 - accuracy: 0.7540\n",
      "\n",
      "Epoch 00411: loss did not improve from 0.85020\n",
      "Epoch 412/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8254 - accuracy: 0.7510\n",
      "\n",
      "Epoch 00412: loss improved from 0.85020 to 0.84870, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 413/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8456 - accuracy: 0.7458\n",
      "\n",
      "Epoch 00413: loss improved from 0.84870 to 0.84852, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 414/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8150 - accuracy: 0.7553\n",
      "\n",
      "Epoch 00414: loss improved from 0.84852 to 0.84825, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 415/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8285 - accuracy: 0.7481\n",
      "\n",
      "Epoch 00415: loss improved from 0.84825 to 0.84691, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 416/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8513 - accuracy: 0.7425\n",
      "\n",
      "Epoch 00416: loss did not improve from 0.84691\n",
      "Epoch 417/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8449 - accuracy: 0.7387\n",
      "\n",
      "Epoch 00417: loss did not improve from 0.84691\n",
      "Epoch 418/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8676 - accuracy: 0.7370\n",
      "\n",
      "Epoch 00418: loss did not improve from 0.84691\n",
      "Epoch 419/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8348 - accuracy: 0.7516\n",
      "\n",
      "Epoch 00419: loss improved from 0.84691 to 0.84465, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 420/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8326 - accuracy: 0.7493\n",
      "\n",
      "Epoch 00420: loss improved from 0.84465 to 0.84296, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 421/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8335 - accuracy: 0.7486\n",
      "\n",
      "Epoch 00421: loss did not improve from 0.84296\n",
      "Epoch 422/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8468 - accuracy: 0.7418\n",
      "\n",
      "Epoch 00422: loss improved from 0.84296 to 0.83526, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 423/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8262 - accuracy: 0.7570\n",
      "\n",
      "Epoch 00423: loss did not improve from 0.83526\n",
      "Epoch 424/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8145 - accuracy: 0.7525\n",
      "\n",
      "Epoch 00424: loss improved from 0.83526 to 0.83250, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 425/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8201 - accuracy: 0.7515\n",
      "\n",
      "Epoch 00425: loss did not improve from 0.83250\n",
      "Epoch 426/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8300 - accuracy: 0.7545\n",
      "\n",
      "Epoch 00426: loss improved from 0.83250 to 0.82649, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 427/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8185 - accuracy: 0.7545\n",
      "\n",
      "Epoch 00427: loss improved from 0.82649 to 0.81736, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 428/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8376 - accuracy: 0.7419\n",
      "\n",
      "Epoch 00428: loss did not improve from 0.81736\n",
      "Epoch 429/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8105 - accuracy: 0.7594\n",
      "\n",
      "Epoch 00429: loss did not improve from 0.81736\n",
      "Epoch 430/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7934 - accuracy: 0.7598\n",
      "\n",
      "Epoch 00430: loss improved from 0.81736 to 0.81423, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 431/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7883 - accuracy: 0.7601\n",
      "\n",
      "Epoch 00431: loss improved from 0.81423 to 0.80904, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 432/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8084 - accuracy: 0.7512\n",
      "\n",
      "Epoch 00432: loss did not improve from 0.80904\n",
      "Epoch 433/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7912 - accuracy: 0.7687\n",
      "\n",
      "Epoch 00433: loss did not improve from 0.80904\n",
      "Epoch 434/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8048 - accuracy: 0.7572\n",
      "\n",
      "Epoch 00434: loss did not improve from 0.80904\n",
      "Epoch 435/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8035 - accuracy: 0.7620\n",
      "\n",
      "Epoch 00435: loss did not improve from 0.80904\n",
      "Epoch 436/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7761 - accuracy: 0.7670\n",
      "\n",
      "Epoch 00436: loss improved from 0.80904 to 0.80587, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 437/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8120 - accuracy: 0.7614\n",
      "\n",
      "Epoch 00437: loss improved from 0.80587 to 0.79667, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 438/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8278 - accuracy: 0.7457\n",
      "\n",
      "Epoch 00438: loss did not improve from 0.79667\n",
      "Epoch 439/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7937 - accuracy: 0.7632\n",
      "\n",
      "Epoch 00439: loss did not improve from 0.79667\n",
      "Epoch 440/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8015 - accuracy: 0.7642\n",
      "\n",
      "Epoch 00440: loss did not improve from 0.79667\n",
      "Epoch 441/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8101 - accuracy: 0.7571\n",
      "\n",
      "Epoch 00441: loss did not improve from 0.79667\n",
      "Epoch 442/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8164 - accuracy: 0.7499\n",
      "\n",
      "Epoch 00442: loss did not improve from 0.79667\n",
      "Epoch 443/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7961 - accuracy: 0.7563\n",
      "\n",
      "Epoch 00443: loss improved from 0.79667 to 0.79337, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 444/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7937 - accuracy: 0.7612\n",
      "\n",
      "Epoch 00444: loss improved from 0.79337 to 0.78651, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 445/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7879 - accuracy: 0.7642\n",
      "\n",
      "Epoch 00445: loss did not improve from 0.78651\n",
      "Epoch 446/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7805 - accuracy: 0.7669\n",
      "\n",
      "Epoch 00446: loss improved from 0.78651 to 0.78635, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 447/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7520 - accuracy: 0.7751\n",
      "\n",
      "Epoch 00447: loss improved from 0.78635 to 0.78591, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 448/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7880 - accuracy: 0.7671\n",
      "\n",
      "Epoch 00448: loss improved from 0.78591 to 0.78321, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 449/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7998 - accuracy: 0.7590\n",
      "\n",
      "Epoch 00449: loss improved from 0.78321 to 0.78237, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 450/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7641 - accuracy: 0.7751\n",
      "\n",
      "Epoch 00450: loss did not improve from 0.78237\n",
      "Epoch 451/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7734 - accuracy: 0.7671\n",
      "\n",
      "Epoch 00451: loss improved from 0.78237 to 0.77837, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 452/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7794 - accuracy: 0.7646\n",
      "\n",
      "Epoch 00452: loss did not improve from 0.77837\n",
      "Epoch 453/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7724 - accuracy: 0.7646\n",
      "\n",
      "Epoch 00453: loss did not improve from 0.77837\n",
      "Epoch 454/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8011 - accuracy: 0.7608\n",
      "\n",
      "Epoch 00454: loss did not improve from 0.77837\n",
      "Epoch 455/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7621 - accuracy: 0.7733\n",
      "\n",
      "Epoch 00455: loss improved from 0.77837 to 0.77199, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 456/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.8017 - accuracy: 0.7605\n",
      "\n",
      "Epoch 00456: loss did not improve from 0.77199\n",
      "Epoch 457/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7855 - accuracy: 0.7591\n",
      "\n",
      "Epoch 00457: loss did not improve from 0.77199\n",
      "Epoch 458/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7670 - accuracy: 0.7715\n",
      "\n",
      "Epoch 00458: loss improved from 0.77199 to 0.76649, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 459/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7504 - accuracy: 0.7739\n",
      "\n",
      "Epoch 00459: loss improved from 0.76649 to 0.76359, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 460/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7386 - accuracy: 0.7811\n",
      "\n",
      "Epoch 00460: loss improved from 0.76359 to 0.75988, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 461/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7648 - accuracy: 0.7696\n",
      "\n",
      "Epoch 00461: loss improved from 0.75988 to 0.75717, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 462/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7628 - accuracy: 0.7678\n",
      "\n",
      "Epoch 00462: loss did not improve from 0.75717\n",
      "Epoch 463/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7460 - accuracy: 0.7784\n",
      "\n",
      "Epoch 00463: loss did not improve from 0.75717\n",
      "Epoch 464/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.7545 - accuracy: 0.7756\n",
      "\n",
      "Epoch 00464: loss did not improve from 0.75717\n",
      "Epoch 465/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7592 - accuracy: 0.7746\n",
      "\n",
      "Epoch 00465: loss did not improve from 0.75717\n",
      "Epoch 466/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7709 - accuracy: 0.7631\n",
      "\n",
      "Epoch 00466: loss did not improve from 0.75717\n",
      "Epoch 467/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7446 - accuracy: 0.7786\n",
      "\n",
      "Epoch 00467: loss improved from 0.75717 to 0.75251, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 468/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7534 - accuracy: 0.7733\n",
      "\n",
      "Epoch 00468: loss did not improve from 0.75251\n",
      "Epoch 469/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7591 - accuracy: 0.7656\n",
      "\n",
      "Epoch 00469: loss did not improve from 0.75251\n",
      "Epoch 470/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7540 - accuracy: 0.7714\n",
      "\n",
      "Epoch 00470: loss did not improve from 0.75251\n",
      "Epoch 471/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7420 - accuracy: 0.7740\n",
      "\n",
      "Epoch 00471: loss did not improve from 0.75251\n",
      "Epoch 472/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7616 - accuracy: 0.7665\n",
      "\n",
      "Epoch 00472: loss did not improve from 0.75251\n",
      "Epoch 473/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7377 - accuracy: 0.7818\n",
      "\n",
      "Epoch 00473: loss did not improve from 0.75251\n",
      "Epoch 474/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7709 - accuracy: 0.7725\n",
      "\n",
      "Epoch 00474: loss improved from 0.75251 to 0.74830, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 475/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7509 - accuracy: 0.7748\n",
      "\n",
      "Epoch 00475: loss did not improve from 0.74830\n",
      "Epoch 476/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7595 - accuracy: 0.7772\n",
      "\n",
      "Epoch 00476: loss improved from 0.74830 to 0.74720, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 477/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7420 - accuracy: 0.7758\n",
      "\n",
      "Epoch 00477: loss did not improve from 0.74720\n",
      "Epoch 478/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7260 - accuracy: 0.7824\n",
      "\n",
      "Epoch 00478: loss improved from 0.74720 to 0.74252, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 479/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7459 - accuracy: 0.7791\n",
      "\n",
      "Epoch 00479: loss improved from 0.74252 to 0.73232, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 480/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7295 - accuracy: 0.7793\n",
      "\n",
      "Epoch 00480: loss did not improve from 0.73232\n",
      "Epoch 481/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7309 - accuracy: 0.7793\n",
      "\n",
      "Epoch 00481: loss did not improve from 0.73232\n",
      "Epoch 482/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7275 - accuracy: 0.7754\n",
      "\n",
      "Epoch 00482: loss did not improve from 0.73232\n",
      "Epoch 483/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7488 - accuracy: 0.7744\n",
      "\n",
      "Epoch 00483: loss did not improve from 0.73232\n",
      "Epoch 484/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7166 - accuracy: 0.7865\n",
      "\n",
      "Epoch 00484: loss improved from 0.73232 to 0.72866, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 485/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7294 - accuracy: 0.7759\n",
      "\n",
      "Epoch 00485: loss did not improve from 0.72866\n",
      "Epoch 486/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7151 - accuracy: 0.7805\n",
      "\n",
      "Epoch 00486: loss did not improve from 0.72866\n",
      "Epoch 487/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7230 - accuracy: 0.7829\n",
      "\n",
      "Epoch 00487: loss improved from 0.72866 to 0.71854, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 488/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6910 - accuracy: 0.7897\n",
      "\n",
      "Epoch 00488: loss improved from 0.71854 to 0.71798, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 489/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7143 - accuracy: 0.7797\n",
      "\n",
      "Epoch 00489: loss did not improve from 0.71798\n",
      "Epoch 490/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7167 - accuracy: 0.7805\n",
      "\n",
      "Epoch 00490: loss did not improve from 0.71798\n",
      "Epoch 491/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7133 - accuracy: 0.7836\n",
      "\n",
      "Epoch 00491: loss improved from 0.71798 to 0.71094, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 492/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6965 - accuracy: 0.7883\n",
      "\n",
      "Epoch 00492: loss did not improve from 0.71094\n",
      "Epoch 493/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7243 - accuracy: 0.7866\n",
      "\n",
      "Epoch 00493: loss did not improve from 0.71094\n",
      "Epoch 494/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7084 - accuracy: 0.7844\n",
      "\n",
      "Epoch 00494: loss did not improve from 0.71094\n",
      "Epoch 495/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7467 - accuracy: 0.7731\n",
      "\n",
      "Epoch 00495: loss did not improve from 0.71094\n",
      "Epoch 496/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6995 - accuracy: 0.7872\n",
      "\n",
      "Epoch 00496: loss improved from 0.71094 to 0.70986, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 497/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6979 - accuracy: 0.7959\n",
      "\n",
      "Epoch 00497: loss did not improve from 0.70986\n",
      "Epoch 498/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6996 - accuracy: 0.7901\n",
      "\n",
      "Epoch 00498: loss improved from 0.70986 to 0.70493, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 499/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7075 - accuracy: 0.7915\n",
      "\n",
      "Epoch 00499: loss did not improve from 0.70493\n",
      "Epoch 500/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6978 - accuracy: 0.7900\n",
      "\n",
      "Epoch 00500: loss improved from 0.70493 to 0.70138, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 501/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.7255 - accuracy: 0.7848\n",
      "\n",
      "Epoch 00501: loss did not improve from 0.70138\n",
      "Epoch 502/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7121 - accuracy: 0.7955\n",
      "\n",
      "Epoch 00502: loss did not improve from 0.70138\n",
      "Epoch 503/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7024 - accuracy: 0.7871\n",
      "\n",
      "Epoch 00503: loss improved from 0.70138 to 0.70098, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 504/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7023 - accuracy: 0.7835\n",
      "\n",
      "Epoch 00504: loss did not improve from 0.70098\n",
      "Epoch 505/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6817 - accuracy: 0.7986\n",
      "\n",
      "Epoch 00505: loss improved from 0.70098 to 0.69408, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 506/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6759 - accuracy: 0.7961\n",
      "\n",
      "Epoch 00506: loss improved from 0.69408 to 0.69169, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 507/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6923 - accuracy: 0.7956\n",
      "\n",
      "Epoch 00507: loss did not improve from 0.69169\n",
      "Epoch 508/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7011 - accuracy: 0.7864\n",
      "\n",
      "Epoch 00508: loss did not improve from 0.69169\n",
      "Epoch 509/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6683 - accuracy: 0.7993\n",
      "\n",
      "Epoch 00509: loss improved from 0.69169 to 0.68846, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 510/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.7036 - accuracy: 0.7853\n",
      "\n",
      "Epoch 00510: loss did not improve from 0.68846\n",
      "Epoch 511/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6964 - accuracy: 0.7911\n",
      "\n",
      "Epoch 00511: loss improved from 0.68846 to 0.68449, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 512/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6860 - accuracy: 0.7904\n",
      "\n",
      "Epoch 00512: loss did not improve from 0.68449\n",
      "Epoch 513/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6854 - accuracy: 0.7936\n",
      "\n",
      "Epoch 00513: loss did not improve from 0.68449\n",
      "Epoch 514/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6830 - accuracy: 0.7978\n",
      "\n",
      "Epoch 00514: loss did not improve from 0.68449\n",
      "Epoch 515/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6583 - accuracy: 0.8026\n",
      "\n",
      "Epoch 00515: loss improved from 0.68449 to 0.67843, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 516/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6873 - accuracy: 0.7951\n",
      "\n",
      "Epoch 00516: loss did not improve from 0.67843\n",
      "Epoch 517/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6929 - accuracy: 0.7854\n",
      "\n",
      "Epoch 00517: loss did not improve from 0.67843\n",
      "Epoch 518/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6846 - accuracy: 0.7959\n",
      "\n",
      "Epoch 00518: loss did not improve from 0.67843\n",
      "Epoch 519/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6658 - accuracy: 0.7938\n",
      "\n",
      "Epoch 00519: loss did not improve from 0.67843\n",
      "Epoch 520/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6878 - accuracy: 0.7938\n",
      "\n",
      "Epoch 00520: loss improved from 0.67843 to 0.67625, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 521/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6585 - accuracy: 0.8021\n",
      "\n",
      "Epoch 00521: loss improved from 0.67625 to 0.67197, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 522/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6617 - accuracy: 0.8044\n",
      "\n",
      "Epoch 00522: loss improved from 0.67197 to 0.66813, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 523/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6660 - accuracy: 0.8023\n",
      "\n",
      "Epoch 00523: loss improved from 0.66813 to 0.66484, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 524/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6542 - accuracy: 0.8043\n",
      "\n",
      "Epoch 00524: loss did not improve from 0.66484\n",
      "Epoch 525/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6722 - accuracy: 0.7944\n",
      "\n",
      "Epoch 00525: loss did not improve from 0.66484\n",
      "Epoch 526/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6656 - accuracy: 0.8042\n",
      "\n",
      "Epoch 00526: loss did not improve from 0.66484\n",
      "Epoch 527/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6517 - accuracy: 0.8074\n",
      "\n",
      "Epoch 00527: loss improved from 0.66484 to 0.66241, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 528/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6754 - accuracy: 0.7984\n",
      "\n",
      "Epoch 00528: loss did not improve from 0.66241\n",
      "Epoch 529/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6383 - accuracy: 0.8078\n",
      "\n",
      "Epoch 00529: loss did not improve from 0.66241\n",
      "Epoch 530/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6458 - accuracy: 0.8090\n",
      "\n",
      "Epoch 00530: loss improved from 0.66241 to 0.65183, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 531/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6550 - accuracy: 0.8090\n",
      "\n",
      "Epoch 00531: loss did not improve from 0.65183\n",
      "Epoch 532/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6447 - accuracy: 0.8068\n",
      "\n",
      "Epoch 00532: loss did not improve from 0.65183\n",
      "Epoch 533/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6463 - accuracy: 0.8059\n",
      "\n",
      "Epoch 00533: loss did not improve from 0.65183\n",
      "Epoch 534/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6636 - accuracy: 0.8067\n",
      "\n",
      "Epoch 00534: loss did not improve from 0.65183\n",
      "Epoch 535/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6423 - accuracy: 0.8061\n",
      "\n",
      "Epoch 00535: loss did not improve from 0.65183\n",
      "Epoch 536/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6713 - accuracy: 0.7910\n",
      "\n",
      "Epoch 00536: loss did not improve from 0.65183\n",
      "Epoch 537/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6573 - accuracy: 0.7973\n",
      "\n",
      "Epoch 00537: loss did not improve from 0.65183\n",
      "Epoch 538/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6805 - accuracy: 0.7928\n",
      "\n",
      "Epoch 00538: loss did not improve from 0.65183\n",
      "Epoch 539/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6679 - accuracy: 0.8012\n",
      "\n",
      "Epoch 00539: loss did not improve from 0.65183\n",
      "Epoch 540/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6598 - accuracy: 0.7968\n",
      "\n",
      "Epoch 00540: loss did not improve from 0.65183\n",
      "Epoch 541/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6261 - accuracy: 0.8090\n",
      "\n",
      "Epoch 00541: loss improved from 0.65183 to 0.65165, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 542/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6612 - accuracy: 0.8060\n",
      "\n",
      "Epoch 00542: loss improved from 0.65165 to 0.64564, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 543/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6109 - accuracy: 0.8196\n",
      "\n",
      "Epoch 00543: loss improved from 0.64564 to 0.64256, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 544/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6337 - accuracy: 0.8076\n",
      "\n",
      "Epoch 00544: loss improved from 0.64256 to 0.64017, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 545/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6484 - accuracy: 0.8052\n",
      "\n",
      "Epoch 00545: loss improved from 0.64017 to 0.63615, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 546/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6303 - accuracy: 0.8116\n",
      "\n",
      "Epoch 00546: loss did not improve from 0.63615\n",
      "Epoch 547/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6296 - accuracy: 0.8144\n",
      "\n",
      "Epoch 00547: loss did not improve from 0.63615\n",
      "Epoch 548/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6352 - accuracy: 0.8069\n",
      "\n",
      "Epoch 00548: loss did not improve from 0.63615\n",
      "Epoch 549/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6602 - accuracy: 0.7997\n",
      "\n",
      "Epoch 00549: loss did not improve from 0.63615\n",
      "Epoch 550/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6503 - accuracy: 0.8034\n",
      "\n",
      "Epoch 00550: loss did not improve from 0.63615\n",
      "Epoch 551/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6273 - accuracy: 0.8155\n",
      "\n",
      "Epoch 00551: loss did not improve from 0.63615\n",
      "Epoch 552/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6323 - accuracy: 0.8112\n",
      "\n",
      "Epoch 00552: loss improved from 0.63615 to 0.62722, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 553/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6332 - accuracy: 0.8105\n",
      "\n",
      "Epoch 00553: loss did not improve from 0.62722\n",
      "Epoch 554/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6553 - accuracy: 0.8007\n",
      "\n",
      "Epoch 00554: loss did not improve from 0.62722\n",
      "Epoch 555/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5941 - accuracy: 0.8216\n",
      "\n",
      "Epoch 00555: loss improved from 0.62722 to 0.62649, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 556/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6129 - accuracy: 0.8129\n",
      "\n",
      "Epoch 00556: loss did not improve from 0.62649\n",
      "Epoch 557/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6265 - accuracy: 0.8102\n",
      "\n",
      "Epoch 00557: loss improved from 0.62649 to 0.62438, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 558/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6502 - accuracy: 0.8020\n",
      "\n",
      "Epoch 00558: loss did not improve from 0.62438\n",
      "Epoch 559/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6408 - accuracy: 0.8063\n",
      "\n",
      "Epoch 00559: loss did not improve from 0.62438\n",
      "Epoch 560/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6457 - accuracy: 0.8027\n",
      "\n",
      "Epoch 00560: loss did not improve from 0.62438\n",
      "Epoch 561/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6425 - accuracy: 0.8043\n",
      "\n",
      "Epoch 00561: loss did not improve from 0.62438\n",
      "Epoch 562/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6152 - accuracy: 0.8164\n",
      "\n",
      "Epoch 00562: loss improved from 0.62438 to 0.62178, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 563/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5838 - accuracy: 0.8267\n",
      "\n",
      "Epoch 00563: loss improved from 0.62178 to 0.61517, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 564/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6124 - accuracy: 0.8128\n",
      "\n",
      "Epoch 00564: loss improved from 0.61517 to 0.61092, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 565/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6145 - accuracy: 0.8125\n",
      "\n",
      "Epoch 00565: loss did not improve from 0.61092\n",
      "Epoch 566/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6113 - accuracy: 0.8163\n",
      "\n",
      "Epoch 00566: loss did not improve from 0.61092\n",
      "Epoch 567/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6042 - accuracy: 0.8180\n",
      "\n",
      "Epoch 00567: loss did not improve from 0.61092\n",
      "Epoch 568/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5909 - accuracy: 0.8151\n",
      "\n",
      "Epoch 00568: loss did not improve from 0.61092\n",
      "Epoch 569/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6268 - accuracy: 0.8086\n",
      "\n",
      "Epoch 00569: loss did not improve from 0.61092\n",
      "Epoch 570/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6165 - accuracy: 0.8111\n",
      "\n",
      "Epoch 00570: loss did not improve from 0.61092\n",
      "Epoch 571/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6103 - accuracy: 0.8138\n",
      "\n",
      "Epoch 00571: loss did not improve from 0.61092\n",
      "Epoch 572/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6352 - accuracy: 0.8113\n",
      "\n",
      "Epoch 00572: loss did not improve from 0.61092\n",
      "Epoch 573/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5981 - accuracy: 0.8207\n",
      "\n",
      "Epoch 00573: loss did not improve from 0.61092\n",
      "Epoch 574/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.5915 - accuracy: 0.8235\n",
      "\n",
      "Epoch 00574: loss improved from 0.61092 to 0.60576, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 575/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5730 - accuracy: 0.8237\n",
      "\n",
      "Epoch 00575: loss did not improve from 0.60576\n",
      "Epoch 576/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6018 - accuracy: 0.8174\n",
      "\n",
      "Epoch 00576: loss did not improve from 0.60576\n",
      "Epoch 577/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.6033 - accuracy: 0.8153\n",
      "\n",
      "Epoch 00577: loss did not improve from 0.60576\n",
      "Epoch 578/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.5951 - accuracy: 0.8197\n",
      "\n",
      "Epoch 00578: loss did not improve from 0.60576\n",
      "Epoch 579/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6194 - accuracy: 0.8100\n",
      "\n",
      "Epoch 00579: loss improved from 0.60576 to 0.60380, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 580/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5941 - accuracy: 0.8214\n",
      "\n",
      "Epoch 00580: loss did not improve from 0.60380\n",
      "Epoch 581/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6158 - accuracy: 0.8036\n",
      "\n",
      "Epoch 00581: loss did not improve from 0.60380\n",
      "Epoch 582/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6172 - accuracy: 0.8116\n",
      "\n",
      "Epoch 00582: loss did not improve from 0.60380\n",
      "Epoch 583/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6067 - accuracy: 0.8101\n",
      "\n",
      "Epoch 00583: loss did not improve from 0.60380\n",
      "Epoch 584/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6017 - accuracy: 0.8174\n",
      "\n",
      "Epoch 00584: loss did not improve from 0.60380\n",
      "Epoch 585/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6129 - accuracy: 0.8184\n",
      "\n",
      "Epoch 00585: loss improved from 0.60380 to 0.59465, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 586/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5876 - accuracy: 0.8192\n",
      "\n",
      "Epoch 00586: loss improved from 0.59465 to 0.59050, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 587/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5728 - accuracy: 0.8304\n",
      "\n",
      "Epoch 00587: loss did not improve from 0.59050\n",
      "Epoch 588/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5786 - accuracy: 0.8197\n",
      "\n",
      "Epoch 00588: loss improved from 0.59050 to 0.58921, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 589/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5884 - accuracy: 0.8243\n",
      "\n",
      "Epoch 00589: loss improved from 0.58921 to 0.58744, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 590/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5690 - accuracy: 0.8319\n",
      "\n",
      "Epoch 00590: loss improved from 0.58744 to 0.58364, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 591/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5833 - accuracy: 0.8256\n",
      "\n",
      "Epoch 00591: loss improved from 0.58364 to 0.58301, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 592/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5924 - accuracy: 0.8210\n",
      "\n",
      "Epoch 00592: loss improved from 0.58301 to 0.58025, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 593/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5717 - accuracy: 0.8246\n",
      "\n",
      "Epoch 00593: loss did not improve from 0.58025\n",
      "Epoch 594/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6066 - accuracy: 0.8151\n",
      "\n",
      "Epoch 00594: loss did not improve from 0.58025\n",
      "Epoch 595/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5800 - accuracy: 0.8214\n",
      "\n",
      "Epoch 00595: loss did not improve from 0.58025\n",
      "Epoch 596/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5796 - accuracy: 0.8238\n",
      "\n",
      "Epoch 00596: loss did not improve from 0.58025\n",
      "Epoch 597/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6017 - accuracy: 0.8184\n",
      "\n",
      "Epoch 00597: loss did not improve from 0.58025\n",
      "Epoch 598/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.6047 - accuracy: 0.8153\n",
      "\n",
      "Epoch 00598: loss did not improve from 0.58025\n",
      "Epoch 599/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5746 - accuracy: 0.8233\n",
      "\n",
      "Epoch 00599: loss did not improve from 0.58025\n",
      "Epoch 600/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5912 - accuracy: 0.8205\n",
      "\n",
      "Epoch 00600: loss improved from 0.58025 to 0.57907, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 601/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5604 - accuracy: 0.8281\n",
      "\n",
      "Epoch 00601: loss improved from 0.57907 to 0.57501, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 602/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5800 - accuracy: 0.8223\n",
      "\n",
      "Epoch 00602: loss did not improve from 0.57501\n",
      "Epoch 603/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5766 - accuracy: 0.8185\n",
      "\n",
      "Epoch 00603: loss improved from 0.57501 to 0.57248, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 604/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5590 - accuracy: 0.8275\n",
      "\n",
      "Epoch 00604: loss improved from 0.57248 to 0.56677, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 605/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5442 - accuracy: 0.8316\n",
      "\n",
      "Epoch 00605: loss did not improve from 0.56677\n",
      "Epoch 606/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5681 - accuracy: 0.8253\n",
      "\n",
      "Epoch 00606: loss did not improve from 0.56677\n",
      "Epoch 607/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5655 - accuracy: 0.8281\n",
      "\n",
      "Epoch 00607: loss improved from 0.56677 to 0.56311, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 608/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5559 - accuracy: 0.8309\n",
      "\n",
      "Epoch 00608: loss did not improve from 0.56311\n",
      "Epoch 609/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5567 - accuracy: 0.8273\n",
      "\n",
      "Epoch 00609: loss did not improve from 0.56311\n",
      "Epoch 610/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5675 - accuracy: 0.8284\n",
      "\n",
      "Epoch 00610: loss did not improve from 0.56311\n",
      "Epoch 611/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5597 - accuracy: 0.8293\n",
      "\n",
      "Epoch 00611: loss did not improve from 0.56311\n",
      "Epoch 612/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5653 - accuracy: 0.8255\n",
      "\n",
      "Epoch 00612: loss did not improve from 0.56311\n",
      "Epoch 613/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5683 - accuracy: 0.8273\n",
      "\n",
      "Epoch 00613: loss did not improve from 0.56311\n",
      "Epoch 614/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5754 - accuracy: 0.8173\n",
      "\n",
      "Epoch 00614: loss did not improve from 0.56311\n",
      "Epoch 615/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5304 - accuracy: 0.8403\n",
      "\n",
      "Epoch 00615: loss did not improve from 0.56311\n",
      "Epoch 616/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5649 - accuracy: 0.8292\n",
      "\n",
      "Epoch 00616: loss did not improve from 0.56311\n",
      "Epoch 617/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5745 - accuracy: 0.8227\n",
      "\n",
      "Epoch 00617: loss did not improve from 0.56311\n",
      "Epoch 618/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5468 - accuracy: 0.8325\n",
      "\n",
      "Epoch 00618: loss improved from 0.56311 to 0.56251, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 619/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5698 - accuracy: 0.8224\n",
      "\n",
      "Epoch 00619: loss improved from 0.56251 to 0.55924, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 620/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5624 - accuracy: 0.8296\n",
      "\n",
      "Epoch 00620: loss did not improve from 0.55924\n",
      "Epoch 621/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5566 - accuracy: 0.8294\n",
      "\n",
      "Epoch 00621: loss improved from 0.55924 to 0.55730, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 622/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5580 - accuracy: 0.8247\n",
      "\n",
      "Epoch 00622: loss did not improve from 0.55730\n",
      "Epoch 623/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5435 - accuracy: 0.8361\n",
      "\n",
      "Epoch 00623: loss improved from 0.55730 to 0.55351, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 624/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5461 - accuracy: 0.8348\n",
      "\n",
      "Epoch 00624: loss improved from 0.55351 to 0.54784, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 625/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5384 - accuracy: 0.8330\n",
      "\n",
      "Epoch 00625: loss improved from 0.54784 to 0.54037, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 626/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5563 - accuracy: 0.8298\n",
      "\n",
      "Epoch 00626: loss did not improve from 0.54037\n",
      "Epoch 627/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5514 - accuracy: 0.8332\n",
      "\n",
      "Epoch 00627: loss did not improve from 0.54037\n",
      "Epoch 628/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5488 - accuracy: 0.8380\n",
      "\n",
      "Epoch 00628: loss did not improve from 0.54037\n",
      "Epoch 629/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5356 - accuracy: 0.8370\n",
      "\n",
      "Epoch 00629: loss did not improve from 0.54037\n",
      "Epoch 630/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5478 - accuracy: 0.8344\n",
      "\n",
      "Epoch 00630: loss did not improve from 0.54037\n",
      "Epoch 631/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5448 - accuracy: 0.8331\n",
      "\n",
      "Epoch 00631: loss did not improve from 0.54037\n",
      "Epoch 632/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5424 - accuracy: 0.8355\n",
      "\n",
      "Epoch 00632: loss did not improve from 0.54037\n",
      "Epoch 633/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5294 - accuracy: 0.8366\n",
      "\n",
      "Epoch 00633: loss did not improve from 0.54037\n",
      "Epoch 634/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5308 - accuracy: 0.8370\n",
      "\n",
      "Epoch 00634: loss improved from 0.54037 to 0.53806, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 635/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5366 - accuracy: 0.8391\n",
      "\n",
      "Epoch 00635: loss improved from 0.53806 to 0.53772, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 636/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5360 - accuracy: 0.8313\n",
      "\n",
      "Epoch 00636: loss improved from 0.53772 to 0.53738, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 637/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5199 - accuracy: 0.8433\n",
      "\n",
      "Epoch 00637: loss improved from 0.53738 to 0.53216, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 638/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5300 - accuracy: 0.8382\n",
      "\n",
      "Epoch 00638: loss did not improve from 0.53216\n",
      "Epoch 639/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5297 - accuracy: 0.8379\n",
      "\n",
      "Epoch 00639: loss did not improve from 0.53216\n",
      "Epoch 640/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5237 - accuracy: 0.8384\n",
      "\n",
      "Epoch 00640: loss did not improve from 0.53216\n",
      "Epoch 641/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5205 - accuracy: 0.8434\n",
      "\n",
      "Epoch 00641: loss did not improve from 0.53216\n",
      "Epoch 642/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5453 - accuracy: 0.8273\n",
      "\n",
      "Epoch 00642: loss did not improve from 0.53216\n",
      "Epoch 643/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5079 - accuracy: 0.8439\n",
      "\n",
      "Epoch 00643: loss did not improve from 0.53216\n",
      "Epoch 644/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5289 - accuracy: 0.8350\n",
      "\n",
      "Epoch 00644: loss improved from 0.53216 to 0.52907, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 645/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5315 - accuracy: 0.8394\n",
      "\n",
      "Epoch 00645: loss improved from 0.52907 to 0.52665, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 646/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5148 - accuracy: 0.8378\n",
      "\n",
      "Epoch 00646: loss improved from 0.52665 to 0.52518, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 647/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5130 - accuracy: 0.8457\n",
      "\n",
      "Epoch 00647: loss improved from 0.52518 to 0.52043, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 648/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5229 - accuracy: 0.8428\n",
      "\n",
      "Epoch 00648: loss did not improve from 0.52043\n",
      "Epoch 649/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5263 - accuracy: 0.8414\n",
      "\n",
      "Epoch 00649: loss did not improve from 0.52043\n",
      "Epoch 650/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5435 - accuracy: 0.8341\n",
      "\n",
      "Epoch 00650: loss did not improve from 0.52043\n",
      "Epoch 651/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5434 - accuracy: 0.8290\n",
      "\n",
      "Epoch 00651: loss did not improve from 0.52043\n",
      "Epoch 652/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5264 - accuracy: 0.8293\n",
      "\n",
      "Epoch 00652: loss did not improve from 0.52043\n",
      "Epoch 653/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5427 - accuracy: 0.8317\n",
      "\n",
      "Epoch 00653: loss did not improve from 0.52043\n",
      "Epoch 654/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5060 - accuracy: 0.8391\n",
      "\n",
      "Epoch 00654: loss did not improve from 0.52043\n",
      "Epoch 655/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5834 - accuracy: 0.8186\n",
      "\n",
      "Epoch 00655: loss did not improve from 0.52043\n",
      "Epoch 656/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5517 - accuracy: 0.8311\n",
      "\n",
      "Epoch 00656: loss did not improve from 0.52043\n",
      "Epoch 657/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5155 - accuracy: 0.8438\n",
      "\n",
      "Epoch 00657: loss did not improve from 0.52043\n",
      "Epoch 658/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5147 - accuracy: 0.8463\n",
      "\n",
      "Epoch 00658: loss did not improve from 0.52043\n",
      "Epoch 659/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5350 - accuracy: 0.8355\n",
      "\n",
      "Epoch 00659: loss improved from 0.52043 to 0.51984, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 660/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5038 - accuracy: 0.8401\n",
      "\n",
      "Epoch 00660: loss did not improve from 0.51984\n",
      "Epoch 661/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5232 - accuracy: 0.8447\n",
      "\n",
      "Epoch 00661: loss did not improve from 0.51984\n",
      "Epoch 662/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5291 - accuracy: 0.8321\n",
      "\n",
      "Epoch 00662: loss did not improve from 0.51984\n",
      "Epoch 663/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4981 - accuracy: 0.8459\n",
      "\n",
      "Epoch 00663: loss improved from 0.51984 to 0.51448, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 664/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5165 - accuracy: 0.8378\n",
      "\n",
      "Epoch 00664: loss improved from 0.51448 to 0.50953, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 665/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5053 - accuracy: 0.8438\n",
      "\n",
      "Epoch 00665: loss did not improve from 0.50953\n",
      "Epoch 666/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5234 - accuracy: 0.8413\n",
      "\n",
      "Epoch 00666: loss did not improve from 0.50953\n",
      "Epoch 667/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5227 - accuracy: 0.8381\n",
      "\n",
      "Epoch 00667: loss improved from 0.50953 to 0.50859, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 668/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4890 - accuracy: 0.8485\n",
      "\n",
      "Epoch 00668: loss improved from 0.50859 to 0.50516, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 669/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4939 - accuracy: 0.8462\n",
      "\n",
      "Epoch 00669: loss improved from 0.50516 to 0.50300, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 670/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4924 - accuracy: 0.8513\n",
      "\n",
      "Epoch 00670: loss improved from 0.50300 to 0.50156, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 671/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4864 - accuracy: 0.8498\n",
      "\n",
      "Epoch 00671: loss did not improve from 0.50156\n",
      "Epoch 672/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5049 - accuracy: 0.8475\n",
      "\n",
      "Epoch 00672: loss did not improve from 0.50156\n",
      "Epoch 673/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4932 - accuracy: 0.8510\n",
      "\n",
      "Epoch 00673: loss did not improve from 0.50156\n",
      "Epoch 674/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5073 - accuracy: 0.8393\n",
      "\n",
      "Epoch 00674: loss did not improve from 0.50156\n",
      "Epoch 675/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5053 - accuracy: 0.8427\n",
      "\n",
      "Epoch 00675: loss improved from 0.50156 to 0.50094, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 676/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5071 - accuracy: 0.8481\n",
      "\n",
      "Epoch 00676: loss did not improve from 0.50094\n",
      "Epoch 677/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4947 - accuracy: 0.8499\n",
      "\n",
      "Epoch 00677: loss did not improve from 0.50094\n",
      "Epoch 678/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5132 - accuracy: 0.8445\n",
      "\n",
      "Epoch 00678: loss did not improve from 0.50094\n",
      "Epoch 679/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4932 - accuracy: 0.8471\n",
      "\n",
      "Epoch 00679: loss did not improve from 0.50094\n",
      "Epoch 680/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4963 - accuracy: 0.8458\n",
      "\n",
      "Epoch 00680: loss did not improve from 0.50094\n",
      "Epoch 681/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5195 - accuracy: 0.8400\n",
      "\n",
      "Epoch 00681: loss did not improve from 0.50094\n",
      "Epoch 682/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5074 - accuracy: 0.8467\n",
      "\n",
      "Epoch 00682: loss improved from 0.50094 to 0.49896, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 683/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4924 - accuracy: 0.8460\n",
      "\n",
      "Epoch 00683: loss did not improve from 0.49896\n",
      "Epoch 684/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5289 - accuracy: 0.8412\n",
      "\n",
      "Epoch 00684: loss did not improve from 0.49896\n",
      "Epoch 685/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4924 - accuracy: 0.8455\n",
      "\n",
      "Epoch 00685: loss did not improve from 0.49896\n",
      "Epoch 686/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5144 - accuracy: 0.8427\n",
      "\n",
      "Epoch 00686: loss did not improve from 0.49896\n",
      "Epoch 687/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4860 - accuracy: 0.8485\n",
      "\n",
      "Epoch 00687: loss did not improve from 0.49896\n",
      "Epoch 688/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4924 - accuracy: 0.8432\n",
      "\n",
      "Epoch 00688: loss did not improve from 0.49896\n",
      "Epoch 689/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4924 - accuracy: 0.8445\n",
      "\n",
      "Epoch 00689: loss improved from 0.49896 to 0.48812, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 690/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4731 - accuracy: 0.8491\n",
      "\n",
      "Epoch 00690: loss improved from 0.48812 to 0.48587, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 691/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4616 - accuracy: 0.8536\n",
      "\n",
      "Epoch 00691: loss improved from 0.48587 to 0.48279, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 692/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4744 - accuracy: 0.8540\n",
      "\n",
      "Epoch 00692: loss improved from 0.48279 to 0.48204, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 693/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4920 - accuracy: 0.8459\n",
      "\n",
      "Epoch 00693: loss did not improve from 0.48204\n",
      "Epoch 694/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4760 - accuracy: 0.8535\n",
      "\n",
      "Epoch 00694: loss did not improve from 0.48204\n",
      "Epoch 695/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4801 - accuracy: 0.8530\n",
      "\n",
      "Epoch 00695: loss did not improve from 0.48204\n",
      "Epoch 696/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5097 - accuracy: 0.8389\n",
      "\n",
      "Epoch 00696: loss did not improve from 0.48204\n",
      "Epoch 697/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4948 - accuracy: 0.8466\n",
      "\n",
      "Epoch 00697: loss did not improve from 0.48204\n",
      "Epoch 698/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5047 - accuracy: 0.8441\n",
      "\n",
      "Epoch 00698: loss did not improve from 0.48204\n",
      "Epoch 699/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5375 - accuracy: 0.8321\n",
      "\n",
      "Epoch 00699: loss did not improve from 0.48204\n",
      "Epoch 700/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4918 - accuracy: 0.8450\n",
      "\n",
      "Epoch 00700: loss did not improve from 0.48204\n",
      "Epoch 701/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4892 - accuracy: 0.8510\n",
      "\n",
      "Epoch 00701: loss improved from 0.48204 to 0.47834, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 702/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4788 - accuracy: 0.8508\n",
      "\n",
      "Epoch 00702: loss improved from 0.47834 to 0.47645, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 703/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4845 - accuracy: 0.8494\n",
      "\n",
      "Epoch 00703: loss did not improve from 0.47645\n",
      "Epoch 704/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4693 - accuracy: 0.8543\n",
      "\n",
      "Epoch 00704: loss improved from 0.47645 to 0.47629, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 705/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4638 - accuracy: 0.8558\n",
      "\n",
      "Epoch 00705: loss improved from 0.47629 to 0.47477, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 706/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4769 - accuracy: 0.8447\n",
      "\n",
      "Epoch 00706: loss did not improve from 0.47477\n",
      "Epoch 707/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4808 - accuracy: 0.8525\n",
      "\n",
      "Epoch 00707: loss did not improve from 0.47477\n",
      "Epoch 708/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4668 - accuracy: 0.8545\n",
      "\n",
      "Epoch 00708: loss improved from 0.47477 to 0.47351, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 709/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4791 - accuracy: 0.8534\n",
      "\n",
      "Epoch 00709: loss improved from 0.47351 to 0.47204, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 710/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4827 - accuracy: 0.8473\n",
      "\n",
      "Epoch 00710: loss did not improve from 0.47204\n",
      "Epoch 711/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4814 - accuracy: 0.8459\n",
      "\n",
      "Epoch 00711: loss did not improve from 0.47204\n",
      "Epoch 712/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4694 - accuracy: 0.8481\n",
      "\n",
      "Epoch 00712: loss did not improve from 0.47204\n",
      "Epoch 713/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4810 - accuracy: 0.8455\n",
      "\n",
      "Epoch 00713: loss did not improve from 0.47204\n",
      "Epoch 714/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4967 - accuracy: 0.8429\n",
      "\n",
      "Epoch 00714: loss did not improve from 0.47204\n",
      "Epoch 715/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4744 - accuracy: 0.8540\n",
      "\n",
      "Epoch 00715: loss did not improve from 0.47204\n",
      "Epoch 716/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4715 - accuracy: 0.8540\n",
      "\n",
      "Epoch 00716: loss did not improve from 0.47204\n",
      "Epoch 717/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4521 - accuracy: 0.8564\n",
      "\n",
      "Epoch 00717: loss improved from 0.47204 to 0.46505, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 718/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4662 - accuracy: 0.8554\n",
      "\n",
      "Epoch 00718: loss did not improve from 0.46505\n",
      "Epoch 719/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4590 - accuracy: 0.8657\n",
      "\n",
      "Epoch 00719: loss did not improve from 0.46505\n",
      "Epoch 720/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4710 - accuracy: 0.8518\n",
      "\n",
      "Epoch 00720: loss did not improve from 0.46505\n",
      "Epoch 721/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4671 - accuracy: 0.8478\n",
      "\n",
      "Epoch 00721: loss did not improve from 0.46505\n",
      "Epoch 722/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4643 - accuracy: 0.8573\n",
      "\n",
      "Epoch 00722: loss did not improve from 0.46505\n",
      "Epoch 723/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4722 - accuracy: 0.8500\n",
      "\n",
      "Epoch 00723: loss did not improve from 0.46505\n",
      "Epoch 724/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4457 - accuracy: 0.8595\n",
      "\n",
      "Epoch 00724: loss did not improve from 0.46505\n",
      "Epoch 725/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4445 - accuracy: 0.8634\n",
      "\n",
      "Epoch 00725: loss improved from 0.46505 to 0.46239, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 726/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4580 - accuracy: 0.8565\n",
      "\n",
      "Epoch 00726: loss improved from 0.46239 to 0.45780, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 727/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4411 - accuracy: 0.8631\n",
      "\n",
      "Epoch 00727: loss improved from 0.45780 to 0.45777, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 728/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4382 - accuracy: 0.8645\n",
      "\n",
      "Epoch 00728: loss improved from 0.45777 to 0.45500, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 729/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4444 - accuracy: 0.8571\n",
      "\n",
      "Epoch 00729: loss improved from 0.45500 to 0.45440, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 730/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4384 - accuracy: 0.8634\n",
      "\n",
      "Epoch 00730: loss did not improve from 0.45440\n",
      "Epoch 731/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4649 - accuracy: 0.8496\n",
      "\n",
      "Epoch 00731: loss did not improve from 0.45440\n",
      "Epoch 732/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4544 - accuracy: 0.8634\n",
      "\n",
      "Epoch 00732: loss did not improve from 0.45440\n",
      "Epoch 733/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4505 - accuracy: 0.8604\n",
      "\n",
      "Epoch 00733: loss did not improve from 0.45440\n",
      "Epoch 734/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4421 - accuracy: 0.8619\n",
      "\n",
      "Epoch 00734: loss did not improve from 0.45440\n",
      "Epoch 735/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4395 - accuracy: 0.8590\n",
      "\n",
      "Epoch 00735: loss did not improve from 0.45440\n",
      "Epoch 736/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4497 - accuracy: 0.8585\n",
      "\n",
      "Epoch 00736: loss improved from 0.45440 to 0.45221, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 737/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4494 - accuracy: 0.8601\n",
      "\n",
      "Epoch 00737: loss did not improve from 0.45221\n",
      "Epoch 738/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4548 - accuracy: 0.8568\n",
      "\n",
      "Epoch 00738: loss did not improve from 0.45221\n",
      "Epoch 739/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4490 - accuracy: 0.8597\n",
      "\n",
      "Epoch 00739: loss did not improve from 0.45221\n",
      "Epoch 740/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4552 - accuracy: 0.8619\n",
      "\n",
      "Epoch 00740: loss did not improve from 0.45221\n",
      "Epoch 741/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4533 - accuracy: 0.8552\n",
      "\n",
      "Epoch 00741: loss improved from 0.45221 to 0.45008, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 742/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4339 - accuracy: 0.8617\n",
      "\n",
      "Epoch 00742: loss improved from 0.45008 to 0.44413, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 743/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4406 - accuracy: 0.8635\n",
      "\n",
      "Epoch 00743: loss improved from 0.44413 to 0.44176, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 744/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4376 - accuracy: 0.8674\n",
      "\n",
      "Epoch 00744: loss did not improve from 0.44176\n",
      "Epoch 745/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4379 - accuracy: 0.8617\n",
      "\n",
      "Epoch 00745: loss did not improve from 0.44176\n",
      "Epoch 746/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4499 - accuracy: 0.8623\n",
      "\n",
      "Epoch 00746: loss did not improve from 0.44176\n",
      "Epoch 747/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4454 - accuracy: 0.8605\n",
      "\n",
      "Epoch 00747: loss did not improve from 0.44176\n",
      "Epoch 748/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4659 - accuracy: 0.8532\n",
      "\n",
      "Epoch 00748: loss did not improve from 0.44176\n",
      "Epoch 749/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.4517 - accuracy: 0.8582\n",
      "\n",
      "Epoch 00749: loss did not improve from 0.44176\n",
      "Epoch 750/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.4663 - accuracy: 0.8576\n",
      "\n",
      "Epoch 00750: loss did not improve from 0.44176\n",
      "Epoch 751/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4430 - accuracy: 0.8584\n",
      "\n",
      "Epoch 00751: loss did not improve from 0.44176\n",
      "Epoch 752/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4488 - accuracy: 0.8529\n",
      "\n",
      "Epoch 00752: loss did not improve from 0.44176\n",
      "Epoch 753/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4580 - accuracy: 0.8574\n",
      "\n",
      "Epoch 00753: loss did not improve from 0.44176\n",
      "Epoch 754/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4711 - accuracy: 0.8477\n",
      "\n",
      "Epoch 00754: loss did not improve from 0.44176\n",
      "Epoch 755/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.4971 - accuracy: 0.8442\n",
      "\n",
      "Epoch 00755: loss did not improve from 0.44176\n",
      "Epoch 756/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.4574 - accuracy: 0.8564\n",
      "\n",
      "Epoch 00756: loss did not improve from 0.44176\n",
      "Epoch 757/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4475 - accuracy: 0.8564\n",
      "\n",
      "Epoch 00757: loss did not improve from 0.44176\n",
      "Epoch 758/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4466 - accuracy: 0.8640\n",
      "\n",
      "Epoch 00758: loss did not improve from 0.44176\n",
      "Epoch 759/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4517 - accuracy: 0.8601\n",
      "\n",
      "Epoch 00759: loss did not improve from 0.44176\n",
      "Epoch 760/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4481 - accuracy: 0.8578\n",
      "\n",
      "Epoch 00760: loss did not improve from 0.44176\n",
      "Epoch 761/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4417 - accuracy: 0.8628\n",
      "\n",
      "Epoch 00761: loss improved from 0.44176 to 0.43965, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 762/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4413 - accuracy: 0.8604\n",
      "\n",
      "Epoch 00762: loss did not improve from 0.43965\n",
      "Epoch 763/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.4349 - accuracy: 0.8606\n",
      "\n",
      "Epoch 00763: loss did not improve from 0.43965\n",
      "Epoch 764/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4218 - accuracy: 0.8682\n",
      "\n",
      "Epoch 00764: loss improved from 0.43965 to 0.43381, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 765/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.4180 - accuracy: 0.8718\n",
      "\n",
      "Epoch 00765: loss improved from 0.43381 to 0.43182, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 766/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.4172 - accuracy: 0.8719\n",
      "\n",
      "Epoch 00766: loss improved from 0.43182 to 0.42600, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 767/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4069 - accuracy: 0.8711\n",
      "\n",
      "Epoch 00767: loss improved from 0.42600 to 0.42468, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 768/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4065 - accuracy: 0.8726\n",
      "\n",
      "Epoch 00768: loss improved from 0.42468 to 0.42123, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 769/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4230 - accuracy: 0.8669\n",
      "\n",
      "Epoch 00769: loss did not improve from 0.42123\n",
      "Epoch 770/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4272 - accuracy: 0.8641\n",
      "\n",
      "Epoch 00770: loss did not improve from 0.42123\n",
      "Epoch 771/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.4252 - accuracy: 0.8702\n",
      "\n",
      "Epoch 00771: loss did not improve from 0.42123\n",
      "Epoch 772/2000\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.4523 - accuracy: 0.8579\n",
      "\n",
      "Epoch 00772: loss did not improve from 0.42123\n",
      "Epoch 773/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4369 - accuracy: 0.8603\n",
      "\n",
      "Epoch 00773: loss did not improve from 0.42123\n",
      "Epoch 774/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4426 - accuracy: 0.8585\n",
      "\n",
      "Epoch 00774: loss did not improve from 0.42123\n",
      "Epoch 775/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4411 - accuracy: 0.8610\n",
      "\n",
      "Epoch 00775: loss did not improve from 0.42123\n",
      "Epoch 776/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.4435 - accuracy: 0.8604\n",
      "\n",
      "Epoch 00776: loss did not improve from 0.42123\n",
      "Epoch 777/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5559 - accuracy: 0.8137\n",
      "\n",
      "Epoch 00777: loss did not improve from 0.42123\n",
      "Epoch 778/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.5244 - accuracy: 0.8264\n",
      "\n",
      "Epoch 00778: loss did not improve from 0.42123\n",
      "Epoch 779/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4798 - accuracy: 0.8494\n",
      "\n",
      "Epoch 00779: loss did not improve from 0.42123\n",
      "Epoch 780/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4426 - accuracy: 0.8613\n",
      "\n",
      "Epoch 00780: loss did not improve from 0.42123\n",
      "Epoch 781/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4271 - accuracy: 0.8628\n",
      "\n",
      "Epoch 00781: loss did not improve from 0.42123\n",
      "Epoch 782/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4277 - accuracy: 0.8651\n",
      "\n",
      "Epoch 00782: loss did not improve from 0.42123\n",
      "Epoch 783/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4120 - accuracy: 0.8728\n",
      "\n",
      "Epoch 00783: loss did not improve from 0.42123\n",
      "Epoch 784/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4290 - accuracy: 0.8606\n",
      "\n",
      "Epoch 00784: loss improved from 0.42123 to 0.42038, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 785/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4246 - accuracy: 0.8682\n",
      "\n",
      "Epoch 00785: loss improved from 0.42038 to 0.41893, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 786/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3990 - accuracy: 0.8711\n",
      "\n",
      "Epoch 00786: loss improved from 0.41893 to 0.41396, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 787/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3934 - accuracy: 0.8850\n",
      "\n",
      "Epoch 00787: loss improved from 0.41396 to 0.41189, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 788/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4141 - accuracy: 0.8688\n",
      "\n",
      "Epoch 00788: loss improved from 0.41189 to 0.41122, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 789/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4287 - accuracy: 0.8628\n",
      "\n",
      "Epoch 00789: loss did not improve from 0.41122\n",
      "Epoch 790/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4142 - accuracy: 0.8678\n",
      "\n",
      "Epoch 00790: loss did not improve from 0.41122\n",
      "Epoch 791/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4033 - accuracy: 0.8756\n",
      "\n",
      "Epoch 00791: loss improved from 0.41122 to 0.41064, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 792/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4034 - accuracy: 0.8752\n",
      "\n",
      "Epoch 00792: loss improved from 0.41064 to 0.40777, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 793/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4222 - accuracy: 0.8619\n",
      "\n",
      "Epoch 00793: loss did not improve from 0.40777\n",
      "Epoch 794/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4018 - accuracy: 0.8731\n",
      "\n",
      "Epoch 00794: loss did not improve from 0.40777\n",
      "Epoch 795/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4005 - accuracy: 0.8731\n",
      "\n",
      "Epoch 00795: loss did not improve from 0.40777\n",
      "Epoch 796/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4207 - accuracy: 0.8682\n",
      "\n",
      "Epoch 00796: loss did not improve from 0.40777\n",
      "Epoch 797/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4038 - accuracy: 0.8685\n",
      "\n",
      "Epoch 00797: loss did not improve from 0.40777\n",
      "Epoch 798/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.4388 - accuracy: 0.8593\n",
      "\n",
      "Epoch 00798: loss did not improve from 0.40777\n",
      "Epoch 799/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4308 - accuracy: 0.8619\n",
      "\n",
      "Epoch 00799: loss did not improve from 0.40777\n",
      "Epoch 800/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.4131 - accuracy: 0.8703\n",
      "\n",
      "Epoch 00800: loss did not improve from 0.40777\n",
      "Epoch 801/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4083 - accuracy: 0.8680\n",
      "\n",
      "Epoch 00801: loss did not improve from 0.40777\n",
      "Epoch 802/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4106 - accuracy: 0.8702\n",
      "\n",
      "Epoch 00802: loss did not improve from 0.40777\n",
      "Epoch 803/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4074 - accuracy: 0.8657\n",
      "\n",
      "Epoch 00803: loss did not improve from 0.40777\n",
      "Epoch 804/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3985 - accuracy: 0.8711\n",
      "\n",
      "Epoch 00804: loss improved from 0.40777 to 0.40492, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 805/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4051 - accuracy: 0.8714\n",
      "\n",
      "Epoch 00805: loss improved from 0.40492 to 0.40408, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 806/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4039 - accuracy: 0.8723\n",
      "\n",
      "Epoch 00806: loss did not improve from 0.40408\n",
      "Epoch 807/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4094 - accuracy: 0.8714\n",
      "\n",
      "Epoch 00807: loss improved from 0.40408 to 0.40058, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 808/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3985 - accuracy: 0.8708\n",
      "\n",
      "Epoch 00808: loss did not improve from 0.40058\n",
      "Epoch 809/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4017 - accuracy: 0.8716\n",
      "\n",
      "Epoch 00809: loss did not improve from 0.40058\n",
      "Epoch 810/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4067 - accuracy: 0.8695\n",
      "\n",
      "Epoch 00810: loss improved from 0.40058 to 0.39811, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 811/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3982 - accuracy: 0.8710\n",
      "\n",
      "Epoch 00811: loss did not improve from 0.39811\n",
      "Epoch 812/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3915 - accuracy: 0.8789\n",
      "\n",
      "Epoch 00812: loss did not improve from 0.39811\n",
      "Epoch 813/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4019 - accuracy: 0.8695\n",
      "\n",
      "Epoch 00813: loss did not improve from 0.39811\n",
      "Epoch 814/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4147 - accuracy: 0.8652\n",
      "\n",
      "Epoch 00814: loss did not improve from 0.39811\n",
      "Epoch 815/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4181 - accuracy: 0.8653\n",
      "\n",
      "Epoch 00815: loss did not improve from 0.39811\n",
      "Epoch 816/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3990 - accuracy: 0.8704\n",
      "\n",
      "Epoch 00816: loss did not improve from 0.39811\n",
      "Epoch 817/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4019 - accuracy: 0.8698\n",
      "\n",
      "Epoch 00817: loss did not improve from 0.39811\n",
      "Epoch 818/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4109 - accuracy: 0.8715\n",
      "\n",
      "Epoch 00818: loss did not improve from 0.39811\n",
      "Epoch 819/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4227 - accuracy: 0.8612\n",
      "\n",
      "Epoch 00819: loss did not improve from 0.39811\n",
      "Epoch 820/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4146 - accuracy: 0.8683\n",
      "\n",
      "Epoch 00820: loss did not improve from 0.39811\n",
      "Epoch 821/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4017 - accuracy: 0.8728\n",
      "\n",
      "Epoch 00821: loss did not improve from 0.39811\n",
      "Epoch 822/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4089 - accuracy: 0.8691\n",
      "\n",
      "Epoch 00822: loss did not improve from 0.39811\n",
      "Epoch 823/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3966 - accuracy: 0.8731\n",
      "\n",
      "Epoch 00823: loss did not improve from 0.39811\n",
      "Epoch 824/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3952 - accuracy: 0.8791\n",
      "\n",
      "Epoch 00824: loss did not improve from 0.39811\n",
      "Epoch 825/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4038 - accuracy: 0.8704\n",
      "\n",
      "Epoch 00825: loss did not improve from 0.39811\n",
      "Epoch 826/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.3863 - accuracy: 0.8788\n",
      "\n",
      "Epoch 00826: loss did not improve from 0.39811\n",
      "Epoch 827/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3999 - accuracy: 0.8684\n",
      "\n",
      "Epoch 00827: loss did not improve from 0.39811\n",
      "Epoch 828/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4053 - accuracy: 0.8718\n",
      "\n",
      "Epoch 00828: loss did not improve from 0.39811\n",
      "Epoch 829/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3950 - accuracy: 0.8720\n",
      "\n",
      "Epoch 00829: loss did not improve from 0.39811\n",
      "Epoch 830/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3837 - accuracy: 0.8770\n",
      "\n",
      "Epoch 00830: loss did not improve from 0.39811\n",
      "Epoch 831/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4085 - accuracy: 0.8752\n",
      "\n",
      "Epoch 00831: loss did not improve from 0.39811\n",
      "Epoch 832/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3949 - accuracy: 0.8706\n",
      "\n",
      "Epoch 00832: loss improved from 0.39811 to 0.39548, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 833/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3714 - accuracy: 0.8822\n",
      "\n",
      "Epoch 00833: loss improved from 0.39548 to 0.38828, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 834/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.3843 - accuracy: 0.8774\n",
      "\n",
      "Epoch 00834: loss did not improve from 0.38828\n",
      "Epoch 835/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3801 - accuracy: 0.8744\n",
      "\n",
      "Epoch 00835: loss improved from 0.38828 to 0.38790, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 836/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3921 - accuracy: 0.8733\n",
      "\n",
      "Epoch 00836: loss did not improve from 0.38790\n",
      "Epoch 837/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.4088 - accuracy: 0.8709\n",
      "\n",
      "Epoch 00837: loss did not improve from 0.38790\n",
      "Epoch 838/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4046 - accuracy: 0.8727\n",
      "\n",
      "Epoch 00838: loss did not improve from 0.38790\n",
      "Epoch 839/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.3847 - accuracy: 0.8750\n",
      "\n",
      "Epoch 00839: loss did not improve from 0.38790\n",
      "Epoch 840/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3872 - accuracy: 0.8726\n",
      "\n",
      "Epoch 00840: loss did not improve from 0.38790\n",
      "Epoch 841/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3869 - accuracy: 0.8766\n",
      "\n",
      "Epoch 00841: loss did not improve from 0.38790\n",
      "Epoch 842/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.4048 - accuracy: 0.8663\n",
      "\n",
      "Epoch 00842: loss improved from 0.38790 to 0.38784, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 843/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3749 - accuracy: 0.8733\n",
      "\n",
      "Epoch 00843: loss improved from 0.38784 to 0.38299, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 844/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3783 - accuracy: 0.8741\n",
      "\n",
      "Epoch 00844: loss did not improve from 0.38299\n",
      "Epoch 845/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3932 - accuracy: 0.8746\n",
      "\n",
      "Epoch 00845: loss did not improve from 0.38299\n",
      "Epoch 846/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3745 - accuracy: 0.8819\n",
      "\n",
      "Epoch 00846: loss did not improve from 0.38299\n",
      "Epoch 847/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4075 - accuracy: 0.8682\n",
      "\n",
      "Epoch 00847: loss did not improve from 0.38299\n",
      "Epoch 848/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3802 - accuracy: 0.8776\n",
      "\n",
      "Epoch 00848: loss did not improve from 0.38299\n",
      "Epoch 849/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3968 - accuracy: 0.8715\n",
      "\n",
      "Epoch 00849: loss did not improve from 0.38299\n",
      "Epoch 850/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3861 - accuracy: 0.8724\n",
      "\n",
      "Epoch 00850: loss improved from 0.38299 to 0.38186, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 851/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3900 - accuracy: 0.8769\n",
      "\n",
      "Epoch 00851: loss did not improve from 0.38186\n",
      "Epoch 852/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.3923 - accuracy: 0.8686\n",
      "\n",
      "Epoch 00852: loss did not improve from 0.38186\n",
      "Epoch 853/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3825 - accuracy: 0.8759\n",
      "\n",
      "Epoch 00853: loss did not improve from 0.38186\n",
      "Epoch 854/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3905 - accuracy: 0.8706\n",
      "\n",
      "Epoch 00854: loss did not improve from 0.38186\n",
      "Epoch 855/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3907 - accuracy: 0.8750\n",
      "\n",
      "Epoch 00855: loss did not improve from 0.38186\n",
      "Epoch 856/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3996 - accuracy: 0.8707\n",
      "\n",
      "Epoch 00856: loss did not improve from 0.38186\n",
      "Epoch 857/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3790 - accuracy: 0.8770\n",
      "\n",
      "Epoch 00857: loss did not improve from 0.38186\n",
      "Epoch 858/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3891 - accuracy: 0.8737\n",
      "\n",
      "Epoch 00858: loss did not improve from 0.38186\n",
      "Epoch 859/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3767 - accuracy: 0.8757\n",
      "\n",
      "Epoch 00859: loss improved from 0.38186 to 0.38023, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 860/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3680 - accuracy: 0.8845\n",
      "\n",
      "Epoch 00860: loss improved from 0.38023 to 0.37694, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 861/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.3589 - accuracy: 0.8836\n",
      "\n",
      "Epoch 00861: loss improved from 0.37694 to 0.37579, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 862/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3593 - accuracy: 0.8844\n",
      "\n",
      "Epoch 00862: loss improved from 0.37579 to 0.37154, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 863/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3868 - accuracy: 0.8755\n",
      "\n",
      "Epoch 00863: loss improved from 0.37154 to 0.36764, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 864/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3496 - accuracy: 0.8857\n",
      "\n",
      "Epoch 00864: loss improved from 0.36764 to 0.36087, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 865/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3525 - accuracy: 0.8893\n",
      "\n",
      "Epoch 00865: loss did not improve from 0.36087\n",
      "Epoch 866/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3656 - accuracy: 0.8810\n",
      "\n",
      "Epoch 00866: loss did not improve from 0.36087\n",
      "Epoch 867/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3836 - accuracy: 0.8763\n",
      "\n",
      "Epoch 00867: loss did not improve from 0.36087\n",
      "Epoch 868/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3804 - accuracy: 0.8732\n",
      "\n",
      "Epoch 00868: loss did not improve from 0.36087\n",
      "Epoch 869/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3787 - accuracy: 0.8784\n",
      "\n",
      "Epoch 00869: loss did not improve from 0.36087\n",
      "Epoch 870/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3917 - accuracy: 0.8705\n",
      "\n",
      "Epoch 00870: loss did not improve from 0.36087\n",
      "Epoch 871/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3866 - accuracy: 0.8732\n",
      "\n",
      "Epoch 00871: loss did not improve from 0.36087\n",
      "Epoch 872/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.3542 - accuracy: 0.8831\n",
      "\n",
      "Epoch 00872: loss did not improve from 0.36087\n",
      "Epoch 873/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3719 - accuracy: 0.8718\n",
      "\n",
      "Epoch 00873: loss did not improve from 0.36087\n",
      "Epoch 874/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3805 - accuracy: 0.8743\n",
      "\n",
      "Epoch 00874: loss did not improve from 0.36087\n",
      "Epoch 875/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3647 - accuracy: 0.8754\n",
      "\n",
      "Epoch 00875: loss did not improve from 0.36087\n",
      "Epoch 876/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3642 - accuracy: 0.8773\n",
      "\n",
      "Epoch 00876: loss did not improve from 0.36087\n",
      "Epoch 877/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3638 - accuracy: 0.8806\n",
      "\n",
      "Epoch 00877: loss did not improve from 0.36087\n",
      "Epoch 878/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3705 - accuracy: 0.8770\n",
      "\n",
      "Epoch 00878: loss improved from 0.36087 to 0.35950, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 879/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.3700 - accuracy: 0.8774\n",
      "\n",
      "Epoch 00879: loss did not improve from 0.35950\n",
      "Epoch 880/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3636 - accuracy: 0.8795\n",
      "\n",
      "Epoch 00880: loss did not improve from 0.35950\n",
      "Epoch 881/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3584 - accuracy: 0.8781\n",
      "\n",
      "Epoch 00881: loss did not improve from 0.35950\n",
      "Epoch 882/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3563 - accuracy: 0.8793\n",
      "\n",
      "Epoch 00882: loss improved from 0.35950 to 0.35614, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 883/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3260 - accuracy: 0.8933\n",
      "\n",
      "Epoch 00883: loss improved from 0.35614 to 0.35182, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 884/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3465 - accuracy: 0.8886\n",
      "\n",
      "Epoch 00884: loss did not improve from 0.35182\n",
      "Epoch 885/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3424 - accuracy: 0.8905\n",
      "\n",
      "Epoch 00885: loss did not improve from 0.35182\n",
      "Epoch 886/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3567 - accuracy: 0.8806\n",
      "\n",
      "Epoch 00886: loss did not improve from 0.35182\n",
      "Epoch 887/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3981 - accuracy: 0.8719\n",
      "\n",
      "Epoch 00887: loss did not improve from 0.35182\n",
      "Epoch 888/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.3730 - accuracy: 0.8743\n",
      "\n",
      "Epoch 00888: loss did not improve from 0.35182\n",
      "Epoch 889/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3682 - accuracy: 0.8769\n",
      "\n",
      "Epoch 00889: loss did not improve from 0.35182\n",
      "Epoch 890/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3919 - accuracy: 0.8676\n",
      "\n",
      "Epoch 00890: loss did not improve from 0.35182\n",
      "Epoch 891/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3605 - accuracy: 0.8818\n",
      "\n",
      "Epoch 00891: loss did not improve from 0.35182\n",
      "Epoch 892/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3576 - accuracy: 0.8783\n",
      "\n",
      "Epoch 00892: loss did not improve from 0.35182\n",
      "Epoch 893/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3534 - accuracy: 0.8845\n",
      "\n",
      "Epoch 00893: loss did not improve from 0.35182\n",
      "Epoch 894/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3527 - accuracy: 0.8847\n",
      "\n",
      "Epoch 00894: loss did not improve from 0.35182\n",
      "Epoch 895/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3688 - accuracy: 0.8801\n",
      "\n",
      "Epoch 00895: loss did not improve from 0.35182\n",
      "Epoch 896/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3523 - accuracy: 0.8820\n",
      "\n",
      "Epoch 00896: loss did not improve from 0.35182\n",
      "Epoch 897/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3649 - accuracy: 0.8811\n",
      "\n",
      "Epoch 00897: loss did not improve from 0.35182\n",
      "Epoch 898/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3634 - accuracy: 0.8821\n",
      "\n",
      "Epoch 00898: loss did not improve from 0.35182\n",
      "Epoch 899/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3496 - accuracy: 0.8837\n",
      "\n",
      "Epoch 00899: loss did not improve from 0.35182\n",
      "Epoch 900/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3573 - accuracy: 0.8879\n",
      "\n",
      "Epoch 00900: loss did not improve from 0.35182\n",
      "Epoch 901/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3730 - accuracy: 0.8758\n",
      "\n",
      "Epoch 00901: loss did not improve from 0.35182\n",
      "Epoch 902/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3552 - accuracy: 0.8835\n",
      "\n",
      "Epoch 00902: loss did not improve from 0.35182\n",
      "Epoch 903/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3559 - accuracy: 0.8836\n",
      "\n",
      "Epoch 00903: loss did not improve from 0.35182\n",
      "Epoch 904/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3537 - accuracy: 0.8838\n",
      "\n",
      "Epoch 00904: loss did not improve from 0.35182\n",
      "Epoch 905/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3517 - accuracy: 0.8851\n",
      "\n",
      "Epoch 00905: loss did not improve from 0.35182\n",
      "Epoch 906/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3570 - accuracy: 0.8749\n",
      "\n",
      "Epoch 00906: loss improved from 0.35182 to 0.34760, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 907/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3302 - accuracy: 0.8922\n",
      "\n",
      "Epoch 00907: loss improved from 0.34760 to 0.34199, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 908/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3579 - accuracy: 0.8819\n",
      "\n",
      "Epoch 00908: loss did not improve from 0.34199\n",
      "Epoch 909/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3265 - accuracy: 0.8917\n",
      "\n",
      "Epoch 00909: loss did not improve from 0.34199\n",
      "Epoch 910/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3356 - accuracy: 0.8915\n",
      "\n",
      "Epoch 00910: loss did not improve from 0.34199\n",
      "Epoch 911/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3513 - accuracy: 0.8840\n",
      "\n",
      "Epoch 00911: loss did not improve from 0.34199\n",
      "Epoch 912/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3355 - accuracy: 0.8872\n",
      "\n",
      "Epoch 00912: loss did not improve from 0.34199\n",
      "Epoch 913/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3458 - accuracy: 0.8850\n",
      "\n",
      "Epoch 00913: loss did not improve from 0.34199\n",
      "Epoch 914/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.3556 - accuracy: 0.8848\n",
      "\n",
      "Epoch 00914: loss did not improve from 0.34199\n",
      "Epoch 915/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3577 - accuracy: 0.8798\n",
      "\n",
      "Epoch 00915: loss did not improve from 0.34199\n",
      "Epoch 916/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3379 - accuracy: 0.8893\n",
      "\n",
      "Epoch 00916: loss did not improve from 0.34199\n",
      "Epoch 917/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3441 - accuracy: 0.8857\n",
      "\n",
      "Epoch 00917: loss did not improve from 0.34199\n",
      "Epoch 918/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3366 - accuracy: 0.8884\n",
      "\n",
      "Epoch 00918: loss improved from 0.34199 to 0.34014, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 919/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3320 - accuracy: 0.8945\n",
      "\n",
      "Epoch 00919: loss did not improve from 0.34014\n",
      "Epoch 920/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3345 - accuracy: 0.8879\n",
      "\n",
      "Epoch 00920: loss did not improve from 0.34014\n",
      "Epoch 921/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3616 - accuracy: 0.8827\n",
      "\n",
      "Epoch 00921: loss did not improve from 0.34014\n",
      "Epoch 922/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3815 - accuracy: 0.8734\n",
      "\n",
      "Epoch 00922: loss did not improve from 0.34014\n",
      "Epoch 923/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3560 - accuracy: 0.8778\n",
      "\n",
      "Epoch 00923: loss did not improve from 0.34014\n",
      "Epoch 924/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3853 - accuracy: 0.8741\n",
      "\n",
      "Epoch 00924: loss did not improve from 0.34014\n",
      "Epoch 925/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3969 - accuracy: 0.8645\n",
      "\n",
      "Epoch 00925: loss did not improve from 0.34014\n",
      "Epoch 926/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3485 - accuracy: 0.8847\n",
      "\n",
      "Epoch 00926: loss did not improve from 0.34014\n",
      "Epoch 927/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3527 - accuracy: 0.8844\n",
      "\n",
      "Epoch 00927: loss did not improve from 0.34014\n",
      "Epoch 928/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3458 - accuracy: 0.8864\n",
      "\n",
      "Epoch 00928: loss did not improve from 0.34014\n",
      "Epoch 929/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3447 - accuracy: 0.8845\n",
      "\n",
      "Epoch 00929: loss did not improve from 0.34014\n",
      "Epoch 930/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3427 - accuracy: 0.8807\n",
      "\n",
      "Epoch 00930: loss did not improve from 0.34014\n",
      "Epoch 931/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3480 - accuracy: 0.8865\n",
      "\n",
      "Epoch 00931: loss did not improve from 0.34014\n",
      "Epoch 932/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3274 - accuracy: 0.8876\n",
      "\n",
      "Epoch 00932: loss improved from 0.34014 to 0.33723, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 933/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3291 - accuracy: 0.8858\n",
      "\n",
      "Epoch 00933: loss improved from 0.33723 to 0.33497, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 934/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3459 - accuracy: 0.8861\n",
      "\n",
      "Epoch 00934: loss did not improve from 0.33497\n",
      "Epoch 935/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3391 - accuracy: 0.8867\n",
      "\n",
      "Epoch 00935: loss did not improve from 0.33497\n",
      "Epoch 936/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3355 - accuracy: 0.8910\n",
      "\n",
      "Epoch 00936: loss did not improve from 0.33497\n",
      "Epoch 937/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3278 - accuracy: 0.8861\n",
      "\n",
      "Epoch 00937: loss did not improve from 0.33497\n",
      "Epoch 938/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3267 - accuracy: 0.8924\n",
      "\n",
      "Epoch 00938: loss did not improve from 0.33497\n",
      "Epoch 939/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3276 - accuracy: 0.8900\n",
      "\n",
      "Epoch 00939: loss did not improve from 0.33497\n",
      "Epoch 940/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3506 - accuracy: 0.8857\n",
      "\n",
      "Epoch 00940: loss did not improve from 0.33497\n",
      "Epoch 941/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3379 - accuracy: 0.8829\n",
      "\n",
      "Epoch 00941: loss did not improve from 0.33497\n",
      "Epoch 942/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3349 - accuracy: 0.8865\n",
      "\n",
      "Epoch 00942: loss did not improve from 0.33497\n",
      "Epoch 943/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3314 - accuracy: 0.8898\n",
      "\n",
      "Epoch 00943: loss did not improve from 0.33497\n",
      "Epoch 944/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.3544 - accuracy: 0.8787\n",
      "\n",
      "Epoch 00944: loss did not improve from 0.33497\n",
      "Epoch 945/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3286 - accuracy: 0.8903\n",
      "\n",
      "Epoch 00945: loss did not improve from 0.33497\n",
      "Epoch 946/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3173 - accuracy: 0.8897\n",
      "\n",
      "Epoch 00946: loss improved from 0.33497 to 0.32894, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 947/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3112 - accuracy: 0.8960\n",
      "\n",
      "Epoch 00947: loss improved from 0.32894 to 0.32508, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 948/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3203 - accuracy: 0.8937\n",
      "\n",
      "Epoch 00948: loss did not improve from 0.32508\n",
      "Epoch 949/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3473 - accuracy: 0.8792\n",
      "\n",
      "Epoch 00949: loss did not improve from 0.32508\n",
      "Epoch 950/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3503 - accuracy: 0.8826\n",
      "\n",
      "Epoch 00950: loss did not improve from 0.32508\n",
      "Epoch 951/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3563 - accuracy: 0.8817\n",
      "\n",
      "Epoch 00951: loss did not improve from 0.32508\n",
      "Epoch 952/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3537 - accuracy: 0.8830\n",
      "\n",
      "Epoch 00952: loss did not improve from 0.32508\n",
      "Epoch 953/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.3574 - accuracy: 0.8828\n",
      "\n",
      "Epoch 00953: loss did not improve from 0.32508\n",
      "Epoch 954/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.3442 - accuracy: 0.8830\n",
      "\n",
      "Epoch 00954: loss did not improve from 0.32508\n",
      "Epoch 955/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3687 - accuracy: 0.8782\n",
      "\n",
      "Epoch 00955: loss did not improve from 0.32508\n",
      "Epoch 956/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3356 - accuracy: 0.8895\n",
      "\n",
      "Epoch 00956: loss did not improve from 0.32508\n",
      "Epoch 957/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3496 - accuracy: 0.8823\n",
      "\n",
      "Epoch 00957: loss did not improve from 0.32508\n",
      "Epoch 958/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3312 - accuracy: 0.8921\n",
      "\n",
      "Epoch 00958: loss did not improve from 0.32508\n",
      "Epoch 959/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3149 - accuracy: 0.8970\n",
      "\n",
      "Epoch 00959: loss did not improve from 0.32508\n",
      "Epoch 960/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3326 - accuracy: 0.8882\n",
      "\n",
      "Epoch 00960: loss did not improve from 0.32508\n",
      "Epoch 961/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3380 - accuracy: 0.8862\n",
      "\n",
      "Epoch 00961: loss did not improve from 0.32508\n",
      "Epoch 962/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3285 - accuracy: 0.8842\n",
      "\n",
      "Epoch 00962: loss improved from 0.32508 to 0.32404, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 963/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3314 - accuracy: 0.8861\n",
      "\n",
      "Epoch 00963: loss did not improve from 0.32404\n",
      "Epoch 964/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3342 - accuracy: 0.8842\n",
      "\n",
      "Epoch 00964: loss did not improve from 0.32404\n",
      "Epoch 965/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3278 - accuracy: 0.8866\n",
      "\n",
      "Epoch 00965: loss did not improve from 0.32404\n",
      "Epoch 966/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3444 - accuracy: 0.8869\n",
      "\n",
      "Epoch 00966: loss did not improve from 0.32404\n",
      "Epoch 967/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3405 - accuracy: 0.8864\n",
      "\n",
      "Epoch 00967: loss did not improve from 0.32404\n",
      "Epoch 968/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3605 - accuracy: 0.8769\n",
      "\n",
      "Epoch 00968: loss did not improve from 0.32404\n",
      "Epoch 969/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3776 - accuracy: 0.8719\n",
      "\n",
      "Epoch 00969: loss did not improve from 0.32404\n",
      "Epoch 970/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3373 - accuracy: 0.8808\n",
      "\n",
      "Epoch 00970: loss did not improve from 0.32404\n",
      "Epoch 971/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3363 - accuracy: 0.8851\n",
      "\n",
      "Epoch 00971: loss did not improve from 0.32404\n",
      "Epoch 972/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3260 - accuracy: 0.8864\n",
      "\n",
      "Epoch 00972: loss did not improve from 0.32404\n",
      "Epoch 973/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3063 - accuracy: 0.8977\n",
      "\n",
      "Epoch 00973: loss improved from 0.32404 to 0.31632, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 974/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3058 - accuracy: 0.8964\n",
      "\n",
      "Epoch 00974: loss improved from 0.31632 to 0.31422, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 975/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3052 - accuracy: 0.8950\n",
      "\n",
      "Epoch 00975: loss did not improve from 0.31422\n",
      "Epoch 976/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3228 - accuracy: 0.8919\n",
      "\n",
      "Epoch 00976: loss did not improve from 0.31422\n",
      "Epoch 977/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3097 - accuracy: 0.8994\n",
      "\n",
      "Epoch 00977: loss did not improve from 0.31422\n",
      "Epoch 978/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3172 - accuracy: 0.8955\n",
      "\n",
      "Epoch 00978: loss did not improve from 0.31422\n",
      "Epoch 979/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3625 - accuracy: 0.8819\n",
      "\n",
      "Epoch 00979: loss did not improve from 0.31422\n",
      "Epoch 980/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3655 - accuracy: 0.8815\n",
      "\n",
      "Epoch 00980: loss did not improve from 0.31422\n",
      "Epoch 981/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3746 - accuracy: 0.8766\n",
      "\n",
      "Epoch 00981: loss did not improve from 0.31422\n",
      "Epoch 982/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3534 - accuracy: 0.8817\n",
      "\n",
      "Epoch 00982: loss did not improve from 0.31422\n",
      "Epoch 983/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3236 - accuracy: 0.8968\n",
      "\n",
      "Epoch 00983: loss improved from 0.31422 to 0.31335, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 984/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3091 - accuracy: 0.8966\n",
      "\n",
      "Epoch 00984: loss did not improve from 0.31335\n",
      "Epoch 985/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3001 - accuracy: 0.8995\n",
      "\n",
      "Epoch 00985: loss improved from 0.31335 to 0.31324, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 986/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2922 - accuracy: 0.9023\n",
      "\n",
      "Epoch 00986: loss did not improve from 0.31324\n",
      "Epoch 987/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3037 - accuracy: 0.9000\n",
      "\n",
      "Epoch 00987: loss improved from 0.31324 to 0.30829, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 988/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3118 - accuracy: 0.8948\n",
      "\n",
      "Epoch 00988: loss improved from 0.30829 to 0.30691, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 989/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2988 - accuracy: 0.8978\n",
      "\n",
      "Epoch 00989: loss improved from 0.30691 to 0.30542, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 990/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3188 - accuracy: 0.8926\n",
      "\n",
      "Epoch 00990: loss did not improve from 0.30542\n",
      "Epoch 991/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2895 - accuracy: 0.8948\n",
      "\n",
      "Epoch 00991: loss improved from 0.30542 to 0.30435, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 992/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3059 - accuracy: 0.8961\n",
      "\n",
      "Epoch 00992: loss did not improve from 0.30435\n",
      "Epoch 993/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3159 - accuracy: 0.8939\n",
      "\n",
      "Epoch 00993: loss did not improve from 0.30435\n",
      "Epoch 994/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3329 - accuracy: 0.8880\n",
      "\n",
      "Epoch 00994: loss did not improve from 0.30435\n",
      "Epoch 995/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3175 - accuracy: 0.8935\n",
      "\n",
      "Epoch 00995: loss did not improve from 0.30435\n",
      "Epoch 996/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3173 - accuracy: 0.8919\n",
      "\n",
      "Epoch 00996: loss did not improve from 0.30435\n",
      "Epoch 997/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2896 - accuracy: 0.9030\n",
      "\n",
      "Epoch 00997: loss did not improve from 0.30435\n",
      "Epoch 998/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3112 - accuracy: 0.8918\n",
      "\n",
      "Epoch 00998: loss did not improve from 0.30435\n",
      "Epoch 999/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3149 - accuracy: 0.8924\n",
      "\n",
      "Epoch 00999: loss did not improve from 0.30435\n",
      "Epoch 1000/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3028 - accuracy: 0.9006\n",
      "\n",
      "Epoch 01000: loss did not improve from 0.30435\n",
      "Epoch 1001/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.3003 - accuracy: 0.8983\n",
      "\n",
      "Epoch 01001: loss did not improve from 0.30435\n",
      "Epoch 1002/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3022 - accuracy: 0.9018\n",
      "\n",
      "Epoch 01002: loss did not improve from 0.30435\n",
      "Epoch 1003/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3169 - accuracy: 0.8910\n",
      "\n",
      "Epoch 01003: loss did not improve from 0.30435\n",
      "Epoch 1004/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3220 - accuracy: 0.8903\n",
      "\n",
      "Epoch 01004: loss did not improve from 0.30435\n",
      "Epoch 1005/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2967 - accuracy: 0.8971\n",
      "\n",
      "Epoch 01005: loss did not improve from 0.30435\n",
      "Epoch 1006/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.3090 - accuracy: 0.8902\n",
      "\n",
      "Epoch 01006: loss did not improve from 0.30435\n",
      "Epoch 1007/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3479 - accuracy: 0.8790\n",
      "\n",
      "Epoch 01007: loss did not improve from 0.30435\n",
      "Epoch 1008/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.3112 - accuracy: 0.8929\n",
      "\n",
      "Epoch 01008: loss did not improve from 0.30435\n",
      "Epoch 1009/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3260 - accuracy: 0.8915\n",
      "\n",
      "Epoch 01009: loss did not improve from 0.30435\n",
      "Epoch 1010/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3130 - accuracy: 0.8927\n",
      "\n",
      "Epoch 01010: loss did not improve from 0.30435\n",
      "Epoch 1011/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3059 - accuracy: 0.9024\n",
      "\n",
      "Epoch 01011: loss did not improve from 0.30435\n",
      "Epoch 1012/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3231 - accuracy: 0.8948\n",
      "\n",
      "Epoch 01012: loss did not improve from 0.30435\n",
      "Epoch 1013/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3096 - accuracy: 0.8929\n",
      "\n",
      "Epoch 01013: loss did not improve from 0.30435\n",
      "Epoch 1014/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2975 - accuracy: 0.9028\n",
      "\n",
      "Epoch 01014: loss did not improve from 0.30435\n",
      "Epoch 1015/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3000 - accuracy: 0.8985\n",
      "\n",
      "Epoch 01015: loss did not improve from 0.30435\n",
      "Epoch 1016/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2989 - accuracy: 0.8988\n",
      "\n",
      "Epoch 01016: loss improved from 0.30435 to 0.30177, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1017/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2970 - accuracy: 0.9022\n",
      "\n",
      "Epoch 01017: loss improved from 0.30177 to 0.30174, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1018/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3180 - accuracy: 0.8895\n",
      "\n",
      "Epoch 01018: loss did not improve from 0.30174\n",
      "Epoch 1019/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3031 - accuracy: 0.8989\n",
      "\n",
      "Epoch 01019: loss did not improve from 0.30174\n",
      "Epoch 1020/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2967 - accuracy: 0.8948\n",
      "\n",
      "Epoch 01020: loss improved from 0.30174 to 0.29700, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1021/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2829 - accuracy: 0.9022\n",
      "\n",
      "Epoch 01021: loss did not improve from 0.29700\n",
      "Epoch 1022/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2904 - accuracy: 0.9021\n",
      "\n",
      "Epoch 01022: loss did not improve from 0.29700\n",
      "Epoch 1023/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3085 - accuracy: 0.8960\n",
      "\n",
      "Epoch 01023: loss did not improve from 0.29700\n",
      "Epoch 1024/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3698 - accuracy: 0.8784\n",
      "\n",
      "Epoch 01024: loss did not improve from 0.29700\n",
      "Epoch 1025/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3650 - accuracy: 0.8741\n",
      "\n",
      "Epoch 01025: loss did not improve from 0.29700\n",
      "Epoch 1026/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3307 - accuracy: 0.8877\n",
      "\n",
      "Epoch 01026: loss did not improve from 0.29700\n",
      "Epoch 1027/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3411 - accuracy: 0.8859\n",
      "\n",
      "Epoch 01027: loss did not improve from 0.29700\n",
      "Epoch 1028/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3425 - accuracy: 0.8818\n",
      "\n",
      "Epoch 01028: loss did not improve from 0.29700\n",
      "Epoch 1029/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3715 - accuracy: 0.8721\n",
      "\n",
      "Epoch 01029: loss did not improve from 0.29700\n",
      "Epoch 1030/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3378 - accuracy: 0.8797\n",
      "\n",
      "Epoch 01030: loss did not improve from 0.29700\n",
      "Epoch 1031/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3123 - accuracy: 0.8925\n",
      "\n",
      "Epoch 01031: loss did not improve from 0.29700\n",
      "Epoch 1032/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2944 - accuracy: 0.8986\n",
      "\n",
      "Epoch 01032: loss did not improve from 0.29700\n",
      "Epoch 1033/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2861 - accuracy: 0.9046\n",
      "\n",
      "Epoch 01033: loss did not improve from 0.29700\n",
      "Epoch 1034/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2990 - accuracy: 0.8970\n",
      "\n",
      "Epoch 01034: loss improved from 0.29700 to 0.29572, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1035/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2843 - accuracy: 0.8987\n",
      "\n",
      "Epoch 01035: loss did not improve from 0.29572\n",
      "Epoch 1036/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2894 - accuracy: 0.9032\n",
      "\n",
      "Epoch 01036: loss did not improve from 0.29572\n",
      "Epoch 1037/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2964 - accuracy: 0.8993\n",
      "\n",
      "Epoch 01037: loss did not improve from 0.29572\n",
      "Epoch 1038/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3066 - accuracy: 0.9018\n",
      "\n",
      "Epoch 01038: loss did not improve from 0.29572\n",
      "Epoch 1039/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2906 - accuracy: 0.8982\n",
      "\n",
      "Epoch 01039: loss improved from 0.29572 to 0.29560, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1040/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2898 - accuracy: 0.8973\n",
      "\n",
      "Epoch 01040: loss improved from 0.29560 to 0.28990, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1041/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2849 - accuracy: 0.9049\n",
      "\n",
      "Epoch 01041: loss improved from 0.28990 to 0.28882, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1042/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2861 - accuracy: 0.8999\n",
      "\n",
      "Epoch 01042: loss improved from 0.28882 to 0.28836, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1043/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2752 - accuracy: 0.9079\n",
      "\n",
      "Epoch 01043: loss improved from 0.28836 to 0.28430, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1044/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2756 - accuracy: 0.9058\n",
      "\n",
      "Epoch 01044: loss improved from 0.28430 to 0.28329, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1045/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2920 - accuracy: 0.8999\n",
      "\n",
      "Epoch 01045: loss did not improve from 0.28329\n",
      "Epoch 1046/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2740 - accuracy: 0.9076\n",
      "\n",
      "Epoch 01046: loss did not improve from 0.28329\n",
      "Epoch 1047/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2705 - accuracy: 0.9063\n",
      "\n",
      "Epoch 01047: loss did not improve from 0.28329\n",
      "Epoch 1048/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2895 - accuracy: 0.9026\n",
      "\n",
      "Epoch 01048: loss did not improve from 0.28329\n",
      "Epoch 1049/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2794 - accuracy: 0.9034\n",
      "\n",
      "Epoch 01049: loss did not improve from 0.28329\n",
      "Epoch 1050/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2906 - accuracy: 0.9001\n",
      "\n",
      "Epoch 01050: loss did not improve from 0.28329\n",
      "Epoch 1051/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2833 - accuracy: 0.8999\n",
      "\n",
      "Epoch 01051: loss did not improve from 0.28329\n",
      "Epoch 1052/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2823 - accuracy: 0.9037\n",
      "\n",
      "Epoch 01052: loss did not improve from 0.28329\n",
      "Epoch 1053/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3027 - accuracy: 0.8964\n",
      "\n",
      "Epoch 01053: loss did not improve from 0.28329\n",
      "Epoch 1054/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2933 - accuracy: 0.9024\n",
      "\n",
      "Epoch 01054: loss did not improve from 0.28329\n",
      "Epoch 1055/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2764 - accuracy: 0.9066\n",
      "\n",
      "Epoch 01055: loss did not improve from 0.28329\n",
      "Epoch 1056/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2807 - accuracy: 0.9060\n",
      "\n",
      "Epoch 01056: loss did not improve from 0.28329\n",
      "Epoch 1057/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.2902 - accuracy: 0.8962\n",
      "\n",
      "Epoch 01057: loss did not improve from 0.28329\n",
      "Epoch 1058/2000\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.2911 - accuracy: 0.8993\n",
      "\n",
      "Epoch 01058: loss did not improve from 0.28329\n",
      "Epoch 1059/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2791 - accuracy: 0.9024\n",
      "\n",
      "Epoch 01059: loss did not improve from 0.28329\n",
      "Epoch 1060/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2793 - accuracy: 0.9035\n",
      "\n",
      "Epoch 01060: loss did not improve from 0.28329\n",
      "Epoch 1061/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2719 - accuracy: 0.9088\n",
      "\n",
      "Epoch 01061: loss did not improve from 0.28329\n",
      "Epoch 1062/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2800 - accuracy: 0.9002\n",
      "\n",
      "Epoch 01062: loss did not improve from 0.28329\n",
      "Epoch 1063/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2802 - accuracy: 0.9029\n",
      "\n",
      "Epoch 01063: loss did not improve from 0.28329\n",
      "Epoch 1064/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2858 - accuracy: 0.9022\n",
      "\n",
      "Epoch 01064: loss did not improve from 0.28329\n",
      "Epoch 1065/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3139 - accuracy: 0.8941\n",
      "\n",
      "Epoch 01065: loss did not improve from 0.28329\n",
      "Epoch 1066/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3594 - accuracy: 0.8784\n",
      "\n",
      "Epoch 01066: loss did not improve from 0.28329\n",
      "Epoch 1067/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3502 - accuracy: 0.8807\n",
      "\n",
      "Epoch 01067: loss did not improve from 0.28329\n",
      "Epoch 1068/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.4282 - accuracy: 0.8563\n",
      "\n",
      "Epoch 01068: loss did not improve from 0.28329\n",
      "Epoch 1069/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3652 - accuracy: 0.8702\n",
      "\n",
      "Epoch 01069: loss did not improve from 0.28329\n",
      "Epoch 1070/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3542 - accuracy: 0.8801\n",
      "\n",
      "Epoch 01070: loss did not improve from 0.28329\n",
      "Epoch 1071/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3222 - accuracy: 0.8948\n",
      "\n",
      "Epoch 01071: loss did not improve from 0.28329\n",
      "Epoch 1072/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3218 - accuracy: 0.8849\n",
      "\n",
      "Epoch 01072: loss did not improve from 0.28329\n",
      "Epoch 1073/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2897 - accuracy: 0.8953\n",
      "\n",
      "Epoch 01073: loss did not improve from 0.28329\n",
      "Epoch 1074/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2746 - accuracy: 0.9057\n",
      "\n",
      "Epoch 01074: loss did not improve from 0.28329\n",
      "Epoch 1075/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2656 - accuracy: 0.9091\n",
      "\n",
      "Epoch 01075: loss improved from 0.28329 to 0.27860, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1076/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2858 - accuracy: 0.8987\n",
      "\n",
      "Epoch 01076: loss improved from 0.27860 to 0.27837, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1077/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2672 - accuracy: 0.9115\n",
      "\n",
      "Epoch 01077: loss improved from 0.27837 to 0.27793, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1078/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.2814 - accuracy: 0.8987\n",
      "\n",
      "Epoch 01078: loss did not improve from 0.27793\n",
      "Epoch 1079/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.2831 - accuracy: 0.8998\n",
      "\n",
      "Epoch 01079: loss did not improve from 0.27793\n",
      "Epoch 1080/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.2816 - accuracy: 0.9036\n",
      "\n",
      "Epoch 01080: loss did not improve from 0.27793\n",
      "Epoch 1081/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.2768 - accuracy: 0.9030\n",
      "\n",
      "Epoch 01081: loss improved from 0.27793 to 0.27471, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1082/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2566 - accuracy: 0.9153\n",
      "\n",
      "Epoch 01082: loss improved from 0.27471 to 0.27409, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1083/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2612 - accuracy: 0.9048\n",
      "\n",
      "Epoch 01083: loss did not improve from 0.27409\n",
      "Epoch 1084/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2697 - accuracy: 0.9034\n",
      "\n",
      "Epoch 01084: loss did not improve from 0.27409\n",
      "Epoch 1085/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2742 - accuracy: 0.9066\n",
      "\n",
      "Epoch 01085: loss improved from 0.27409 to 0.27299, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1086/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2659 - accuracy: 0.9081\n",
      "\n",
      "Epoch 01086: loss improved from 0.27299 to 0.27055, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1087/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2654 - accuracy: 0.9077\n",
      "\n",
      "Epoch 01087: loss improved from 0.27055 to 0.26834, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1088/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2571 - accuracy: 0.9101\n",
      "\n",
      "Epoch 01088: loss did not improve from 0.26834\n",
      "Epoch 1089/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2760 - accuracy: 0.9025\n",
      "\n",
      "Epoch 01089: loss did not improve from 0.26834\n",
      "Epoch 1090/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2635 - accuracy: 0.9122\n",
      "\n",
      "Epoch 01090: loss did not improve from 0.26834\n",
      "Epoch 1091/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2756 - accuracy: 0.9031\n",
      "\n",
      "Epoch 01091: loss did not improve from 0.26834\n",
      "Epoch 1092/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2713 - accuracy: 0.9010\n",
      "\n",
      "Epoch 01092: loss did not improve from 0.26834\n",
      "Epoch 1093/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2823 - accuracy: 0.9050\n",
      "\n",
      "Epoch 01093: loss did not improve from 0.26834\n",
      "Epoch 1094/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2857 - accuracy: 0.8999\n",
      "\n",
      "Epoch 01094: loss did not improve from 0.26834\n",
      "Epoch 1095/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2852 - accuracy: 0.9032\n",
      "\n",
      "Epoch 01095: loss did not improve from 0.26834\n",
      "Epoch 1096/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2956 - accuracy: 0.8937\n",
      "\n",
      "Epoch 01096: loss did not improve from 0.26834\n",
      "Epoch 1097/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3035 - accuracy: 0.8953\n",
      "\n",
      "Epoch 01097: loss did not improve from 0.26834\n",
      "Epoch 1098/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2887 - accuracy: 0.9019\n",
      "\n",
      "Epoch 01098: loss did not improve from 0.26834\n",
      "Epoch 1099/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2693 - accuracy: 0.9095\n",
      "\n",
      "Epoch 01099: loss did not improve from 0.26834\n",
      "Epoch 1100/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2932 - accuracy: 0.9032\n",
      "\n",
      "Epoch 01100: loss did not improve from 0.26834\n",
      "Epoch 1101/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2930 - accuracy: 0.9018\n",
      "\n",
      "Epoch 01101: loss did not improve from 0.26834\n",
      "Epoch 1102/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2694 - accuracy: 0.9087\n",
      "\n",
      "Epoch 01102: loss did not improve from 0.26834\n",
      "Epoch 1103/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2795 - accuracy: 0.9061\n",
      "\n",
      "Epoch 01103: loss did not improve from 0.26834\n",
      "Epoch 1104/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2673 - accuracy: 0.9057\n",
      "\n",
      "Epoch 01104: loss did not improve from 0.26834\n",
      "Epoch 1105/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2678 - accuracy: 0.9091\n",
      "\n",
      "Epoch 01105: loss did not improve from 0.26834\n",
      "Epoch 1106/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2449 - accuracy: 0.9155\n",
      "\n",
      "Epoch 01106: loss did not improve from 0.26834\n",
      "Epoch 1107/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2810 - accuracy: 0.9053\n",
      "\n",
      "Epoch 01107: loss improved from 0.26834 to 0.26826, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1108/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2616 - accuracy: 0.9138\n",
      "\n",
      "Epoch 01108: loss improved from 0.26826 to 0.26608, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1109/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2690 - accuracy: 0.9073\n",
      "\n",
      "Epoch 01109: loss did not improve from 0.26608\n",
      "Epoch 1110/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2651 - accuracy: 0.9026\n",
      "\n",
      "Epoch 01110: loss did not improve from 0.26608\n",
      "Epoch 1111/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2612 - accuracy: 0.9079\n",
      "\n",
      "Epoch 01111: loss did not improve from 0.26608\n",
      "Epoch 1112/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2725 - accuracy: 0.9044\n",
      "\n",
      "Epoch 01112: loss did not improve from 0.26608\n",
      "Epoch 1113/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2737 - accuracy: 0.9023\n",
      "\n",
      "Epoch 01113: loss did not improve from 0.26608\n",
      "Epoch 1114/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2806 - accuracy: 0.9012\n",
      "\n",
      "Epoch 01114: loss did not improve from 0.26608\n",
      "Epoch 1115/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2636 - accuracy: 0.9091\n",
      "\n",
      "Epoch 01115: loss did not improve from 0.26608\n",
      "Epoch 1116/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2636 - accuracy: 0.9111\n",
      "\n",
      "Epoch 01116: loss did not improve from 0.26608\n",
      "Epoch 1117/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2554 - accuracy: 0.9133\n",
      "\n",
      "Epoch 01117: loss did not improve from 0.26608\n",
      "Epoch 1118/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2670 - accuracy: 0.9055\n",
      "\n",
      "Epoch 01118: loss improved from 0.26608 to 0.26390, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1119/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2631 - accuracy: 0.9083\n",
      "\n",
      "Epoch 01119: loss did not improve from 0.26390\n",
      "Epoch 1120/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2701 - accuracy: 0.9035\n",
      "\n",
      "Epoch 01120: loss did not improve from 0.26390\n",
      "Epoch 1121/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2766 - accuracy: 0.9044\n",
      "\n",
      "Epoch 01121: loss did not improve from 0.26390\n",
      "Epoch 1122/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2644 - accuracy: 0.9054\n",
      "\n",
      "Epoch 01122: loss did not improve from 0.26390\n",
      "Epoch 1123/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2715 - accuracy: 0.9042\n",
      "\n",
      "Epoch 01123: loss did not improve from 0.26390\n",
      "Epoch 1124/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2675 - accuracy: 0.9092\n",
      "\n",
      "Epoch 01124: loss did not improve from 0.26390\n",
      "Epoch 1125/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2673 - accuracy: 0.9057\n",
      "\n",
      "Epoch 01125: loss did not improve from 0.26390\n",
      "Epoch 1126/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2869 - accuracy: 0.8979\n",
      "\n",
      "Epoch 01126: loss did not improve from 0.26390\n",
      "Epoch 1127/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2958 - accuracy: 0.8964\n",
      "\n",
      "Epoch 01127: loss did not improve from 0.26390\n",
      "Epoch 1128/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3312 - accuracy: 0.8871\n",
      "\n",
      "Epoch 01128: loss did not improve from 0.26390\n",
      "Epoch 1129/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3286 - accuracy: 0.8819\n",
      "\n",
      "Epoch 01129: loss did not improve from 0.26390\n",
      "Epoch 1130/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3047 - accuracy: 0.8930\n",
      "\n",
      "Epoch 01130: loss did not improve from 0.26390\n",
      "Epoch 1131/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2942 - accuracy: 0.8995\n",
      "\n",
      "Epoch 01131: loss did not improve from 0.26390\n",
      "Epoch 1132/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2710 - accuracy: 0.9107\n",
      "\n",
      "Epoch 01132: loss did not improve from 0.26390\n",
      "Epoch 1133/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2555 - accuracy: 0.9133\n",
      "\n",
      "Epoch 01133: loss did not improve from 0.26390\n",
      "Epoch 1134/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2552 - accuracy: 0.9090\n",
      "\n",
      "Epoch 01134: loss did not improve from 0.26390\n",
      "Epoch 1135/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2723 - accuracy: 0.9041\n",
      "\n",
      "Epoch 01135: loss did not improve from 0.26390\n",
      "Epoch 1136/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2685 - accuracy: 0.9058\n",
      "\n",
      "Epoch 01136: loss did not improve from 0.26390\n",
      "Epoch 1137/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2439 - accuracy: 0.9147\n",
      "\n",
      "Epoch 01137: loss improved from 0.26390 to 0.26087, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1138/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2663 - accuracy: 0.9048\n",
      "\n",
      "Epoch 01138: loss did not improve from 0.26087\n",
      "Epoch 1139/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2636 - accuracy: 0.9096\n",
      "\n",
      "Epoch 01139: loss did not improve from 0.26087\n",
      "Epoch 1140/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2580 - accuracy: 0.9089\n",
      "\n",
      "Epoch 01140: loss did not improve from 0.26087\n",
      "Epoch 1141/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2696 - accuracy: 0.9032\n",
      "\n",
      "Epoch 01141: loss improved from 0.26087 to 0.26032, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1142/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2424 - accuracy: 0.9143\n",
      "\n",
      "Epoch 01142: loss improved from 0.26032 to 0.25968, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1143/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2577 - accuracy: 0.9080\n",
      "\n",
      "Epoch 01143: loss did not improve from 0.25968\n",
      "Epoch 1144/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2605 - accuracy: 0.9060\n",
      "\n",
      "Epoch 01144: loss improved from 0.25968 to 0.25821, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1145/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2429 - accuracy: 0.9172\n",
      "\n",
      "Epoch 01145: loss improved from 0.25821 to 0.25702, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1146/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2654 - accuracy: 0.9065\n",
      "\n",
      "Epoch 01146: loss did not improve from 0.25702\n",
      "Epoch 1147/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2518 - accuracy: 0.9098\n",
      "\n",
      "Epoch 01147: loss improved from 0.25702 to 0.25626, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1148/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2567 - accuracy: 0.9113\n",
      "\n",
      "Epoch 01148: loss did not improve from 0.25626\n",
      "Epoch 1149/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2483 - accuracy: 0.9134\n",
      "\n",
      "Epoch 01149: loss improved from 0.25626 to 0.25477, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1150/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2557 - accuracy: 0.9057\n",
      "\n",
      "Epoch 01150: loss improved from 0.25477 to 0.25142, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1151/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2519 - accuracy: 0.9083\n",
      "\n",
      "Epoch 01151: loss did not improve from 0.25142\n",
      "Epoch 1152/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2634 - accuracy: 0.9088\n",
      "\n",
      "Epoch 01152: loss did not improve from 0.25142\n",
      "Epoch 1153/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2666 - accuracy: 0.9014\n",
      "\n",
      "Epoch 01153: loss did not improve from 0.25142\n",
      "Epoch 1154/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2649 - accuracy: 0.9098\n",
      "\n",
      "Epoch 01154: loss did not improve from 0.25142\n",
      "Epoch 1155/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2820 - accuracy: 0.8998\n",
      "\n",
      "Epoch 01155: loss did not improve from 0.25142\n",
      "Epoch 1156/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2773 - accuracy: 0.9080\n",
      "\n",
      "Epoch 01156: loss did not improve from 0.25142\n",
      "Epoch 1157/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2725 - accuracy: 0.9024\n",
      "\n",
      "Epoch 01157: loss did not improve from 0.25142\n",
      "Epoch 1158/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3147 - accuracy: 0.8844\n",
      "\n",
      "Epoch 01158: loss did not improve from 0.25142\n",
      "Epoch 1159/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3139 - accuracy: 0.8888\n",
      "\n",
      "Epoch 01159: loss did not improve from 0.25142\n",
      "Epoch 1160/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3862 - accuracy: 0.8734\n",
      "\n",
      "Epoch 01160: loss did not improve from 0.25142\n",
      "Epoch 1161/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3431 - accuracy: 0.8790\n",
      "\n",
      "Epoch 01161: loss did not improve from 0.25142\n",
      "Epoch 1162/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3213 - accuracy: 0.8873\n",
      "\n",
      "Epoch 01162: loss did not improve from 0.25142\n",
      "Epoch 1163/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2954 - accuracy: 0.8925\n",
      "\n",
      "Epoch 01163: loss did not improve from 0.25142\n",
      "Epoch 1164/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2533 - accuracy: 0.9118\n",
      "\n",
      "Epoch 01164: loss did not improve from 0.25142\n",
      "Epoch 1165/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2538 - accuracy: 0.9104\n",
      "\n",
      "Epoch 01165: loss did not improve from 0.25142\n",
      "Epoch 1166/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2439 - accuracy: 0.9139\n",
      "\n",
      "Epoch 01166: loss improved from 0.25142 to 0.25070, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1167/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2341 - accuracy: 0.9190\n",
      "\n",
      "Epoch 01167: loss improved from 0.25070 to 0.25046, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1168/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2407 - accuracy: 0.9156\n",
      "\n",
      "Epoch 01168: loss did not improve from 0.25046\n",
      "Epoch 1169/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2596 - accuracy: 0.9067\n",
      "\n",
      "Epoch 01169: loss did not improve from 0.25046\n",
      "Epoch 1170/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2365 - accuracy: 0.9182\n",
      "\n",
      "Epoch 01170: loss did not improve from 0.25046\n",
      "Epoch 1171/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2535 - accuracy: 0.9038\n",
      "\n",
      "Epoch 01171: loss did not improve from 0.25046\n",
      "Epoch 1172/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2473 - accuracy: 0.9086\n",
      "\n",
      "Epoch 01172: loss improved from 0.25046 to 0.24822, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1173/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2312 - accuracy: 0.9160\n",
      "\n",
      "Epoch 01173: loss improved from 0.24822 to 0.24571, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1174/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2468 - accuracy: 0.9114\n",
      "\n",
      "Epoch 01174: loss did not improve from 0.24571\n",
      "Epoch 1175/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2538 - accuracy: 0.9123\n",
      "\n",
      "Epoch 01175: loss did not improve from 0.24571\n",
      "Epoch 1176/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2464 - accuracy: 0.9118\n",
      "\n",
      "Epoch 01176: loss did not improve from 0.24571\n",
      "Epoch 1177/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2594 - accuracy: 0.9072\n",
      "\n",
      "Epoch 01177: loss did not improve from 0.24571\n",
      "Epoch 1178/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2517 - accuracy: 0.9146\n",
      "\n",
      "Epoch 01178: loss did not improve from 0.24571\n",
      "Epoch 1179/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2408 - accuracy: 0.9167\n",
      "\n",
      "Epoch 01179: loss did not improve from 0.24571\n",
      "Epoch 1180/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2427 - accuracy: 0.9118\n",
      "\n",
      "Epoch 01180: loss did not improve from 0.24571\n",
      "Epoch 1181/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2502 - accuracy: 0.9091\n",
      "\n",
      "Epoch 01181: loss did not improve from 0.24571\n",
      "Epoch 1182/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2448 - accuracy: 0.9132\n",
      "\n",
      "Epoch 01182: loss did not improve from 0.24571\n",
      "Epoch 1183/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2619 - accuracy: 0.9074\n",
      "\n",
      "Epoch 01183: loss did not improve from 0.24571\n",
      "Epoch 1184/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2844 - accuracy: 0.8977\n",
      "\n",
      "Epoch 01184: loss did not improve from 0.24571\n",
      "Epoch 1185/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3701 - accuracy: 0.8699\n",
      "\n",
      "Epoch 01185: loss did not improve from 0.24571\n",
      "Epoch 1186/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3671 - accuracy: 0.8741\n",
      "\n",
      "Epoch 01186: loss did not improve from 0.24571\n",
      "Epoch 1187/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3283 - accuracy: 0.8842\n",
      "\n",
      "Epoch 01187: loss did not improve from 0.24571\n",
      "Epoch 1188/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3155 - accuracy: 0.8871\n",
      "\n",
      "Epoch 01188: loss did not improve from 0.24571\n",
      "Epoch 1189/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2942 - accuracy: 0.8985\n",
      "\n",
      "Epoch 01189: loss did not improve from 0.24571\n",
      "Epoch 1190/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2800 - accuracy: 0.9010\n",
      "\n",
      "Epoch 01190: loss did not improve from 0.24571\n",
      "Epoch 1191/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2661 - accuracy: 0.9118\n",
      "\n",
      "Epoch 01191: loss did not improve from 0.24571\n",
      "Epoch 1192/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2467 - accuracy: 0.9125\n",
      "\n",
      "Epoch 01192: loss did not improve from 0.24571\n",
      "Epoch 1193/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2361 - accuracy: 0.9193\n",
      "\n",
      "Epoch 01193: loss did not improve from 0.24571\n",
      "Epoch 1194/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2433 - accuracy: 0.9108\n",
      "\n",
      "Epoch 01194: loss improved from 0.24571 to 0.24351, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1195/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2326 - accuracy: 0.9137\n",
      "\n",
      "Epoch 01195: loss improved from 0.24351 to 0.23908, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1196/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2342 - accuracy: 0.9187\n",
      "\n",
      "Epoch 01196: loss did not improve from 0.23908\n",
      "Epoch 1197/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2465 - accuracy: 0.9123\n",
      "\n",
      "Epoch 01197: loss did not improve from 0.23908\n",
      "Epoch 1198/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2336 - accuracy: 0.9142\n",
      "\n",
      "Epoch 01198: loss did not improve from 0.23908\n",
      "Epoch 1199/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2192 - accuracy: 0.9196\n",
      "\n",
      "Epoch 01199: loss did not improve from 0.23908\n",
      "Epoch 1200/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2418 - accuracy: 0.9102\n",
      "\n",
      "Epoch 01200: loss improved from 0.23908 to 0.23845, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1201/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2254 - accuracy: 0.9200\n",
      "\n",
      "Epoch 01201: loss improved from 0.23845 to 0.23817, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1202/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2334 - accuracy: 0.9133\n",
      "\n",
      "Epoch 01202: loss did not improve from 0.23817\n",
      "Epoch 1203/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2315 - accuracy: 0.9160\n",
      "\n",
      "Epoch 01203: loss did not improve from 0.23817\n",
      "Epoch 1204/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2411 - accuracy: 0.9112\n",
      "\n",
      "Epoch 01204: loss did not improve from 0.23817\n",
      "Epoch 1205/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2398 - accuracy: 0.9108\n",
      "\n",
      "Epoch 01205: loss did not improve from 0.23817\n",
      "Epoch 1206/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2364 - accuracy: 0.9129\n",
      "\n",
      "Epoch 01206: loss improved from 0.23817 to 0.23756, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1207/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2205 - accuracy: 0.9201\n",
      "\n",
      "Epoch 01207: loss improved from 0.23756 to 0.23733, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1208/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2335 - accuracy: 0.9116\n",
      "\n",
      "Epoch 01208: loss improved from 0.23733 to 0.23682, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1209/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2427 - accuracy: 0.9124\n",
      "\n",
      "Epoch 01209: loss did not improve from 0.23682\n",
      "Epoch 1210/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2410 - accuracy: 0.9136\n",
      "\n",
      "Epoch 01210: loss did not improve from 0.23682\n",
      "Epoch 1211/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2556 - accuracy: 0.9046\n",
      "\n",
      "Epoch 01211: loss did not improve from 0.23682\n",
      "Epoch 1212/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2647 - accuracy: 0.9072\n",
      "\n",
      "Epoch 01212: loss did not improve from 0.23682\n",
      "Epoch 1213/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3044 - accuracy: 0.8942\n",
      "\n",
      "Epoch 01213: loss did not improve from 0.23682\n",
      "Epoch 1214/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2870 - accuracy: 0.8983\n",
      "\n",
      "Epoch 01214: loss did not improve from 0.23682\n",
      "Epoch 1215/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2771 - accuracy: 0.9076\n",
      "\n",
      "Epoch 01215: loss did not improve from 0.23682\n",
      "Epoch 1216/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2597 - accuracy: 0.9090\n",
      "\n",
      "Epoch 01216: loss did not improve from 0.23682\n",
      "Epoch 1217/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2556 - accuracy: 0.9074\n",
      "\n",
      "Epoch 01217: loss did not improve from 0.23682\n",
      "Epoch 1218/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2455 - accuracy: 0.9128\n",
      "\n",
      "Epoch 01218: loss did not improve from 0.23682\n",
      "Epoch 1219/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2671 - accuracy: 0.9030\n",
      "\n",
      "Epoch 01219: loss did not improve from 0.23682\n",
      "Epoch 1220/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2523 - accuracy: 0.9095\n",
      "\n",
      "Epoch 01220: loss did not improve from 0.23682\n",
      "Epoch 1221/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2473 - accuracy: 0.9095\n",
      "\n",
      "Epoch 01221: loss did not improve from 0.23682\n",
      "Epoch 1222/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2549 - accuracy: 0.9027\n",
      "\n",
      "Epoch 01222: loss did not improve from 0.23682\n",
      "Epoch 1223/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2289 - accuracy: 0.9197\n",
      "\n",
      "Epoch 01223: loss did not improve from 0.23682\n",
      "Epoch 1224/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2425 - accuracy: 0.9132\n",
      "\n",
      "Epoch 01224: loss did not improve from 0.23682\n",
      "Epoch 1225/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2366 - accuracy: 0.9171\n",
      "\n",
      "Epoch 01225: loss did not improve from 0.23682\n",
      "Epoch 1226/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2287 - accuracy: 0.9150\n",
      "\n",
      "Epoch 01226: loss improved from 0.23682 to 0.23270, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1227/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2397 - accuracy: 0.9111\n",
      "\n",
      "Epoch 01227: loss did not improve from 0.23270\n",
      "Epoch 1228/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2368 - accuracy: 0.9122\n",
      "\n",
      "Epoch 01228: loss did not improve from 0.23270\n",
      "Epoch 1229/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2462 - accuracy: 0.9110\n",
      "\n",
      "Epoch 01229: loss did not improve from 0.23270\n",
      "Epoch 1230/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2326 - accuracy: 0.9141\n",
      "\n",
      "Epoch 01230: loss did not improve from 0.23270\n",
      "Epoch 1231/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2522 - accuracy: 0.9083\n",
      "\n",
      "Epoch 01231: loss did not improve from 0.23270\n",
      "Epoch 1232/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2374 - accuracy: 0.9128\n",
      "\n",
      "Epoch 01232: loss did not improve from 0.23270\n",
      "Epoch 1233/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2222 - accuracy: 0.9145\n",
      "\n",
      "Epoch 01233: loss did not improve from 0.23270\n",
      "Epoch 1234/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2371 - accuracy: 0.9112\n",
      "\n",
      "Epoch 01234: loss did not improve from 0.23270\n",
      "Epoch 1235/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2422 - accuracy: 0.9134\n",
      "\n",
      "Epoch 01235: loss did not improve from 0.23270\n",
      "Epoch 1236/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2422 - accuracy: 0.9100\n",
      "\n",
      "Epoch 01236: loss did not improve from 0.23270\n",
      "Epoch 1237/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2350 - accuracy: 0.9141\n",
      "\n",
      "Epoch 01237: loss did not improve from 0.23270\n",
      "Epoch 1238/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2388 - accuracy: 0.9131\n",
      "\n",
      "Epoch 01238: loss did not improve from 0.23270\n",
      "Epoch 1239/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2294 - accuracy: 0.9146\n",
      "\n",
      "Epoch 01239: loss did not improve from 0.23270\n",
      "Epoch 1240/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2384 - accuracy: 0.9122\n",
      "\n",
      "Epoch 01240: loss did not improve from 0.23270\n",
      "Epoch 1241/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2494 - accuracy: 0.9125\n",
      "\n",
      "Epoch 01241: loss did not improve from 0.23270\n",
      "Epoch 1242/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2536 - accuracy: 0.9117\n",
      "\n",
      "Epoch 01242: loss did not improve from 0.23270\n",
      "Epoch 1243/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2725 - accuracy: 0.9014\n",
      "\n",
      "Epoch 01243: loss did not improve from 0.23270\n",
      "Epoch 1244/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2648 - accuracy: 0.9008\n",
      "\n",
      "Epoch 01244: loss did not improve from 0.23270\n",
      "Epoch 1245/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2633 - accuracy: 0.8994\n",
      "\n",
      "Epoch 01245: loss did not improve from 0.23270\n",
      "Epoch 1246/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2447 - accuracy: 0.9126\n",
      "\n",
      "Epoch 01246: loss did not improve from 0.23270\n",
      "Epoch 1247/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2668 - accuracy: 0.9059\n",
      "\n",
      "Epoch 01247: loss did not improve from 0.23270\n",
      "Epoch 1248/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2280 - accuracy: 0.9181\n",
      "\n",
      "Epoch 01248: loss did not improve from 0.23270\n",
      "Epoch 1249/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2352 - accuracy: 0.9154\n",
      "\n",
      "Epoch 01249: loss did not improve from 0.23270\n",
      "Epoch 1250/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2450 - accuracy: 0.9089\n",
      "\n",
      "Epoch 01250: loss did not improve from 0.23270\n",
      "Epoch 1251/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2258 - accuracy: 0.9184\n",
      "\n",
      "Epoch 01251: loss did not improve from 0.23270\n",
      "Epoch 1252/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2320 - accuracy: 0.9193\n",
      "\n",
      "Epoch 01252: loss did not improve from 0.23270\n",
      "Epoch 1253/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2313 - accuracy: 0.9136\n",
      "\n",
      "Epoch 01253: loss did not improve from 0.23270\n",
      "Epoch 1254/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2265 - accuracy: 0.9231\n",
      "\n",
      "Epoch 01254: loss did not improve from 0.23270\n",
      "Epoch 1255/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2346 - accuracy: 0.9174\n",
      "\n",
      "Epoch 01255: loss did not improve from 0.23270\n",
      "Epoch 1256/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2242 - accuracy: 0.9181\n",
      "\n",
      "Epoch 01256: loss did not improve from 0.23270\n",
      "Epoch 1257/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2598 - accuracy: 0.9071\n",
      "\n",
      "Epoch 01257: loss did not improve from 0.23270\n",
      "Epoch 1258/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2593 - accuracy: 0.9076\n",
      "\n",
      "Epoch 01258: loss did not improve from 0.23270\n",
      "Epoch 1259/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2622 - accuracy: 0.9049\n",
      "\n",
      "Epoch 01259: loss did not improve from 0.23270\n",
      "Epoch 1260/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2397 - accuracy: 0.9160\n",
      "\n",
      "Epoch 01260: loss did not improve from 0.23270\n",
      "Epoch 1261/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2227 - accuracy: 0.9219\n",
      "\n",
      "Epoch 01261: loss did not improve from 0.23270\n",
      "Epoch 1262/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2272 - accuracy: 0.9168\n",
      "\n",
      "Epoch 01262: loss improved from 0.23270 to 0.23231, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1263/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2258 - accuracy: 0.9190\n",
      "\n",
      "Epoch 01263: loss did not improve from 0.23231\n",
      "Epoch 1264/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2305 - accuracy: 0.9195\n",
      "\n",
      "Epoch 01264: loss improved from 0.23231 to 0.23193, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1265/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2207 - accuracy: 0.9220\n",
      "\n",
      "Epoch 01265: loss improved from 0.23193 to 0.22826, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1266/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2282 - accuracy: 0.9161\n",
      "\n",
      "Epoch 01266: loss improved from 0.22826 to 0.22738, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1267/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2184 - accuracy: 0.9126\n",
      "\n",
      "Epoch 01267: loss did not improve from 0.22738\n",
      "Epoch 1268/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2212 - accuracy: 0.9156\n",
      "\n",
      "Epoch 01268: loss improved from 0.22738 to 0.22642, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1269/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2306 - accuracy: 0.9144\n",
      "\n",
      "Epoch 01269: loss did not improve from 0.22642\n",
      "Epoch 1270/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2441 - accuracy: 0.9096\n",
      "\n",
      "Epoch 01270: loss did not improve from 0.22642\n",
      "Epoch 1271/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2474 - accuracy: 0.9107\n",
      "\n",
      "Epoch 01271: loss did not improve from 0.22642\n",
      "Epoch 1272/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2676 - accuracy: 0.9061\n",
      "\n",
      "Epoch 01272: loss did not improve from 0.22642\n",
      "Epoch 1273/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2676 - accuracy: 0.9061\n",
      "\n",
      "Epoch 01273: loss did not improve from 0.22642\n",
      "Epoch 1274/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3017 - accuracy: 0.8912\n",
      "\n",
      "Epoch 01274: loss did not improve from 0.22642\n",
      "Epoch 1275/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3003 - accuracy: 0.8968\n",
      "\n",
      "Epoch 01275: loss did not improve from 0.22642\n",
      "Epoch 1276/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2873 - accuracy: 0.8981\n",
      "\n",
      "Epoch 01276: loss did not improve from 0.22642\n",
      "Epoch 1277/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2521 - accuracy: 0.9098\n",
      "\n",
      "Epoch 01277: loss did not improve from 0.22642\n",
      "Epoch 1278/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2799 - accuracy: 0.9018\n",
      "\n",
      "Epoch 01278: loss did not improve from 0.22642\n",
      "Epoch 1279/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2927 - accuracy: 0.8966\n",
      "\n",
      "Epoch 01279: loss did not improve from 0.22642\n",
      "Epoch 1280/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3198 - accuracy: 0.8941\n",
      "\n",
      "Epoch 01280: loss did not improve from 0.22642\n",
      "Epoch 1281/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3109 - accuracy: 0.8933\n",
      "\n",
      "Epoch 01281: loss did not improve from 0.22642\n",
      "Epoch 1282/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2864 - accuracy: 0.9014\n",
      "\n",
      "Epoch 01282: loss did not improve from 0.22642\n",
      "Epoch 1283/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2487 - accuracy: 0.9115\n",
      "\n",
      "Epoch 01283: loss did not improve from 0.22642\n",
      "Epoch 1284/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2316 - accuracy: 0.9157\n",
      "\n",
      "Epoch 01284: loss did not improve from 0.22642\n",
      "Epoch 1285/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2258 - accuracy: 0.9197\n",
      "\n",
      "Epoch 01285: loss did not improve from 0.22642\n",
      "Epoch 1286/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2234 - accuracy: 0.9138\n",
      "\n",
      "Epoch 01286: loss did not improve from 0.22642\n",
      "Epoch 1287/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2274 - accuracy: 0.9215\n",
      "\n",
      "Epoch 01287: loss did not improve from 0.22642\n",
      "Epoch 1288/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2234 - accuracy: 0.9193\n",
      "\n",
      "Epoch 01288: loss improved from 0.22642 to 0.22470, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1289/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2120 - accuracy: 0.9181\n",
      "\n",
      "Epoch 01289: loss improved from 0.22470 to 0.22145, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1290/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2012 - accuracy: 0.9258\n",
      "\n",
      "Epoch 01290: loss improved from 0.22145 to 0.22038, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1291/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2233 - accuracy: 0.9174\n",
      "\n",
      "Epoch 01291: loss did not improve from 0.22038\n",
      "Epoch 1292/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2236 - accuracy: 0.9157\n",
      "\n",
      "Epoch 01292: loss did not improve from 0.22038\n",
      "Epoch 1293/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2174 - accuracy: 0.9174\n",
      "\n",
      "Epoch 01293: loss improved from 0.22038 to 0.21816, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1294/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2070 - accuracy: 0.9200\n",
      "\n",
      "Epoch 01294: loss improved from 0.21816 to 0.21816, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1295/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2176 - accuracy: 0.9167\n",
      "\n",
      "Epoch 01295: loss did not improve from 0.21816\n",
      "Epoch 1296/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2124 - accuracy: 0.9180\n",
      "\n",
      "Epoch 01296: loss did not improve from 0.21816\n",
      "Epoch 1297/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2038 - accuracy: 0.9219\n",
      "\n",
      "Epoch 01297: loss did not improve from 0.21816\n",
      "Epoch 1298/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2215 - accuracy: 0.9119\n",
      "\n",
      "Epoch 01298: loss did not improve from 0.21816\n",
      "Epoch 1299/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2255 - accuracy: 0.9138\n",
      "\n",
      "Epoch 01299: loss did not improve from 0.21816\n",
      "Epoch 1300/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2154 - accuracy: 0.9169\n",
      "\n",
      "Epoch 01300: loss did not improve from 0.21816\n",
      "Epoch 1301/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2203 - accuracy: 0.9186\n",
      "\n",
      "Epoch 01301: loss did not improve from 0.21816\n",
      "Epoch 1302/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2163 - accuracy: 0.9234\n",
      "\n",
      "Epoch 01302: loss did not improve from 0.21816\n",
      "Epoch 1303/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2062 - accuracy: 0.9194\n",
      "\n",
      "Epoch 01303: loss did not improve from 0.21816\n",
      "Epoch 1304/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2136 - accuracy: 0.9260\n",
      "\n",
      "Epoch 01304: loss did not improve from 0.21816\n",
      "Epoch 1305/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2184 - accuracy: 0.9120\n",
      "\n",
      "Epoch 01305: loss did not improve from 0.21816\n",
      "Epoch 1306/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2363 - accuracy: 0.9157\n",
      "\n",
      "Epoch 01306: loss did not improve from 0.21816\n",
      "Epoch 1307/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2272 - accuracy: 0.9175\n",
      "\n",
      "Epoch 01307: loss did not improve from 0.21816\n",
      "Epoch 1308/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2181 - accuracy: 0.9175\n",
      "\n",
      "Epoch 01308: loss did not improve from 0.21816\n",
      "Epoch 1309/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2204 - accuracy: 0.9185\n",
      "\n",
      "Epoch 01309: loss did not improve from 0.21816\n",
      "Epoch 1310/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2400 - accuracy: 0.9140\n",
      "\n",
      "Epoch 01310: loss did not improve from 0.21816\n",
      "Epoch 1311/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2745 - accuracy: 0.9042\n",
      "\n",
      "Epoch 01311: loss did not improve from 0.21816\n",
      "Epoch 1312/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2531 - accuracy: 0.9090\n",
      "\n",
      "Epoch 01312: loss did not improve from 0.21816\n",
      "Epoch 1313/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2598 - accuracy: 0.9067\n",
      "\n",
      "Epoch 01313: loss did not improve from 0.21816\n",
      "Epoch 1314/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2443 - accuracy: 0.9111\n",
      "\n",
      "Epoch 01314: loss did not improve from 0.21816\n",
      "Epoch 1315/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2436 - accuracy: 0.9101\n",
      "\n",
      "Epoch 01315: loss did not improve from 0.21816\n",
      "Epoch 1316/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2453 - accuracy: 0.9070\n",
      "\n",
      "Epoch 01316: loss did not improve from 0.21816\n",
      "Epoch 1317/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2360 - accuracy: 0.9127\n",
      "\n",
      "Epoch 01317: loss did not improve from 0.21816\n",
      "Epoch 1318/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2265 - accuracy: 0.9181\n",
      "\n",
      "Epoch 01318: loss did not improve from 0.21816\n",
      "Epoch 1319/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2219 - accuracy: 0.9173\n",
      "\n",
      "Epoch 01319: loss did not improve from 0.21816\n",
      "Epoch 1320/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2168 - accuracy: 0.9176\n",
      "\n",
      "Epoch 01320: loss did not improve from 0.21816\n",
      "Epoch 1321/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2153 - accuracy: 0.9176\n",
      "\n",
      "Epoch 01321: loss did not improve from 0.21816\n",
      "Epoch 1322/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2279 - accuracy: 0.9174\n",
      "\n",
      "Epoch 01322: loss did not improve from 0.21816\n",
      "Epoch 1323/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2141 - accuracy: 0.9168\n",
      "\n",
      "Epoch 01323: loss improved from 0.21816 to 0.21590, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1324/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2018 - accuracy: 0.9232\n",
      "\n",
      "Epoch 01324: loss did not improve from 0.21590\n",
      "Epoch 1325/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2156 - accuracy: 0.9186\n",
      "\n",
      "Epoch 01325: loss did not improve from 0.21590\n",
      "Epoch 1326/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2157 - accuracy: 0.9141\n",
      "\n",
      "Epoch 01326: loss did not improve from 0.21590\n",
      "Epoch 1327/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2006 - accuracy: 0.9235\n",
      "\n",
      "Epoch 01327: loss did not improve from 0.21590\n",
      "Epoch 1328/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2046 - accuracy: 0.9194\n",
      "\n",
      "Epoch 01328: loss improved from 0.21590 to 0.21239, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1329/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2055 - accuracy: 0.9211\n",
      "\n",
      "Epoch 01329: loss did not improve from 0.21239\n",
      "Epoch 1330/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2267 - accuracy: 0.9158\n",
      "\n",
      "Epoch 01330: loss did not improve from 0.21239\n",
      "Epoch 1331/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2079 - accuracy: 0.9268\n",
      "\n",
      "Epoch 01331: loss did not improve from 0.21239\n",
      "Epoch 1332/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2084 - accuracy: 0.9226\n",
      "\n",
      "Epoch 01332: loss did not improve from 0.21239\n",
      "Epoch 1333/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2063 - accuracy: 0.9217\n",
      "\n",
      "Epoch 01333: loss did not improve from 0.21239\n",
      "Epoch 1334/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2202 - accuracy: 0.9200\n",
      "\n",
      "Epoch 01334: loss did not improve from 0.21239\n",
      "Epoch 1335/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2010 - accuracy: 0.9243\n",
      "\n",
      "Epoch 01335: loss did not improve from 0.21239\n",
      "Epoch 1336/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2128 - accuracy: 0.9178\n",
      "\n",
      "Epoch 01336: loss did not improve from 0.21239\n",
      "Epoch 1337/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1993 - accuracy: 0.9242\n",
      "\n",
      "Epoch 01337: loss did not improve from 0.21239\n",
      "Epoch 1338/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2119 - accuracy: 0.9180\n",
      "\n",
      "Epoch 01338: loss did not improve from 0.21239\n",
      "Epoch 1339/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2156 - accuracy: 0.9190\n",
      "\n",
      "Epoch 01339: loss did not improve from 0.21239\n",
      "Epoch 1340/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2230 - accuracy: 0.9156\n",
      "\n",
      "Epoch 01340: loss did not improve from 0.21239\n",
      "Epoch 1341/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2234 - accuracy: 0.9191\n",
      "\n",
      "Epoch 01341: loss did not improve from 0.21239\n",
      "Epoch 1342/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2650 - accuracy: 0.9076\n",
      "\n",
      "Epoch 01342: loss did not improve from 0.21239\n",
      "Epoch 1343/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3074 - accuracy: 0.8942\n",
      "\n",
      "Epoch 01343: loss did not improve from 0.21239\n",
      "Epoch 1344/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2900 - accuracy: 0.8945\n",
      "\n",
      "Epoch 01344: loss did not improve from 0.21239\n",
      "Epoch 1345/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2671 - accuracy: 0.9092\n",
      "\n",
      "Epoch 01345: loss did not improve from 0.21239\n",
      "Epoch 1346/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2823 - accuracy: 0.9009\n",
      "\n",
      "Epoch 01346: loss did not improve from 0.21239\n",
      "Epoch 1347/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3961 - accuracy: 0.8597\n",
      "\n",
      "Epoch 01347: loss did not improve from 0.21239\n",
      "Epoch 1348/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3299 - accuracy: 0.8852\n",
      "\n",
      "Epoch 01348: loss did not improve from 0.21239\n",
      "Epoch 1349/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2796 - accuracy: 0.8990\n",
      "\n",
      "Epoch 01349: loss did not improve from 0.21239\n",
      "Epoch 1350/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2432 - accuracy: 0.9099\n",
      "\n",
      "Epoch 01350: loss did not improve from 0.21239\n",
      "Epoch 1351/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2218 - accuracy: 0.9173\n",
      "\n",
      "Epoch 01351: loss did not improve from 0.21239\n",
      "Epoch 1352/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2357 - accuracy: 0.9090\n",
      "\n",
      "Epoch 01352: loss did not improve from 0.21239\n",
      "Epoch 1353/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2164 - accuracy: 0.9202\n",
      "\n",
      "Epoch 01353: loss did not improve from 0.21239\n",
      "Epoch 1354/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2115 - accuracy: 0.9207\n",
      "\n",
      "Epoch 01354: loss did not improve from 0.21239\n",
      "Epoch 1355/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2027 - accuracy: 0.9208\n",
      "\n",
      "Epoch 01355: loss improved from 0.21239 to 0.21229, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1356/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2070 - accuracy: 0.9176\n",
      "\n",
      "Epoch 01356: loss improved from 0.21229 to 0.21193, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1357/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1978 - accuracy: 0.9284\n",
      "\n",
      "Epoch 01357: loss improved from 0.21193 to 0.20756, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1358/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1942 - accuracy: 0.9225\n",
      "\n",
      "Epoch 01358: loss improved from 0.20756 to 0.20591, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1359/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1938 - accuracy: 0.9239\n",
      "\n",
      "Epoch 01359: loss improved from 0.20591 to 0.20474, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1360/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2065 - accuracy: 0.9238\n",
      "\n",
      "Epoch 01360: loss improved from 0.20474 to 0.20459, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1361/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2010 - accuracy: 0.9154\n",
      "\n",
      "Epoch 01361: loss did not improve from 0.20459\n",
      "Epoch 1362/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2120 - accuracy: 0.9180\n",
      "\n",
      "Epoch 01362: loss did not improve from 0.20459\n",
      "Epoch 1363/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2043 - accuracy: 0.9195\n",
      "\n",
      "Epoch 01363: loss did not improve from 0.20459\n",
      "Epoch 1364/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1942 - accuracy: 0.9225\n",
      "\n",
      "Epoch 01364: loss did not improve from 0.20459\n",
      "Epoch 1365/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1952 - accuracy: 0.9185\n",
      "\n",
      "Epoch 01365: loss did not improve from 0.20459\n",
      "Epoch 1366/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2085 - accuracy: 0.9172\n",
      "\n",
      "Epoch 01366: loss did not improve from 0.20459\n",
      "Epoch 1367/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2141 - accuracy: 0.9208\n",
      "\n",
      "Epoch 01367: loss did not improve from 0.20459\n",
      "Epoch 1368/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2143 - accuracy: 0.9121\n",
      "\n",
      "Epoch 01368: loss did not improve from 0.20459\n",
      "Epoch 1369/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2085 - accuracy: 0.9221\n",
      "\n",
      "Epoch 01369: loss did not improve from 0.20459\n",
      "Epoch 1370/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2118 - accuracy: 0.9204\n",
      "\n",
      "Epoch 01370: loss did not improve from 0.20459\n",
      "Epoch 1371/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1968 - accuracy: 0.9253\n",
      "\n",
      "Epoch 01371: loss did not improve from 0.20459\n",
      "Epoch 1372/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1840 - accuracy: 0.9266\n",
      "\n",
      "Epoch 01372: loss improved from 0.20459 to 0.20455, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1373/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2003 - accuracy: 0.9201\n",
      "\n",
      "Epoch 01373: loss did not improve from 0.20455\n",
      "Epoch 1374/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2134 - accuracy: 0.9159\n",
      "\n",
      "Epoch 01374: loss did not improve from 0.20455\n",
      "Epoch 1375/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2043 - accuracy: 0.9192\n",
      "\n",
      "Epoch 01375: loss did not improve from 0.20455\n",
      "Epoch 1376/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2098 - accuracy: 0.9184\n",
      "\n",
      "Epoch 01376: loss did not improve from 0.20455\n",
      "Epoch 1377/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2025 - accuracy: 0.9220\n",
      "\n",
      "Epoch 01377: loss did not improve from 0.20455\n",
      "Epoch 1378/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2076 - accuracy: 0.9181\n",
      "\n",
      "Epoch 01378: loss did not improve from 0.20455\n",
      "Epoch 1379/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1954 - accuracy: 0.9225\n",
      "\n",
      "Epoch 01379: loss did not improve from 0.20455\n",
      "Epoch 1380/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1974 - accuracy: 0.9241\n",
      "\n",
      "Epoch 01380: loss did not improve from 0.20455\n",
      "Epoch 1381/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2164 - accuracy: 0.9171\n",
      "\n",
      "Epoch 01381: loss did not improve from 0.20455\n",
      "Epoch 1382/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2117 - accuracy: 0.9193\n",
      "\n",
      "Epoch 01382: loss did not improve from 0.20455\n",
      "Epoch 1383/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2210 - accuracy: 0.9179\n",
      "\n",
      "Epoch 01383: loss did not improve from 0.20455\n",
      "Epoch 1384/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2249 - accuracy: 0.9170\n",
      "\n",
      "Epoch 01384: loss did not improve from 0.20455\n",
      "Epoch 1385/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2161 - accuracy: 0.9199\n",
      "\n",
      "Epoch 01385: loss did not improve from 0.20455\n",
      "Epoch 1386/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2255 - accuracy: 0.9147\n",
      "\n",
      "Epoch 01386: loss did not improve from 0.20455\n",
      "Epoch 1387/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2369 - accuracy: 0.9138\n",
      "\n",
      "Epoch 01387: loss did not improve from 0.20455\n",
      "Epoch 1388/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2405 - accuracy: 0.9139\n",
      "\n",
      "Epoch 01388: loss did not improve from 0.20455\n",
      "Epoch 1389/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2195 - accuracy: 0.9171\n",
      "\n",
      "Epoch 01389: loss did not improve from 0.20455\n",
      "Epoch 1390/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2165 - accuracy: 0.9181\n",
      "\n",
      "Epoch 01390: loss did not improve from 0.20455\n",
      "Epoch 1391/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2116 - accuracy: 0.9176\n",
      "\n",
      "Epoch 01391: loss did not improve from 0.20455\n",
      "Epoch 1392/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2028 - accuracy: 0.9218\n",
      "\n",
      "Epoch 01392: loss did not improve from 0.20455\n",
      "Epoch 1393/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2186 - accuracy: 0.9171\n",
      "\n",
      "Epoch 01393: loss did not improve from 0.20455\n",
      "Epoch 1394/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2134 - accuracy: 0.9187\n",
      "\n",
      "Epoch 01394: loss did not improve from 0.20455\n",
      "Epoch 1395/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2247 - accuracy: 0.9136\n",
      "\n",
      "Epoch 01395: loss did not improve from 0.20455\n",
      "Epoch 1396/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2135 - accuracy: 0.9160\n",
      "\n",
      "Epoch 01396: loss did not improve from 0.20455\n",
      "Epoch 1397/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2279 - accuracy: 0.9144\n",
      "\n",
      "Epoch 01397: loss did not improve from 0.20455\n",
      "Epoch 1398/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1970 - accuracy: 0.9235\n",
      "\n",
      "Epoch 01398: loss did not improve from 0.20455\n",
      "Epoch 1399/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1971 - accuracy: 0.9241\n",
      "\n",
      "Epoch 01399: loss did not improve from 0.20455\n",
      "Epoch 1400/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2097 - accuracy: 0.9187\n",
      "\n",
      "Epoch 01400: loss did not improve from 0.20455\n",
      "Epoch 1401/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1952 - accuracy: 0.9191\n",
      "\n",
      "Epoch 01401: loss did not improve from 0.20455\n",
      "Epoch 1402/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2058 - accuracy: 0.9165\n",
      "\n",
      "Epoch 01402: loss did not improve from 0.20455\n",
      "Epoch 1403/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2055 - accuracy: 0.9209\n",
      "\n",
      "Epoch 01403: loss did not improve from 0.20455\n",
      "Epoch 1404/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1989 - accuracy: 0.9240\n",
      "\n",
      "Epoch 01404: loss did not improve from 0.20455\n",
      "Epoch 1405/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2206 - accuracy: 0.9160\n",
      "\n",
      "Epoch 01405: loss did not improve from 0.20455\n",
      "Epoch 1406/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2477 - accuracy: 0.9066\n",
      "\n",
      "Epoch 01406: loss did not improve from 0.20455\n",
      "Epoch 1407/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2431 - accuracy: 0.9117\n",
      "\n",
      "Epoch 01407: loss did not improve from 0.20455\n",
      "Epoch 1408/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2572 - accuracy: 0.9029\n",
      "\n",
      "Epoch 01408: loss did not improve from 0.20455\n",
      "Epoch 1409/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2739 - accuracy: 0.9036\n",
      "\n",
      "Epoch 01409: loss did not improve from 0.20455\n",
      "Epoch 1410/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2809 - accuracy: 0.8984\n",
      "\n",
      "Epoch 01410: loss did not improve from 0.20455\n",
      "Epoch 1411/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2620 - accuracy: 0.9047\n",
      "\n",
      "Epoch 01411: loss did not improve from 0.20455\n",
      "Epoch 1412/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2611 - accuracy: 0.9012\n",
      "\n",
      "Epoch 01412: loss did not improve from 0.20455\n",
      "Epoch 1413/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2420 - accuracy: 0.9080\n",
      "\n",
      "Epoch 01413: loss did not improve from 0.20455\n",
      "Epoch 1414/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2286 - accuracy: 0.9101\n",
      "\n",
      "Epoch 01414: loss did not improve from 0.20455\n",
      "Epoch 1415/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2157 - accuracy: 0.9164\n",
      "\n",
      "Epoch 01415: loss did not improve from 0.20455\n",
      "Epoch 1416/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2141 - accuracy: 0.9156\n",
      "\n",
      "Epoch 01416: loss did not improve from 0.20455\n",
      "Epoch 1417/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2036 - accuracy: 0.9207\n",
      "\n",
      "Epoch 01417: loss improved from 0.20455 to 0.20224, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1418/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1827 - accuracy: 0.9314\n",
      "\n",
      "Epoch 01418: loss improved from 0.20224 to 0.20009, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1419/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1786 - accuracy: 0.9320\n",
      "\n",
      "Epoch 01419: loss improved from 0.20009 to 0.19765, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1420/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1941 - accuracy: 0.9252\n",
      "\n",
      "Epoch 01420: loss did not improve from 0.19765\n",
      "Epoch 1421/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1848 - accuracy: 0.9315\n",
      "\n",
      "Epoch 01421: loss did not improve from 0.19765\n",
      "Epoch 1422/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2025 - accuracy: 0.9194\n",
      "\n",
      "Epoch 01422: loss did not improve from 0.19765\n",
      "Epoch 1423/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2033 - accuracy: 0.9202\n",
      "\n",
      "Epoch 01423: loss did not improve from 0.19765\n",
      "Epoch 1424/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1915 - accuracy: 0.9247\n",
      "\n",
      "Epoch 01424: loss did not improve from 0.19765\n",
      "Epoch 1425/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1989 - accuracy: 0.9218\n",
      "\n",
      "Epoch 01425: loss did not improve from 0.19765\n",
      "Epoch 1426/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1881 - accuracy: 0.9264\n",
      "\n",
      "Epoch 01426: loss improved from 0.19765 to 0.19741, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1427/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2057 - accuracy: 0.9190\n",
      "\n",
      "Epoch 01427: loss did not improve from 0.19741\n",
      "Epoch 1428/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2053 - accuracy: 0.9164\n",
      "\n",
      "Epoch 01428: loss did not improve from 0.19741\n",
      "Epoch 1429/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1920 - accuracy: 0.9251\n",
      "\n",
      "Epoch 01429: loss did not improve from 0.19741\n",
      "Epoch 1430/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1963 - accuracy: 0.9215\n",
      "\n",
      "Epoch 01430: loss did not improve from 0.19741\n",
      "Epoch 1431/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1925 - accuracy: 0.9247\n",
      "\n",
      "Epoch 01431: loss did not improve from 0.19741\n",
      "Epoch 1432/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2002 - accuracy: 0.9217\n",
      "\n",
      "Epoch 01432: loss did not improve from 0.19741\n",
      "Epoch 1433/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2003 - accuracy: 0.9224\n",
      "\n",
      "Epoch 01433: loss did not improve from 0.19741\n",
      "Epoch 1434/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2035 - accuracy: 0.9218\n",
      "\n",
      "Epoch 01434: loss did not improve from 0.19741\n",
      "Epoch 1435/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1937 - accuracy: 0.9212\n",
      "\n",
      "Epoch 01435: loss did not improve from 0.19741\n",
      "Epoch 1436/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2002 - accuracy: 0.9219\n",
      "\n",
      "Epoch 01436: loss did not improve from 0.19741\n",
      "Epoch 1437/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2024 - accuracy: 0.9190\n",
      "\n",
      "Epoch 01437: loss did not improve from 0.19741\n",
      "Epoch 1438/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2208 - accuracy: 0.9168\n",
      "\n",
      "Epoch 01438: loss did not improve from 0.19741\n",
      "Epoch 1439/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2310 - accuracy: 0.9154\n",
      "\n",
      "Epoch 01439: loss did not improve from 0.19741\n",
      "Epoch 1440/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2408 - accuracy: 0.9133\n",
      "\n",
      "Epoch 01440: loss did not improve from 0.19741\n",
      "Epoch 1441/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2750 - accuracy: 0.8988\n",
      "\n",
      "Epoch 01441: loss did not improve from 0.19741\n",
      "Epoch 1442/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2686 - accuracy: 0.8985\n",
      "\n",
      "Epoch 01442: loss did not improve from 0.19741\n",
      "Epoch 1443/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2912 - accuracy: 0.8980\n",
      "\n",
      "Epoch 01443: loss did not improve from 0.19741\n",
      "Epoch 1444/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2665 - accuracy: 0.9017\n",
      "\n",
      "Epoch 01444: loss did not improve from 0.19741\n",
      "Epoch 1445/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2966 - accuracy: 0.8912\n",
      "\n",
      "Epoch 01445: loss did not improve from 0.19741\n",
      "Epoch 1446/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2762 - accuracy: 0.8980\n",
      "\n",
      "Epoch 01446: loss did not improve from 0.19741\n",
      "Epoch 1447/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2712 - accuracy: 0.9000\n",
      "\n",
      "Epoch 01447: loss did not improve from 0.19741\n",
      "Epoch 1448/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2337 - accuracy: 0.9126\n",
      "\n",
      "Epoch 01448: loss did not improve from 0.19741\n",
      "Epoch 1449/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2017 - accuracy: 0.9247\n",
      "\n",
      "Epoch 01449: loss did not improve from 0.19741\n",
      "Epoch 1450/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1909 - accuracy: 0.9211\n",
      "\n",
      "Epoch 01450: loss did not improve from 0.19741\n",
      "Epoch 1451/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1981 - accuracy: 0.9213\n",
      "\n",
      "Epoch 01451: loss did not improve from 0.19741\n",
      "Epoch 1452/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1975 - accuracy: 0.9216\n",
      "\n",
      "Epoch 01452: loss did not improve from 0.19741\n",
      "Epoch 1453/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2010 - accuracy: 0.9152\n",
      "\n",
      "Epoch 01453: loss improved from 0.19741 to 0.19671, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1454/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2043 - accuracy: 0.9173\n",
      "\n",
      "Epoch 01454: loss improved from 0.19671 to 0.19470, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1455/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1858 - accuracy: 0.9260\n",
      "\n",
      "Epoch 01455: loss improved from 0.19470 to 0.19292, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1456/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1766 - accuracy: 0.9304\n",
      "\n",
      "Epoch 01456: loss improved from 0.19292 to 0.19239, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1457/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1888 - accuracy: 0.9246\n",
      "\n",
      "Epoch 01457: loss improved from 0.19239 to 0.19232, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1458/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2024 - accuracy: 0.9141\n",
      "\n",
      "Epoch 01458: loss did not improve from 0.19232\n",
      "Epoch 1459/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1904 - accuracy: 0.9241\n",
      "\n",
      "Epoch 01459: loss did not improve from 0.19232\n",
      "Epoch 1460/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1994 - accuracy: 0.9222\n",
      "\n",
      "Epoch 01460: loss did not improve from 0.19232\n",
      "Epoch 1461/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1845 - accuracy: 0.9242\n",
      "\n",
      "Epoch 01461: loss did not improve from 0.19232\n",
      "Epoch 1462/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1852 - accuracy: 0.9254\n",
      "\n",
      "Epoch 01462: loss did not improve from 0.19232\n",
      "Epoch 1463/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1931 - accuracy: 0.9205\n",
      "\n",
      "Epoch 01463: loss did not improve from 0.19232\n",
      "Epoch 1464/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1854 - accuracy: 0.9305\n",
      "\n",
      "Epoch 01464: loss did not improve from 0.19232\n",
      "Epoch 1465/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1820 - accuracy: 0.9245\n",
      "\n",
      "Epoch 01465: loss did not improve from 0.19232\n",
      "Epoch 1466/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1950 - accuracy: 0.9189\n",
      "\n",
      "Epoch 01466: loss did not improve from 0.19232\n",
      "Epoch 1467/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1873 - accuracy: 0.9241\n",
      "\n",
      "Epoch 01467: loss did not improve from 0.19232\n",
      "Epoch 1468/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1890 - accuracy: 0.9174\n",
      "\n",
      "Epoch 01468: loss did not improve from 0.19232\n",
      "Epoch 1469/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1842 - accuracy: 0.9266\n",
      "\n",
      "Epoch 01469: loss did not improve from 0.19232\n",
      "Epoch 1470/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1771 - accuracy: 0.9295\n",
      "\n",
      "Epoch 01470: loss did not improve from 0.19232\n",
      "Epoch 1471/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2056 - accuracy: 0.9243\n",
      "\n",
      "Epoch 01471: loss did not improve from 0.19232\n",
      "Epoch 1472/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2290 - accuracy: 0.9158\n",
      "\n",
      "Epoch 01472: loss did not improve from 0.19232\n",
      "Epoch 1473/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2221 - accuracy: 0.9169\n",
      "\n",
      "Epoch 01473: loss did not improve from 0.19232\n",
      "Epoch 1474/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2011 - accuracy: 0.9208\n",
      "\n",
      "Epoch 01474: loss did not improve from 0.19232\n",
      "Epoch 1475/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2141 - accuracy: 0.9192\n",
      "\n",
      "Epoch 01475: loss did not improve from 0.19232\n",
      "Epoch 1476/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2131 - accuracy: 0.9182\n",
      "\n",
      "Epoch 01476: loss did not improve from 0.19232\n",
      "Epoch 1477/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2010 - accuracy: 0.9137\n",
      "\n",
      "Epoch 01477: loss did not improve from 0.19232\n",
      "Epoch 1478/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1889 - accuracy: 0.9273\n",
      "\n",
      "Epoch 01478: loss did not improve from 0.19232\n",
      "Epoch 1479/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1928 - accuracy: 0.9220\n",
      "\n",
      "Epoch 01479: loss did not improve from 0.19232\n",
      "Epoch 1480/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1961 - accuracy: 0.9235\n",
      "\n",
      "Epoch 01480: loss did not improve from 0.19232\n",
      "Epoch 1481/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1942 - accuracy: 0.9215\n",
      "\n",
      "Epoch 01481: loss did not improve from 0.19232\n",
      "Epoch 1482/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1946 - accuracy: 0.9271\n",
      "\n",
      "Epoch 01482: loss did not improve from 0.19232\n",
      "Epoch 1483/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1889 - accuracy: 0.9265\n",
      "\n",
      "Epoch 01483: loss did not improve from 0.19232\n",
      "Epoch 1484/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2020 - accuracy: 0.9255\n",
      "\n",
      "Epoch 01484: loss did not improve from 0.19232\n",
      "Epoch 1485/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1908 - accuracy: 0.9271\n",
      "\n",
      "Epoch 01485: loss did not improve from 0.19232\n",
      "Epoch 1486/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1893 - accuracy: 0.9232\n",
      "\n",
      "Epoch 01486: loss did not improve from 0.19232\n",
      "Epoch 1487/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2042 - accuracy: 0.9214\n",
      "\n",
      "Epoch 01487: loss did not improve from 0.19232\n",
      "Epoch 1488/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1965 - accuracy: 0.9208\n",
      "\n",
      "Epoch 01488: loss did not improve from 0.19232\n",
      "Epoch 1489/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2260 - accuracy: 0.9116\n",
      "\n",
      "Epoch 01489: loss did not improve from 0.19232\n",
      "Epoch 1490/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2050 - accuracy: 0.9181\n",
      "\n",
      "Epoch 01490: loss did not improve from 0.19232\n",
      "Epoch 1491/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2209 - accuracy: 0.9172\n",
      "\n",
      "Epoch 01491: loss did not improve from 0.19232\n",
      "Epoch 1492/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2205 - accuracy: 0.9198\n",
      "\n",
      "Epoch 01492: loss did not improve from 0.19232\n",
      "Epoch 1493/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2165 - accuracy: 0.9171\n",
      "\n",
      "Epoch 01493: loss did not improve from 0.19232\n",
      "Epoch 1494/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2026 - accuracy: 0.9227\n",
      "\n",
      "Epoch 01494: loss did not improve from 0.19232\n",
      "Epoch 1495/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1962 - accuracy: 0.9249\n",
      "\n",
      "Epoch 01495: loss did not improve from 0.19232\n",
      "Epoch 1496/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1923 - accuracy: 0.9247\n",
      "\n",
      "Epoch 01496: loss did not improve from 0.19232\n",
      "Epoch 1497/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1936 - accuracy: 0.9203\n",
      "\n",
      "Epoch 01497: loss did not improve from 0.19232\n",
      "Epoch 1498/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2016 - accuracy: 0.9137\n",
      "\n",
      "Epoch 01498: loss did not improve from 0.19232\n",
      "Epoch 1499/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1762 - accuracy: 0.9311\n",
      "\n",
      "Epoch 01499: loss improved from 0.19232 to 0.18810, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1500/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2029 - accuracy: 0.9170\n",
      "\n",
      "Epoch 01500: loss did not improve from 0.18810\n",
      "Epoch 1501/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1860 - accuracy: 0.9229\n",
      "\n",
      "Epoch 01501: loss improved from 0.18810 to 0.18803, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1502/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1886 - accuracy: 0.9217\n",
      "\n",
      "Epoch 01502: loss did not improve from 0.18803\n",
      "Epoch 1503/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1765 - accuracy: 0.9249\n",
      "\n",
      "Epoch 01503: loss did not improve from 0.18803\n",
      "Epoch 1504/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1890 - accuracy: 0.9250\n",
      "\n",
      "Epoch 01504: loss did not improve from 0.18803\n",
      "Epoch 1505/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1996 - accuracy: 0.9163\n",
      "\n",
      "Epoch 01505: loss did not improve from 0.18803\n",
      "Epoch 1506/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2312 - accuracy: 0.9109\n",
      "\n",
      "Epoch 01506: loss did not improve from 0.18803\n",
      "Epoch 1507/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2069 - accuracy: 0.9205\n",
      "\n",
      "Epoch 01507: loss did not improve from 0.18803\n",
      "Epoch 1508/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2256 - accuracy: 0.9115\n",
      "\n",
      "Epoch 01508: loss did not improve from 0.18803\n",
      "Epoch 1509/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1897 - accuracy: 0.9234\n",
      "\n",
      "Epoch 01509: loss did not improve from 0.18803\n",
      "Epoch 1510/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1970 - accuracy: 0.9253\n",
      "\n",
      "Epoch 01510: loss did not improve from 0.18803\n",
      "Epoch 1511/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2127 - accuracy: 0.9127\n",
      "\n",
      "Epoch 01511: loss did not improve from 0.18803\n",
      "Epoch 1512/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2054 - accuracy: 0.9167\n",
      "\n",
      "Epoch 01512: loss did not improve from 0.18803\n",
      "Epoch 1513/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2141 - accuracy: 0.9139\n",
      "\n",
      "Epoch 01513: loss did not improve from 0.18803\n",
      "Epoch 1514/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2024 - accuracy: 0.9163\n",
      "\n",
      "Epoch 01514: loss did not improve from 0.18803\n",
      "Epoch 1515/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1870 - accuracy: 0.9250\n",
      "\n",
      "Epoch 01515: loss did not improve from 0.18803\n",
      "Epoch 1516/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1948 - accuracy: 0.9241\n",
      "\n",
      "Epoch 01516: loss did not improve from 0.18803\n",
      "Epoch 1517/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1855 - accuracy: 0.9254\n",
      "\n",
      "Epoch 01517: loss did not improve from 0.18803\n",
      "Epoch 1518/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1859 - accuracy: 0.9260\n",
      "\n",
      "Epoch 01518: loss did not improve from 0.18803\n",
      "Epoch 1519/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1866 - accuracy: 0.9237\n",
      "\n",
      "Epoch 01519: loss did not improve from 0.18803\n",
      "Epoch 1520/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1958 - accuracy: 0.9190\n",
      "\n",
      "Epoch 01520: loss did not improve from 0.18803\n",
      "Epoch 1521/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1894 - accuracy: 0.9250\n",
      "\n",
      "Epoch 01521: loss did not improve from 0.18803\n",
      "Epoch 1522/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1897 - accuracy: 0.9226\n",
      "\n",
      "Epoch 01522: loss did not improve from 0.18803\n",
      "Epoch 1523/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1907 - accuracy: 0.9218\n",
      "\n",
      "Epoch 01523: loss did not improve from 0.18803\n",
      "Epoch 1524/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2100 - accuracy: 0.9196\n",
      "\n",
      "Epoch 01524: loss did not improve from 0.18803\n",
      "Epoch 1525/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2193 - accuracy: 0.9172\n",
      "\n",
      "Epoch 01525: loss did not improve from 0.18803\n",
      "Epoch 1526/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2133 - accuracy: 0.9158\n",
      "\n",
      "Epoch 01526: loss did not improve from 0.18803\n",
      "Epoch 1527/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2004 - accuracy: 0.9208\n",
      "\n",
      "Epoch 01527: loss did not improve from 0.18803\n",
      "Epoch 1528/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3398 - accuracy: 0.8873\n",
      "\n",
      "Epoch 01528: loss did not improve from 0.18803\n",
      "Epoch 1529/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.4130 - accuracy: 0.8608\n",
      "\n",
      "Epoch 01529: loss did not improve from 0.18803\n",
      "Epoch 1530/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3475 - accuracy: 0.8771\n",
      "\n",
      "Epoch 01530: loss did not improve from 0.18803\n",
      "Epoch 1531/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3194 - accuracy: 0.8825\n",
      "\n",
      "Epoch 01531: loss did not improve from 0.18803\n",
      "Epoch 1532/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2560 - accuracy: 0.9051\n",
      "\n",
      "Epoch 01532: loss did not improve from 0.18803\n",
      "Epoch 1533/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2353 - accuracy: 0.9089\n",
      "\n",
      "Epoch 01533: loss did not improve from 0.18803\n",
      "Epoch 1534/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2083 - accuracy: 0.9176\n",
      "\n",
      "Epoch 01534: loss did not improve from 0.18803\n",
      "Epoch 1535/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2094 - accuracy: 0.9168\n",
      "\n",
      "Epoch 01535: loss did not improve from 0.18803\n",
      "Epoch 1536/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1915 - accuracy: 0.9201\n",
      "\n",
      "Epoch 01536: loss did not improve from 0.18803\n",
      "Epoch 1537/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1762 - accuracy: 0.9284\n",
      "\n",
      "Epoch 01537: loss improved from 0.18803 to 0.18704, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1538/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1860 - accuracy: 0.9235\n",
      "\n",
      "Epoch 01538: loss did not improve from 0.18704\n",
      "Epoch 1539/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1832 - accuracy: 0.9235\n",
      "\n",
      "Epoch 01539: loss did not improve from 0.18704\n",
      "Epoch 1540/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1882 - accuracy: 0.9169\n",
      "\n",
      "Epoch 01540: loss improved from 0.18704 to 0.18445, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1541/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1821 - accuracy: 0.9255\n",
      "\n",
      "Epoch 01541: loss improved from 0.18445 to 0.18260, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1542/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1783 - accuracy: 0.9240\n",
      "\n",
      "Epoch 01542: loss improved from 0.18260 to 0.18089, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1543/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1805 - accuracy: 0.9190\n",
      "\n",
      "Epoch 01543: loss did not improve from 0.18089\n",
      "Epoch 1544/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1748 - accuracy: 0.9281\n",
      "\n",
      "Epoch 01544: loss did not improve from 0.18089\n",
      "Epoch 1545/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1844 - accuracy: 0.9199\n",
      "\n",
      "Epoch 01545: loss did not improve from 0.18089\n",
      "Epoch 1546/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1897 - accuracy: 0.9160\n",
      "\n",
      "Epoch 01546: loss did not improve from 0.18089\n",
      "Epoch 1547/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1652 - accuracy: 0.9314\n",
      "\n",
      "Epoch 01547: loss improved from 0.18089 to 0.17894, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1548/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1791 - accuracy: 0.9213\n",
      "\n",
      "Epoch 01548: loss did not improve from 0.17894\n",
      "Epoch 1549/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1764 - accuracy: 0.9213\n",
      "\n",
      "Epoch 01549: loss did not improve from 0.17894\n",
      "Epoch 1550/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1746 - accuracy: 0.9248\n",
      "\n",
      "Epoch 01550: loss improved from 0.17894 to 0.17796, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1551/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1784 - accuracy: 0.9192\n",
      "\n",
      "Epoch 01551: loss did not improve from 0.17796\n",
      "Epoch 1552/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1770 - accuracy: 0.9291\n",
      "\n",
      "Epoch 01552: loss did not improve from 0.17796\n",
      "Epoch 1553/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1700 - accuracy: 0.9327\n",
      "\n",
      "Epoch 01553: loss did not improve from 0.17796\n",
      "Epoch 1554/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1724 - accuracy: 0.9232\n",
      "\n",
      "Epoch 01554: loss did not improve from 0.17796\n",
      "Epoch 1555/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1715 - accuracy: 0.9304\n",
      "\n",
      "Epoch 01555: loss did not improve from 0.17796\n",
      "Epoch 1556/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1727 - accuracy: 0.9333\n",
      "\n",
      "Epoch 01556: loss did not improve from 0.17796\n",
      "Epoch 1557/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1858 - accuracy: 0.9280\n",
      "\n",
      "Epoch 01557: loss did not improve from 0.17796\n",
      "Epoch 1558/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1821 - accuracy: 0.9259\n",
      "\n",
      "Epoch 01558: loss did not improve from 0.17796\n",
      "Epoch 1559/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1787 - accuracy: 0.9285\n",
      "\n",
      "Epoch 01559: loss did not improve from 0.17796\n",
      "Epoch 1560/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1770 - accuracy: 0.9277\n",
      "\n",
      "Epoch 01560: loss did not improve from 0.17796\n",
      "Epoch 1561/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1717 - accuracy: 0.9272\n",
      "\n",
      "Epoch 01561: loss did not improve from 0.17796\n",
      "Epoch 1562/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1893 - accuracy: 0.9272\n",
      "\n",
      "Epoch 01562: loss did not improve from 0.17796\n",
      "Epoch 1563/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1719 - accuracy: 0.9313\n",
      "\n",
      "Epoch 01563: loss did not improve from 0.17796\n",
      "Epoch 1564/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1746 - accuracy: 0.9338\n",
      "\n",
      "Epoch 01564: loss did not improve from 0.17796\n",
      "Epoch 1565/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1729 - accuracy: 0.9283\n",
      "\n",
      "Epoch 01565: loss did not improve from 0.17796\n",
      "Epoch 1566/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1944 - accuracy: 0.9210\n",
      "\n",
      "Epoch 01566: loss did not improve from 0.17796\n",
      "Epoch 1567/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1769 - accuracy: 0.9277\n",
      "\n",
      "Epoch 01567: loss did not improve from 0.17796\n",
      "Epoch 1568/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1725 - accuracy: 0.9304\n",
      "\n",
      "Epoch 01568: loss did not improve from 0.17796\n",
      "Epoch 1569/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1799 - accuracy: 0.9233\n",
      "\n",
      "Epoch 01569: loss did not improve from 0.17796\n",
      "Epoch 1570/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1721 - accuracy: 0.9279\n",
      "\n",
      "Epoch 01570: loss improved from 0.17796 to 0.17795, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1571/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1753 - accuracy: 0.9281\n",
      "\n",
      "Epoch 01571: loss did not improve from 0.17795\n",
      "Epoch 1572/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1756 - accuracy: 0.9300\n",
      "\n",
      "Epoch 01572: loss did not improve from 0.17795\n",
      "Epoch 1573/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1805 - accuracy: 0.9263\n",
      "\n",
      "Epoch 01573: loss did not improve from 0.17795\n",
      "Epoch 1574/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1915 - accuracy: 0.9199\n",
      "\n",
      "Epoch 01574: loss did not improve from 0.17795\n",
      "Epoch 1575/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1826 - accuracy: 0.9258\n",
      "\n",
      "Epoch 01575: loss did not improve from 0.17795\n",
      "Epoch 1576/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1758 - accuracy: 0.9269\n",
      "\n",
      "Epoch 01576: loss did not improve from 0.17795\n",
      "Epoch 1577/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1809 - accuracy: 0.9246\n",
      "\n",
      "Epoch 01577: loss did not improve from 0.17795\n",
      "Epoch 1578/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1722 - accuracy: 0.9281\n",
      "\n",
      "Epoch 01578: loss did not improve from 0.17795\n",
      "Epoch 1579/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1875 - accuracy: 0.9216\n",
      "\n",
      "Epoch 01579: loss did not improve from 0.17795\n",
      "Epoch 1580/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1716 - accuracy: 0.9265\n",
      "\n",
      "Epoch 01580: loss did not improve from 0.17795\n",
      "Epoch 1581/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1911 - accuracy: 0.9197\n",
      "\n",
      "Epoch 01581: loss did not improve from 0.17795\n",
      "Epoch 1582/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2189 - accuracy: 0.9171\n",
      "\n",
      "Epoch 01582: loss did not improve from 0.17795\n",
      "Epoch 1583/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2356 - accuracy: 0.9153\n",
      "\n",
      "Epoch 01583: loss did not improve from 0.17795\n",
      "Epoch 1584/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2227 - accuracy: 0.9164\n",
      "\n",
      "Epoch 01584: loss did not improve from 0.17795\n",
      "Epoch 1585/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2500 - accuracy: 0.9043\n",
      "\n",
      "Epoch 01585: loss did not improve from 0.17795\n",
      "Epoch 1586/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2250 - accuracy: 0.9157\n",
      "\n",
      "Epoch 01586: loss did not improve from 0.17795\n",
      "Epoch 1587/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2338 - accuracy: 0.9088\n",
      "\n",
      "Epoch 01587: loss did not improve from 0.17795\n",
      "Epoch 1588/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2772 - accuracy: 0.8982\n",
      "\n",
      "Epoch 01588: loss did not improve from 0.17795\n",
      "Epoch 1589/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3126 - accuracy: 0.8955\n",
      "\n",
      "Epoch 01589: loss did not improve from 0.17795\n",
      "Epoch 1590/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2912 - accuracy: 0.8958\n",
      "\n",
      "Epoch 01590: loss did not improve from 0.17795\n",
      "Epoch 1591/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2759 - accuracy: 0.8924\n",
      "\n",
      "Epoch 01591: loss did not improve from 0.17795\n",
      "Epoch 1592/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2169 - accuracy: 0.9184\n",
      "\n",
      "Epoch 01592: loss did not improve from 0.17795\n",
      "Epoch 1593/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2095 - accuracy: 0.9181\n",
      "\n",
      "Epoch 01593: loss did not improve from 0.17795\n",
      "Epoch 1594/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1916 - accuracy: 0.9222\n",
      "\n",
      "Epoch 01594: loss did not improve from 0.17795\n",
      "Epoch 1595/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1747 - accuracy: 0.9250\n",
      "\n",
      "Epoch 01595: loss did not improve from 0.17795\n",
      "Epoch 1596/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1735 - accuracy: 0.9268\n",
      "\n",
      "Epoch 01596: loss did not improve from 0.17795\n",
      "Epoch 1597/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1712 - accuracy: 0.9264\n",
      "\n",
      "Epoch 01597: loss did not improve from 0.17795\n",
      "Epoch 1598/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1839 - accuracy: 0.9253\n",
      "\n",
      "Epoch 01598: loss did not improve from 0.17795\n",
      "Epoch 1599/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1757 - accuracy: 0.9271\n",
      "\n",
      "Epoch 01599: loss did not improve from 0.17795\n",
      "Epoch 1600/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1673 - accuracy: 0.9316\n",
      "\n",
      "Epoch 01600: loss improved from 0.17795 to 0.17740, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1601/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1702 - accuracy: 0.9299\n",
      "\n",
      "Epoch 01601: loss improved from 0.17740 to 0.17598, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1602/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1701 - accuracy: 0.9276\n",
      "\n",
      "Epoch 01602: loss did not improve from 0.17598\n",
      "Epoch 1603/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1757 - accuracy: 0.9233\n",
      "\n",
      "Epoch 01603: loss did not improve from 0.17598\n",
      "Epoch 1604/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1720 - accuracy: 0.9259\n",
      "\n",
      "Epoch 01604: loss improved from 0.17598 to 0.17552, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1605/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1688 - accuracy: 0.9270\n",
      "\n",
      "Epoch 01605: loss did not improve from 0.17552\n",
      "Epoch 1606/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1791 - accuracy: 0.9202\n",
      "\n",
      "Epoch 01606: loss did not improve from 0.17552\n",
      "Epoch 1607/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1639 - accuracy: 0.9305\n",
      "\n",
      "Epoch 01607: loss improved from 0.17552 to 0.17540, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1608/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1795 - accuracy: 0.9241\n",
      "\n",
      "Epoch 01608: loss improved from 0.17540 to 0.17364, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1609/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.1753 - accuracy: 0.9297\n",
      "\n",
      "Epoch 01609: loss did not improve from 0.17364\n",
      "Epoch 1610/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1742 - accuracy: 0.9313\n",
      "\n",
      "Epoch 01610: loss did not improve from 0.17364\n",
      "Epoch 1611/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1860 - accuracy: 0.9220\n",
      "\n",
      "Epoch 01611: loss did not improve from 0.17364\n",
      "Epoch 1612/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.1741 - accuracy: 0.9269\n",
      "\n",
      "Epoch 01612: loss did not improve from 0.17364\n",
      "Epoch 1613/2000\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.1857 - accuracy: 0.9229\n",
      "\n",
      "Epoch 01613: loss did not improve from 0.17364\n",
      "Epoch 1614/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.1862 - accuracy: 0.9207\n",
      "\n",
      "Epoch 01614: loss did not improve from 0.17364\n",
      "Epoch 1615/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1804 - accuracy: 0.9203\n",
      "\n",
      "Epoch 01615: loss did not improve from 0.17364\n",
      "Epoch 1616/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1946 - accuracy: 0.9194\n",
      "\n",
      "Epoch 01616: loss did not improve from 0.17364\n",
      "Epoch 1617/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1906 - accuracy: 0.9255\n",
      "\n",
      "Epoch 01617: loss did not improve from 0.17364\n",
      "Epoch 1618/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1947 - accuracy: 0.9237\n",
      "\n",
      "Epoch 01618: loss did not improve from 0.17364\n",
      "Epoch 1619/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1806 - accuracy: 0.9266\n",
      "\n",
      "Epoch 01619: loss did not improve from 0.17364\n",
      "Epoch 1620/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1864 - accuracy: 0.9214\n",
      "\n",
      "Epoch 01620: loss did not improve from 0.17364\n",
      "Epoch 1621/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1766 - accuracy: 0.9223\n",
      "\n",
      "Epoch 01621: loss did not improve from 0.17364\n",
      "Epoch 1622/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1788 - accuracy: 0.9271\n",
      "\n",
      "Epoch 01622: loss did not improve from 0.17364\n",
      "Epoch 1623/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1988 - accuracy: 0.9218\n",
      "\n",
      "Epoch 01623: loss did not improve from 0.17364\n",
      "Epoch 1624/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1918 - accuracy: 0.9273\n",
      "\n",
      "Epoch 01624: loss did not improve from 0.17364\n",
      "Epoch 1625/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2259 - accuracy: 0.9129\n",
      "\n",
      "Epoch 01625: loss did not improve from 0.17364\n",
      "Epoch 1626/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2047 - accuracy: 0.9193\n",
      "\n",
      "Epoch 01626: loss did not improve from 0.17364\n",
      "Epoch 1627/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2099 - accuracy: 0.9159\n",
      "\n",
      "Epoch 01627: loss did not improve from 0.17364\n",
      "Epoch 1628/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2156 - accuracy: 0.9117\n",
      "\n",
      "Epoch 01628: loss did not improve from 0.17364\n",
      "Epoch 1629/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1974 - accuracy: 0.9222\n",
      "\n",
      "Epoch 01629: loss did not improve from 0.17364\n",
      "Epoch 1630/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1873 - accuracy: 0.9273\n",
      "\n",
      "Epoch 01630: loss did not improve from 0.17364\n",
      "Epoch 1631/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1748 - accuracy: 0.9250\n",
      "\n",
      "Epoch 01631: loss did not improve from 0.17364\n",
      "Epoch 1632/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1846 - accuracy: 0.9171\n",
      "\n",
      "Epoch 01632: loss did not improve from 0.17364\n",
      "Epoch 1633/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1740 - accuracy: 0.9227\n",
      "\n",
      "Epoch 01633: loss did not improve from 0.17364\n",
      "Epoch 1634/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1658 - accuracy: 0.9249\n",
      "\n",
      "Epoch 01634: loss improved from 0.17364 to 0.17188, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1635/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1664 - accuracy: 0.9274\n",
      "\n",
      "Epoch 01635: loss improved from 0.17188 to 0.17110, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1636/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.1689 - accuracy: 0.9250\n",
      "\n",
      "Epoch 01636: loss improved from 0.17110 to 0.17071, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1637/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1686 - accuracy: 0.9270\n",
      "\n",
      "Epoch 01637: loss improved from 0.17071 to 0.16997, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1638/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1668 - accuracy: 0.9290\n",
      "\n",
      "Epoch 01638: loss did not improve from 0.16997\n",
      "Epoch 1639/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1694 - accuracy: 0.9281\n",
      "\n",
      "Epoch 01639: loss did not improve from 0.16997\n",
      "Epoch 1640/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1726 - accuracy: 0.9272\n",
      "\n",
      "Epoch 01640: loss did not improve from 0.16997\n",
      "Epoch 1641/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1574 - accuracy: 0.9387\n",
      "\n",
      "Epoch 01641: loss did not improve from 0.16997\n",
      "Epoch 1642/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.1699 - accuracy: 0.9274\n",
      "\n",
      "Epoch 01642: loss did not improve from 0.16997\n",
      "Epoch 1643/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1686 - accuracy: 0.9219\n",
      "\n",
      "Epoch 01643: loss did not improve from 0.16997\n",
      "Epoch 1644/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1669 - accuracy: 0.9312\n",
      "\n",
      "Epoch 01644: loss did not improve from 0.16997\n",
      "Epoch 1645/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1638 - accuracy: 0.9294\n",
      "\n",
      "Epoch 01645: loss did not improve from 0.16997\n",
      "Epoch 1646/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1610 - accuracy: 0.9309\n",
      "\n",
      "Epoch 01646: loss did not improve from 0.16997\n",
      "Epoch 1647/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1786 - accuracy: 0.9234\n",
      "\n",
      "Epoch 01647: loss did not improve from 0.16997\n",
      "Epoch 1648/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1760 - accuracy: 0.9261\n",
      "\n",
      "Epoch 01648: loss did not improve from 0.16997\n",
      "Epoch 1649/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1575 - accuracy: 0.9338\n",
      "\n",
      "Epoch 01649: loss did not improve from 0.16997\n",
      "Epoch 1650/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1702 - accuracy: 0.9226\n",
      "\n",
      "Epoch 01650: loss did not improve from 0.16997\n",
      "Epoch 1651/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1801 - accuracy: 0.9258\n",
      "\n",
      "Epoch 01651: loss did not improve from 0.16997\n",
      "Epoch 1652/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1857 - accuracy: 0.9210\n",
      "\n",
      "Epoch 01652: loss did not improve from 0.16997\n",
      "Epoch 1653/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1711 - accuracy: 0.9312\n",
      "\n",
      "Epoch 01653: loss did not improve from 0.16997\n",
      "Epoch 1654/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1713 - accuracy: 0.9277\n",
      "\n",
      "Epoch 01654: loss did not improve from 0.16997\n",
      "Epoch 1655/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2156 - accuracy: 0.9131\n",
      "\n",
      "Epoch 01655: loss did not improve from 0.16997\n",
      "Epoch 1656/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.2520 - accuracy: 0.9029\n",
      "\n",
      "Epoch 01656: loss did not improve from 0.16997\n",
      "Epoch 1657/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2640 - accuracy: 0.9000\n",
      "\n",
      "Epoch 01657: loss did not improve from 0.16997\n",
      "Epoch 1658/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2707 - accuracy: 0.9018\n",
      "\n",
      "Epoch 01658: loss did not improve from 0.16997\n",
      "Epoch 1659/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2630 - accuracy: 0.8995\n",
      "\n",
      "Epoch 01659: loss did not improve from 0.16997\n",
      "Epoch 1660/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2699 - accuracy: 0.8968\n",
      "\n",
      "Epoch 01660: loss did not improve from 0.16997\n",
      "Epoch 1661/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2573 - accuracy: 0.9021\n",
      "\n",
      "Epoch 01661: loss did not improve from 0.16997\n",
      "Epoch 1662/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2292 - accuracy: 0.9137\n",
      "\n",
      "Epoch 01662: loss did not improve from 0.16997\n",
      "Epoch 1663/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2250 - accuracy: 0.9149\n",
      "\n",
      "Epoch 01663: loss did not improve from 0.16997\n",
      "Epoch 1664/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2196 - accuracy: 0.9114\n",
      "\n",
      "Epoch 01664: loss did not improve from 0.16997\n",
      "Epoch 1665/2000\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.1946 - accuracy: 0.9213\n",
      "\n",
      "Epoch 01665: loss did not improve from 0.16997\n",
      "Epoch 1666/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1815 - accuracy: 0.9242\n",
      "\n",
      "Epoch 01666: loss did not improve from 0.16997\n",
      "Epoch 1667/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1824 - accuracy: 0.9207\n",
      "\n",
      "Epoch 01667: loss did not improve from 0.16997\n",
      "Epoch 1668/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1544 - accuracy: 0.9315\n",
      "\n",
      "Epoch 01668: loss did not improve from 0.16997\n",
      "Epoch 1669/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1661 - accuracy: 0.9278\n",
      "\n",
      "Epoch 01669: loss did not improve from 0.16997\n",
      "Epoch 1670/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1692 - accuracy: 0.9257\n",
      "\n",
      "Epoch 01670: loss improved from 0.16997 to 0.16925, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1671/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1689 - accuracy: 0.9214\n",
      "\n",
      "Epoch 01671: loss did not improve from 0.16925\n",
      "Epoch 1672/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1549 - accuracy: 0.9312\n",
      "\n",
      "Epoch 01672: loss did not improve from 0.16925\n",
      "Epoch 1673/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1695 - accuracy: 0.9255\n",
      "\n",
      "Epoch 01673: loss improved from 0.16925 to 0.16901, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1674/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1570 - accuracy: 0.9348\n",
      "\n",
      "Epoch 01674: loss improved from 0.16901 to 0.16713, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1675/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1670 - accuracy: 0.9288\n",
      "\n",
      "Epoch 01675: loss did not improve from 0.16713\n",
      "Epoch 1676/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1658 - accuracy: 0.9293\n",
      "\n",
      "Epoch 01676: loss did not improve from 0.16713\n",
      "Epoch 1677/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1521 - accuracy: 0.9324\n",
      "\n",
      "Epoch 01677: loss did not improve from 0.16713\n",
      "Epoch 1678/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1641 - accuracy: 0.9254\n",
      "\n",
      "Epoch 01678: loss did not improve from 0.16713\n",
      "Epoch 1679/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.1609 - accuracy: 0.9330\n",
      "\n",
      "Epoch 01679: loss did not improve from 0.16713\n",
      "Epoch 1680/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1584 - accuracy: 0.9319\n",
      "\n",
      "Epoch 01680: loss did not improve from 0.16713\n",
      "Epoch 1681/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1659 - accuracy: 0.9292\n",
      "\n",
      "Epoch 01681: loss did not improve from 0.16713\n",
      "Epoch 1682/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1593 - accuracy: 0.9294\n",
      "\n",
      "Epoch 01682: loss improved from 0.16713 to 0.16601, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1683/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1617 - accuracy: 0.9301\n",
      "\n",
      "Epoch 01683: loss did not improve from 0.16601\n",
      "Epoch 1684/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1609 - accuracy: 0.9360\n",
      "\n",
      "Epoch 01684: loss did not improve from 0.16601\n",
      "Epoch 1685/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1596 - accuracy: 0.9294\n",
      "\n",
      "Epoch 01685: loss did not improve from 0.16601\n",
      "Epoch 1686/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1660 - accuracy: 0.9271\n",
      "\n",
      "Epoch 01686: loss did not improve from 0.16601\n",
      "Epoch 1687/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1625 - accuracy: 0.9288\n",
      "\n",
      "Epoch 01687: loss did not improve from 0.16601\n",
      "Epoch 1688/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1649 - accuracy: 0.9277\n",
      "\n",
      "Epoch 01688: loss did not improve from 0.16601\n",
      "Epoch 1689/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1721 - accuracy: 0.9247\n",
      "\n",
      "Epoch 01689: loss did not improve from 0.16601\n",
      "Epoch 1690/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1631 - accuracy: 0.9277\n",
      "\n",
      "Epoch 01690: loss did not improve from 0.16601\n",
      "Epoch 1691/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1656 - accuracy: 0.9293\n",
      "\n",
      "Epoch 01691: loss did not improve from 0.16601\n",
      "Epoch 1692/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1739 - accuracy: 0.9281\n",
      "\n",
      "Epoch 01692: loss did not improve from 0.16601\n",
      "Epoch 1693/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1876 - accuracy: 0.9227\n",
      "\n",
      "Epoch 01693: loss did not improve from 0.16601\n",
      "Epoch 1694/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1761 - accuracy: 0.9231\n",
      "\n",
      "Epoch 01694: loss did not improve from 0.16601\n",
      "Epoch 1695/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1718 - accuracy: 0.9296\n",
      "\n",
      "Epoch 01695: loss did not improve from 0.16601\n",
      "Epoch 1696/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1692 - accuracy: 0.9271\n",
      "\n",
      "Epoch 01696: loss did not improve from 0.16601\n",
      "Epoch 1697/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1720 - accuracy: 0.9219\n",
      "\n",
      "Epoch 01697: loss did not improve from 0.16601\n",
      "Epoch 1698/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1719 - accuracy: 0.9318\n",
      "\n",
      "Epoch 01698: loss did not improve from 0.16601\n",
      "Epoch 1699/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1625 - accuracy: 0.9284\n",
      "\n",
      "Epoch 01699: loss did not improve from 0.16601\n",
      "Epoch 1700/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1641 - accuracy: 0.9243\n",
      "\n",
      "Epoch 01700: loss did not improve from 0.16601\n",
      "Epoch 1701/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1579 - accuracy: 0.9317\n",
      "\n",
      "Epoch 01701: loss did not improve from 0.16601\n",
      "Epoch 1702/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1610 - accuracy: 0.9281\n",
      "\n",
      "Epoch 01702: loss did not improve from 0.16601\n",
      "Epoch 1703/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1620 - accuracy: 0.9272\n",
      "\n",
      "Epoch 01703: loss did not improve from 0.16601\n",
      "Epoch 1704/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1603 - accuracy: 0.9307\n",
      "\n",
      "Epoch 01704: loss did not improve from 0.16601\n",
      "Epoch 1705/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1678 - accuracy: 0.9303\n",
      "\n",
      "Epoch 01705: loss did not improve from 0.16601\n",
      "Epoch 1706/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1792 - accuracy: 0.9217\n",
      "\n",
      "Epoch 01706: loss did not improve from 0.16601\n",
      "Epoch 1707/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1661 - accuracy: 0.9272\n",
      "\n",
      "Epoch 01707: loss did not improve from 0.16601\n",
      "Epoch 1708/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1720 - accuracy: 0.9241\n",
      "\n",
      "Epoch 01708: loss did not improve from 0.16601\n",
      "Epoch 1709/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1874 - accuracy: 0.9174\n",
      "\n",
      "Epoch 01709: loss did not improve from 0.16601\n",
      "Epoch 1710/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1854 - accuracy: 0.9232\n",
      "\n",
      "Epoch 01710: loss did not improve from 0.16601\n",
      "Epoch 1711/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1769 - accuracy: 0.9234\n",
      "\n",
      "Epoch 01711: loss did not improve from 0.16601\n",
      "Epoch 1712/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1799 - accuracy: 0.9247\n",
      "\n",
      "Epoch 01712: loss did not improve from 0.16601\n",
      "Epoch 1713/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1997 - accuracy: 0.9230\n",
      "\n",
      "Epoch 01713: loss did not improve from 0.16601\n",
      "Epoch 1714/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2573 - accuracy: 0.9030\n",
      "\n",
      "Epoch 01714: loss did not improve from 0.16601\n",
      "Epoch 1715/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2650 - accuracy: 0.9070\n",
      "\n",
      "Epoch 01715: loss did not improve from 0.16601\n",
      "Epoch 1716/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2420 - accuracy: 0.9097\n",
      "\n",
      "Epoch 01716: loss did not improve from 0.16601\n",
      "Epoch 1717/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.2165 - accuracy: 0.9125\n",
      "\n",
      "Epoch 01717: loss did not improve from 0.16601\n",
      "Epoch 1718/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2365 - accuracy: 0.9114\n",
      "\n",
      "Epoch 01718: loss did not improve from 0.16601\n",
      "Epoch 1719/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2393 - accuracy: 0.9059\n",
      "\n",
      "Epoch 01719: loss did not improve from 0.16601\n",
      "Epoch 1720/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2322 - accuracy: 0.9116\n",
      "\n",
      "Epoch 01720: loss did not improve from 0.16601\n",
      "Epoch 1721/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2421 - accuracy: 0.9063\n",
      "\n",
      "Epoch 01721: loss did not improve from 0.16601\n",
      "Epoch 1722/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2546 - accuracy: 0.9071\n",
      "\n",
      "Epoch 01722: loss did not improve from 0.16601\n",
      "Epoch 1723/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.2354 - accuracy: 0.9102\n",
      "\n",
      "Epoch 01723: loss did not improve from 0.16601\n",
      "Epoch 1724/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1988 - accuracy: 0.9232\n",
      "\n",
      "Epoch 01724: loss did not improve from 0.16601\n",
      "Epoch 1725/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1874 - accuracy: 0.9222\n",
      "\n",
      "Epoch 01725: loss did not improve from 0.16601\n",
      "Epoch 1726/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1818 - accuracy: 0.9216\n",
      "\n",
      "Epoch 01726: loss did not improve from 0.16601\n",
      "Epoch 1727/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1698 - accuracy: 0.9289\n",
      "\n",
      "Epoch 01727: loss did not improve from 0.16601\n",
      "Epoch 1728/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1648 - accuracy: 0.9345\n",
      "\n",
      "Epoch 01728: loss did not improve from 0.16601\n",
      "Epoch 1729/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1658 - accuracy: 0.9326\n",
      "\n",
      "Epoch 01729: loss did not improve from 0.16601\n",
      "Epoch 1730/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1735 - accuracy: 0.9273\n",
      "\n",
      "Epoch 01730: loss did not improve from 0.16601\n",
      "Epoch 1731/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1661 - accuracy: 0.9257\n",
      "\n",
      "Epoch 01731: loss did not improve from 0.16601\n",
      "Epoch 1732/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1588 - accuracy: 0.9277\n",
      "\n",
      "Epoch 01732: loss improved from 0.16601 to 0.16266, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1733/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1606 - accuracy: 0.9252\n",
      "\n",
      "Epoch 01733: loss did not improve from 0.16266\n",
      "Epoch 1734/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1571 - accuracy: 0.9320\n",
      "\n",
      "Epoch 01734: loss did not improve from 0.16266\n",
      "Epoch 1735/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1585 - accuracy: 0.9340\n",
      "\n",
      "Epoch 01735: loss did not improve from 0.16266\n",
      "Epoch 1736/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1554 - accuracy: 0.9335\n",
      "\n",
      "Epoch 01736: loss did not improve from 0.16266\n",
      "Epoch 1737/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1607 - accuracy: 0.9291\n",
      "\n",
      "Epoch 01737: loss did not improve from 0.16266\n",
      "Epoch 1738/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1542 - accuracy: 0.9277\n",
      "\n",
      "Epoch 01738: loss did not improve from 0.16266\n",
      "Epoch 1739/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1679 - accuracy: 0.9226\n",
      "\n",
      "Epoch 01739: loss did not improve from 0.16266\n",
      "Epoch 1740/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1663 - accuracy: 0.9271\n",
      "\n",
      "Epoch 01740: loss did not improve from 0.16266\n",
      "Epoch 1741/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1603 - accuracy: 0.9341\n",
      "\n",
      "Epoch 01741: loss did not improve from 0.16266\n",
      "Epoch 1742/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1534 - accuracy: 0.9325\n",
      "\n",
      "Epoch 01742: loss did not improve from 0.16266\n",
      "Epoch 1743/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1576 - accuracy: 0.9300\n",
      "\n",
      "Epoch 01743: loss did not improve from 0.16266\n",
      "Epoch 1744/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1662 - accuracy: 0.9259\n",
      "\n",
      "Epoch 01744: loss did not improve from 0.16266\n",
      "Epoch 1745/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1485 - accuracy: 0.9384\n",
      "\n",
      "Epoch 01745: loss did not improve from 0.16266\n",
      "Epoch 1746/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1586 - accuracy: 0.9298\n",
      "\n",
      "Epoch 01746: loss did not improve from 0.16266\n",
      "Epoch 1747/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1567 - accuracy: 0.9315\n",
      "\n",
      "Epoch 01747: loss did not improve from 0.16266\n",
      "Epoch 1748/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1635 - accuracy: 0.9309\n",
      "\n",
      "Epoch 01748: loss did not improve from 0.16266\n",
      "Epoch 1749/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1625 - accuracy: 0.9249\n",
      "\n",
      "Epoch 01749: loss did not improve from 0.16266\n",
      "Epoch 1750/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1522 - accuracy: 0.9310\n",
      "\n",
      "Epoch 01750: loss improved from 0.16266 to 0.16061, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1751/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1605 - accuracy: 0.9265\n",
      "\n",
      "Epoch 01751: loss did not improve from 0.16061\n",
      "Epoch 1752/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1580 - accuracy: 0.9242\n",
      "\n",
      "Epoch 01752: loss did not improve from 0.16061\n",
      "Epoch 1753/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1606 - accuracy: 0.9295\n",
      "\n",
      "Epoch 01753: loss did not improve from 0.16061\n",
      "Epoch 1754/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1616 - accuracy: 0.9266\n",
      "\n",
      "Epoch 01754: loss did not improve from 0.16061\n",
      "Epoch 1755/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1701 - accuracy: 0.9256\n",
      "\n",
      "Epoch 01755: loss did not improve from 0.16061\n",
      "Epoch 1756/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1668 - accuracy: 0.9284\n",
      "\n",
      "Epoch 01756: loss did not improve from 0.16061\n",
      "Epoch 1757/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1747 - accuracy: 0.9244\n",
      "\n",
      "Epoch 01757: loss did not improve from 0.16061\n",
      "Epoch 1758/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1613 - accuracy: 0.9299\n",
      "\n",
      "Epoch 01758: loss did not improve from 0.16061\n",
      "Epoch 1759/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1663 - accuracy: 0.9280\n",
      "\n",
      "Epoch 01759: loss did not improve from 0.16061\n",
      "Epoch 1760/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1647 - accuracy: 0.9283\n",
      "\n",
      "Epoch 01760: loss did not improve from 0.16061\n",
      "Epoch 1761/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1533 - accuracy: 0.9309\n",
      "\n",
      "Epoch 01761: loss did not improve from 0.16061\n",
      "Epoch 1762/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1594 - accuracy: 0.9342\n",
      "\n",
      "Epoch 01762: loss did not improve from 0.16061\n",
      "Epoch 1763/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1850 - accuracy: 0.9189\n",
      "\n",
      "Epoch 01763: loss did not improve from 0.16061\n",
      "Epoch 1764/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.2175 - accuracy: 0.9163\n",
      "\n",
      "Epoch 01764: loss did not improve from 0.16061\n",
      "Epoch 1765/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.3096 - accuracy: 0.8849\n",
      "\n",
      "Epoch 01765: loss did not improve from 0.16061\n",
      "Epoch 1766/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.3133 - accuracy: 0.8919\n",
      "\n",
      "Epoch 01766: loss did not improve from 0.16061\n",
      "Epoch 1767/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3010 - accuracy: 0.8912\n",
      "\n",
      "Epoch 01767: loss did not improve from 0.16061\n",
      "Epoch 1768/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3159 - accuracy: 0.8815\n",
      "\n",
      "Epoch 01768: loss did not improve from 0.16061\n",
      "Epoch 1769/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2763 - accuracy: 0.8986\n",
      "\n",
      "Epoch 01769: loss did not improve from 0.16061\n",
      "Epoch 1770/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.2193 - accuracy: 0.9080\n",
      "\n",
      "Epoch 01770: loss did not improve from 0.16061\n",
      "Epoch 1771/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.2117 - accuracy: 0.9155\n",
      "\n",
      "Epoch 01771: loss did not improve from 0.16061\n",
      "Epoch 1772/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1818 - accuracy: 0.9259\n",
      "\n",
      "Epoch 01772: loss did not improve from 0.16061\n",
      "Epoch 1773/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1755 - accuracy: 0.9252\n",
      "\n",
      "Epoch 01773: loss did not improve from 0.16061\n",
      "Epoch 1774/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1771 - accuracy: 0.9238\n",
      "\n",
      "Epoch 01774: loss did not improve from 0.16061\n",
      "Epoch 1775/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1673 - accuracy: 0.9265\n",
      "\n",
      "Epoch 01775: loss did not improve from 0.16061\n",
      "Epoch 1776/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1711 - accuracy: 0.9298\n",
      "\n",
      "Epoch 01776: loss did not improve from 0.16061\n",
      "Epoch 1777/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1843 - accuracy: 0.9209\n",
      "\n",
      "Epoch 01777: loss did not improve from 0.16061\n",
      "Epoch 1778/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1683 - accuracy: 0.9282\n",
      "\n",
      "Epoch 01778: loss did not improve from 0.16061\n",
      "Epoch 1779/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1639 - accuracy: 0.9268\n",
      "\n",
      "Epoch 01779: loss did not improve from 0.16061\n",
      "Epoch 1780/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1658 - accuracy: 0.9233\n",
      "\n",
      "Epoch 01780: loss did not improve from 0.16061\n",
      "Epoch 1781/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1666 - accuracy: 0.9257\n",
      "\n",
      "Epoch 01781: loss did not improve from 0.16061\n",
      "Epoch 1782/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1462 - accuracy: 0.9349\n",
      "\n",
      "Epoch 01782: loss did not improve from 0.16061\n",
      "Epoch 1783/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.1545 - accuracy: 0.9300\n",
      "\n",
      "Epoch 01783: loss did not improve from 0.16061\n",
      "Epoch 1784/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1551 - accuracy: 0.9300\n",
      "\n",
      "Epoch 01784: loss did not improve from 0.16061\n",
      "Epoch 1785/2000\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.1553 - accuracy: 0.9295\n",
      "\n",
      "Epoch 01785: loss improved from 0.16061 to 0.16018, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1786/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1609 - accuracy: 0.9252\n",
      "\n",
      "Epoch 01786: loss did not improve from 0.16018\n",
      "Epoch 1787/2000\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 0.1592 - accuracy: 0.9249\n",
      "\n",
      "Epoch 01787: loss did not improve from 0.16018\n",
      "Epoch 1788/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1565 - accuracy: 0.9268\n",
      "\n",
      "Epoch 01788: loss improved from 0.16018 to 0.15773, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1789/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1553 - accuracy: 0.9262\n",
      "\n",
      "Epoch 01789: loss did not improve from 0.15773\n",
      "Epoch 1790/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1443 - accuracy: 0.9332\n",
      "\n",
      "Epoch 01790: loss did not improve from 0.15773\n",
      "Epoch 1791/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1624 - accuracy: 0.9249\n",
      "\n",
      "Epoch 01791: loss did not improve from 0.15773\n",
      "Epoch 1792/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1516 - accuracy: 0.9300\n",
      "\n",
      "Epoch 01792: loss improved from 0.15773 to 0.15743, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1793/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1550 - accuracy: 0.9283\n",
      "\n",
      "Epoch 01793: loss did not improve from 0.15743\n",
      "Epoch 1794/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1574 - accuracy: 0.9278\n",
      "\n",
      "Epoch 01794: loss did not improve from 0.15743\n",
      "Epoch 1795/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1583 - accuracy: 0.9301\n",
      "\n",
      "Epoch 01795: loss did not improve from 0.15743\n",
      "Epoch 1796/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1520 - accuracy: 0.9276\n",
      "\n",
      "Epoch 01796: loss did not improve from 0.15743\n",
      "Epoch 1797/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1583 - accuracy: 0.9240\n",
      "\n",
      "Epoch 01797: loss did not improve from 0.15743\n",
      "Epoch 1798/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1489 - accuracy: 0.9335\n",
      "\n",
      "Epoch 01798: loss did not improve from 0.15743\n",
      "Epoch 1799/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1509 - accuracy: 0.9334\n",
      "\n",
      "Epoch 01799: loss did not improve from 0.15743\n",
      "Epoch 1800/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1577 - accuracy: 0.9288\n",
      "\n",
      "Epoch 01800: loss did not improve from 0.15743\n",
      "Epoch 1801/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1684 - accuracy: 0.9237\n",
      "\n",
      "Epoch 01801: loss did not improve from 0.15743\n",
      "Epoch 1802/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1659 - accuracy: 0.9217\n",
      "\n",
      "Epoch 01802: loss did not improve from 0.15743\n",
      "Epoch 1803/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1588 - accuracy: 0.9290\n",
      "\n",
      "Epoch 01803: loss did not improve from 0.15743\n",
      "Epoch 1804/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1516 - accuracy: 0.9305\n",
      "\n",
      "Epoch 01804: loss did not improve from 0.15743\n",
      "Epoch 1805/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1497 - accuracy: 0.9305\n",
      "\n",
      "Epoch 01805: loss did not improve from 0.15743\n",
      "Epoch 1806/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1586 - accuracy: 0.9263\n",
      "\n",
      "Epoch 01806: loss did not improve from 0.15743\n",
      "Epoch 1807/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1541 - accuracy: 0.9295\n",
      "\n",
      "Epoch 01807: loss did not improve from 0.15743\n",
      "Epoch 1808/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.1761 - accuracy: 0.9229\n",
      "\n",
      "Epoch 01808: loss did not improve from 0.15743\n",
      "Epoch 1809/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.1814 - accuracy: 0.9210\n",
      "\n",
      "Epoch 01809: loss did not improve from 0.15743\n",
      "Epoch 1810/2000\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.1692 - accuracy: 0.9280\n",
      "\n",
      "Epoch 01810: loss did not improve from 0.15743\n",
      "Epoch 1811/2000\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.1945 - accuracy: 0.9192\n",
      "\n",
      "Epoch 01811: loss did not improve from 0.15743\n",
      "Epoch 1812/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1739 - accuracy: 0.9261\n",
      "\n",
      "Epoch 01812: loss did not improve from 0.15743\n",
      "Epoch 1813/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1839 - accuracy: 0.9201\n",
      "\n",
      "Epoch 01813: loss did not improve from 0.15743\n",
      "Epoch 1814/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1707 - accuracy: 0.9333\n",
      "\n",
      "Epoch 01814: loss did not improve from 0.15743\n",
      "Epoch 1815/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1799 - accuracy: 0.9253\n",
      "\n",
      "Epoch 01815: loss did not improve from 0.15743\n",
      "Epoch 1816/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1761 - accuracy: 0.9237\n",
      "\n",
      "Epoch 01816: loss did not improve from 0.15743\n",
      "Epoch 1817/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1683 - accuracy: 0.9266\n",
      "\n",
      "Epoch 01817: loss did not improve from 0.15743\n",
      "Epoch 1818/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1651 - accuracy: 0.9264\n",
      "\n",
      "Epoch 01818: loss did not improve from 0.15743\n",
      "Epoch 1819/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1530 - accuracy: 0.9325\n",
      "\n",
      "Epoch 01819: loss did not improve from 0.15743\n",
      "Epoch 1820/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1560 - accuracy: 0.9266\n",
      "\n",
      "Epoch 01820: loss did not improve from 0.15743\n",
      "Epoch 1821/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1593 - accuracy: 0.9241\n",
      "\n",
      "Epoch 01821: loss did not improve from 0.15743\n",
      "Epoch 1822/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1439 - accuracy: 0.9367\n",
      "\n",
      "Epoch 01822: loss did not improve from 0.15743\n",
      "Epoch 1823/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1601 - accuracy: 0.9268\n",
      "\n",
      "Epoch 01823: loss did not improve from 0.15743\n",
      "Epoch 1824/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1471 - accuracy: 0.9298\n",
      "\n",
      "Epoch 01824: loss did not improve from 0.15743\n",
      "Epoch 1825/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1582 - accuracy: 0.9223\n",
      "\n",
      "Epoch 01825: loss did not improve from 0.15743\n",
      "Epoch 1826/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1528 - accuracy: 0.9296\n",
      "\n",
      "Epoch 01826: loss did not improve from 0.15743\n",
      "Epoch 1827/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1570 - accuracy: 0.9233\n",
      "\n",
      "Epoch 01827: loss improved from 0.15743 to 0.15627, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1828/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1445 - accuracy: 0.9344\n",
      "\n",
      "Epoch 01828: loss did not improve from 0.15627\n",
      "Epoch 1829/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1778 - accuracy: 0.9253\n",
      "\n",
      "Epoch 01829: loss did not improve from 0.15627\n",
      "Epoch 1830/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1581 - accuracy: 0.9318\n",
      "\n",
      "Epoch 01830: loss did not improve from 0.15627\n",
      "Epoch 1831/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1640 - accuracy: 0.9287\n",
      "\n",
      "Epoch 01831: loss did not improve from 0.15627\n",
      "Epoch 1832/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1943 - accuracy: 0.9170\n",
      "\n",
      "Epoch 01832: loss did not improve from 0.15627\n",
      "Epoch 1833/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2015 - accuracy: 0.9201\n",
      "\n",
      "Epoch 01833: loss did not improve from 0.15627\n",
      "Epoch 1834/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2278 - accuracy: 0.9115\n",
      "\n",
      "Epoch 01834: loss did not improve from 0.15627\n",
      "Epoch 1835/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2440 - accuracy: 0.9046\n",
      "\n",
      "Epoch 01835: loss did not improve from 0.15627\n",
      "Epoch 1836/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2218 - accuracy: 0.9142\n",
      "\n",
      "Epoch 01836: loss did not improve from 0.15627\n",
      "Epoch 1837/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2516 - accuracy: 0.9073\n",
      "\n",
      "Epoch 01837: loss did not improve from 0.15627\n",
      "Epoch 1838/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2829 - accuracy: 0.8956\n",
      "\n",
      "Epoch 01838: loss did not improve from 0.15627\n",
      "Epoch 1839/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2479 - accuracy: 0.9056\n",
      "\n",
      "Epoch 01839: loss did not improve from 0.15627\n",
      "Epoch 1840/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1891 - accuracy: 0.9261\n",
      "\n",
      "Epoch 01840: loss did not improve from 0.15627\n",
      "Epoch 1841/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1977 - accuracy: 0.9217\n",
      "\n",
      "Epoch 01841: loss did not improve from 0.15627\n",
      "Epoch 1842/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1784 - accuracy: 0.9212\n",
      "\n",
      "Epoch 01842: loss did not improve from 0.15627\n",
      "Epoch 1843/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1662 - accuracy: 0.9271\n",
      "\n",
      "Epoch 01843: loss did not improve from 0.15627\n",
      "Epoch 1844/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1670 - accuracy: 0.9322\n",
      "\n",
      "Epoch 01844: loss did not improve from 0.15627\n",
      "Epoch 1845/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1636 - accuracy: 0.9264\n",
      "\n",
      "Epoch 01845: loss did not improve from 0.15627\n",
      "Epoch 1846/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1590 - accuracy: 0.9299\n",
      "\n",
      "Epoch 01846: loss did not improve from 0.15627\n",
      "Epoch 1847/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1576 - accuracy: 0.9253\n",
      "\n",
      "Epoch 01847: loss did not improve from 0.15627\n",
      "Epoch 1848/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1444 - accuracy: 0.9325\n",
      "\n",
      "Epoch 01848: loss did not improve from 0.15627\n",
      "Epoch 1849/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1544 - accuracy: 0.9254\n",
      "\n",
      "Epoch 01849: loss improved from 0.15627 to 0.15468, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1850/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1486 - accuracy: 0.9280\n",
      "\n",
      "Epoch 01850: loss improved from 0.15468 to 0.15452, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1851/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1584 - accuracy: 0.9226\n",
      "\n",
      "Epoch 01851: loss did not improve from 0.15452\n",
      "Epoch 1852/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1484 - accuracy: 0.9341\n",
      "\n",
      "Epoch 01852: loss did not improve from 0.15452\n",
      "Epoch 1853/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1479 - accuracy: 0.9332\n",
      "\n",
      "Epoch 01853: loss did not improve from 0.15452\n",
      "Epoch 1854/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1568 - accuracy: 0.9270\n",
      "\n",
      "Epoch 01854: loss did not improve from 0.15452\n",
      "Epoch 1855/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1468 - accuracy: 0.9345\n",
      "\n",
      "Epoch 01855: loss did not improve from 0.15452\n",
      "Epoch 1856/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1554 - accuracy: 0.9289\n",
      "\n",
      "Epoch 01856: loss did not improve from 0.15452\n",
      "Epoch 1857/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1614 - accuracy: 0.9214\n",
      "\n",
      "Epoch 01857: loss did not improve from 0.15452\n",
      "Epoch 1858/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1493 - accuracy: 0.9298\n",
      "\n",
      "Epoch 01858: loss did not improve from 0.15452\n",
      "Epoch 1859/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1474 - accuracy: 0.9320\n",
      "\n",
      "Epoch 01859: loss did not improve from 0.15452\n",
      "Epoch 1860/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1559 - accuracy: 0.9268\n",
      "\n",
      "Epoch 01860: loss did not improve from 0.15452\n",
      "Epoch 1861/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1677 - accuracy: 0.9196\n",
      "\n",
      "Epoch 01861: loss did not improve from 0.15452\n",
      "Epoch 1862/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1539 - accuracy: 0.9307\n",
      "\n",
      "Epoch 01862: loss did not improve from 0.15452\n",
      "Epoch 1863/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1647 - accuracy: 0.9268\n",
      "\n",
      "Epoch 01863: loss did not improve from 0.15452\n",
      "Epoch 1864/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1517 - accuracy: 0.9306\n",
      "\n",
      "Epoch 01864: loss did not improve from 0.15452\n",
      "Epoch 1865/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1553 - accuracy: 0.9301\n",
      "\n",
      "Epoch 01865: loss did not improve from 0.15452\n",
      "Epoch 1866/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1464 - accuracy: 0.9325\n",
      "\n",
      "Epoch 01866: loss did not improve from 0.15452\n",
      "Epoch 1867/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1524 - accuracy: 0.9353\n",
      "\n",
      "Epoch 01867: loss did not improve from 0.15452\n",
      "Epoch 1868/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1576 - accuracy: 0.9250\n",
      "\n",
      "Epoch 01868: loss did not improve from 0.15452\n",
      "Epoch 1869/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1558 - accuracy: 0.9283\n",
      "\n",
      "Epoch 01869: loss did not improve from 0.15452\n",
      "Epoch 1870/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1560 - accuracy: 0.9286\n",
      "\n",
      "Epoch 01870: loss did not improve from 0.15452\n",
      "Epoch 1871/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1580 - accuracy: 0.9278\n",
      "\n",
      "Epoch 01871: loss did not improve from 0.15452\n",
      "Epoch 1872/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1516 - accuracy: 0.9240\n",
      "\n",
      "Epoch 01872: loss did not improve from 0.15452\n",
      "Epoch 1873/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1540 - accuracy: 0.9258\n",
      "\n",
      "Epoch 01873: loss did not improve from 0.15452\n",
      "Epoch 1874/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1469 - accuracy: 0.9289\n",
      "\n",
      "Epoch 01874: loss improved from 0.15452 to 0.15420, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1875/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1494 - accuracy: 0.9288\n",
      "\n",
      "Epoch 01875: loss did not improve from 0.15420\n",
      "Epoch 1876/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1575 - accuracy: 0.9247\n",
      "\n",
      "Epoch 01876: loss did not improve from 0.15420\n",
      "Epoch 1877/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1532 - accuracy: 0.9321\n",
      "\n",
      "Epoch 01877: loss did not improve from 0.15420\n",
      "Epoch 1878/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1505 - accuracy: 0.9294\n",
      "\n",
      "Epoch 01878: loss did not improve from 0.15420\n",
      "Epoch 1879/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1561 - accuracy: 0.9285\n",
      "\n",
      "Epoch 01879: loss did not improve from 0.15420\n",
      "Epoch 1880/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1580 - accuracy: 0.9247\n",
      "\n",
      "Epoch 01880: loss did not improve from 0.15420\n",
      "Epoch 1881/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1631 - accuracy: 0.9266\n",
      "\n",
      "Epoch 01881: loss did not improve from 0.15420\n",
      "Epoch 1882/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1509 - accuracy: 0.9264\n",
      "\n",
      "Epoch 01882: loss did not improve from 0.15420\n",
      "Epoch 1883/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1587 - accuracy: 0.9263\n",
      "\n",
      "Epoch 01883: loss did not improve from 0.15420\n",
      "Epoch 1884/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1556 - accuracy: 0.9321\n",
      "\n",
      "Epoch 01884: loss did not improve from 0.15420\n",
      "Epoch 1885/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1650 - accuracy: 0.9260\n",
      "\n",
      "Epoch 01885: loss did not improve from 0.15420\n",
      "Epoch 1886/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1494 - accuracy: 0.9309\n",
      "\n",
      "Epoch 01886: loss did not improve from 0.15420\n",
      "Epoch 1887/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1641 - accuracy: 0.9272\n",
      "\n",
      "Epoch 01887: loss did not improve from 0.15420\n",
      "Epoch 1888/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1553 - accuracy: 0.9339\n",
      "\n",
      "Epoch 01888: loss did not improve from 0.15420\n",
      "Epoch 1889/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1723 - accuracy: 0.9270\n",
      "\n",
      "Epoch 01889: loss did not improve from 0.15420\n",
      "Epoch 1890/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2143 - accuracy: 0.9132\n",
      "\n",
      "Epoch 01890: loss did not improve from 0.15420\n",
      "Epoch 1891/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2677 - accuracy: 0.9046\n",
      "\n",
      "Epoch 01891: loss did not improve from 0.15420\n",
      "Epoch 1892/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2782 - accuracy: 0.8984\n",
      "\n",
      "Epoch 01892: loss did not improve from 0.15420\n",
      "Epoch 1893/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3781 - accuracy: 0.8708\n",
      "\n",
      "Epoch 01893: loss did not improve from 0.15420\n",
      "Epoch 1894/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3870 - accuracy: 0.8737\n",
      "\n",
      "Epoch 01894: loss did not improve from 0.15420\n",
      "Epoch 1895/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.3033 - accuracy: 0.8893\n",
      "\n",
      "Epoch 01895: loss did not improve from 0.15420\n",
      "Epoch 1896/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2224 - accuracy: 0.9124\n",
      "\n",
      "Epoch 01896: loss did not improve from 0.15420\n",
      "Epoch 1897/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2036 - accuracy: 0.9195\n",
      "\n",
      "Epoch 01897: loss did not improve from 0.15420\n",
      "Epoch 1898/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1743 - accuracy: 0.9243\n",
      "\n",
      "Epoch 01898: loss did not improve from 0.15420\n",
      "Epoch 1899/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1653 - accuracy: 0.9291\n",
      "\n",
      "Epoch 01899: loss did not improve from 0.15420\n",
      "Epoch 1900/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1625 - accuracy: 0.9303\n",
      "\n",
      "Epoch 01900: loss did not improve from 0.15420\n",
      "Epoch 1901/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1545 - accuracy: 0.9317\n",
      "\n",
      "Epoch 01901: loss did not improve from 0.15420\n",
      "Epoch 1902/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1483 - accuracy: 0.9327\n",
      "\n",
      "Epoch 01902: loss did not improve from 0.15420\n",
      "Epoch 1903/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1446 - accuracy: 0.9339\n",
      "\n",
      "Epoch 01903: loss improved from 0.15420 to 0.15393, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1904/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1446 - accuracy: 0.9315\n",
      "\n",
      "Epoch 01904: loss improved from 0.15393 to 0.15269, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1905/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1540 - accuracy: 0.9247\n",
      "\n",
      "Epoch 01905: loss did not improve from 0.15269\n",
      "Epoch 1906/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1417 - accuracy: 0.9349\n",
      "\n",
      "Epoch 01906: loss improved from 0.15269 to 0.15185, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1907/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1426 - accuracy: 0.9344\n",
      "\n",
      "Epoch 01907: loss improved from 0.15185 to 0.15019, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1908/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1527 - accuracy: 0.9311\n",
      "\n",
      "Epoch 01908: loss did not improve from 0.15019\n",
      "Epoch 1909/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1514 - accuracy: 0.9288\n",
      "\n",
      "Epoch 01909: loss did not improve from 0.15019\n",
      "Epoch 1910/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1515 - accuracy: 0.9297\n",
      "\n",
      "Epoch 01910: loss did not improve from 0.15019\n",
      "Epoch 1911/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1439 - accuracy: 0.9342\n",
      "\n",
      "Epoch 01911: loss did not improve from 0.15019\n",
      "Epoch 1912/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1416 - accuracy: 0.9308\n",
      "\n",
      "Epoch 01912: loss did not improve from 0.15019\n",
      "Epoch 1913/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1468 - accuracy: 0.9295\n",
      "\n",
      "Epoch 01913: loss did not improve from 0.15019\n",
      "Epoch 1914/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1572 - accuracy: 0.9291\n",
      "\n",
      "Epoch 01914: loss did not improve from 0.15019\n",
      "Epoch 1915/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1464 - accuracy: 0.9300\n",
      "\n",
      "Epoch 01915: loss did not improve from 0.15019\n",
      "Epoch 1916/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1494 - accuracy: 0.9295\n",
      "\n",
      "Epoch 01916: loss did not improve from 0.15019\n",
      "Epoch 1917/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1501 - accuracy: 0.9273\n",
      "\n",
      "Epoch 01917: loss did not improve from 0.15019\n",
      "Epoch 1918/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1461 - accuracy: 0.9345\n",
      "\n",
      "Epoch 01918: loss did not improve from 0.15019\n",
      "Epoch 1919/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1470 - accuracy: 0.9303\n",
      "\n",
      "Epoch 01919: loss did not improve from 0.15019\n",
      "Epoch 1920/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1453 - accuracy: 0.9311\n",
      "\n",
      "Epoch 01920: loss did not improve from 0.15019\n",
      "Epoch 1921/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1400 - accuracy: 0.9312\n",
      "\n",
      "Epoch 01921: loss did not improve from 0.15019\n",
      "Epoch 1922/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1470 - accuracy: 0.9333\n",
      "\n",
      "Epoch 01922: loss did not improve from 0.15019\n",
      "Epoch 1923/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1522 - accuracy: 0.9247\n",
      "\n",
      "Epoch 01923: loss did not improve from 0.15019\n",
      "Epoch 1924/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1532 - accuracy: 0.9216\n",
      "\n",
      "Epoch 01924: loss improved from 0.15019 to 0.14936, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1925/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1434 - accuracy: 0.9310\n",
      "\n",
      "Epoch 01925: loss did not improve from 0.14936\n",
      "Epoch 1926/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1541 - accuracy: 0.9282\n",
      "\n",
      "Epoch 01926: loss did not improve from 0.14936\n",
      "Epoch 1927/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1521 - accuracy: 0.9295\n",
      "\n",
      "Epoch 01927: loss did not improve from 0.14936\n",
      "Epoch 1928/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1437 - accuracy: 0.9333\n",
      "\n",
      "Epoch 01928: loss did not improve from 0.14936\n",
      "Epoch 1929/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1408 - accuracy: 0.9335\n",
      "\n",
      "Epoch 01929: loss did not improve from 0.14936\n",
      "Epoch 1930/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1526 - accuracy: 0.9300\n",
      "\n",
      "Epoch 01930: loss did not improve from 0.14936\n",
      "Epoch 1931/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1438 - accuracy: 0.9336\n",
      "\n",
      "Epoch 01931: loss did not improve from 0.14936\n",
      "Epoch 1932/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1464 - accuracy: 0.9313\n",
      "\n",
      "Epoch 01932: loss did not improve from 0.14936\n",
      "Epoch 1933/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1386 - accuracy: 0.9379\n",
      "\n",
      "Epoch 01933: loss did not improve from 0.14936\n",
      "Epoch 1934/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1621 - accuracy: 0.9229\n",
      "\n",
      "Epoch 01934: loss did not improve from 0.14936\n",
      "Epoch 1935/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1456 - accuracy: 0.9261\n",
      "\n",
      "Epoch 01935: loss did not improve from 0.14936\n",
      "Epoch 1936/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1553 - accuracy: 0.9287\n",
      "\n",
      "Epoch 01936: loss did not improve from 0.14936\n",
      "Epoch 1937/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1491 - accuracy: 0.9347\n",
      "\n",
      "Epoch 01937: loss did not improve from 0.14936\n",
      "Epoch 1938/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1578 - accuracy: 0.9313\n",
      "\n",
      "Epoch 01938: loss did not improve from 0.14936\n",
      "Epoch 1939/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2241 - accuracy: 0.9084\n",
      "\n",
      "Epoch 01939: loss did not improve from 0.14936\n",
      "Epoch 1940/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2200 - accuracy: 0.9206\n",
      "\n",
      "Epoch 01940: loss did not improve from 0.14936\n",
      "Epoch 1941/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2554 - accuracy: 0.9056\n",
      "\n",
      "Epoch 01941: loss did not improve from 0.14936\n",
      "Epoch 1942/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.3181 - accuracy: 0.8871\n",
      "\n",
      "Epoch 01942: loss did not improve from 0.14936\n",
      "Epoch 1943/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2795 - accuracy: 0.9009\n",
      "\n",
      "Epoch 01943: loss did not improve from 0.14936\n",
      "Epoch 1944/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2433 - accuracy: 0.9041\n",
      "\n",
      "Epoch 01944: loss did not improve from 0.14936\n",
      "Epoch 1945/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.2132 - accuracy: 0.9144\n",
      "\n",
      "Epoch 01945: loss did not improve from 0.14936\n",
      "Epoch 1946/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1913 - accuracy: 0.9203\n",
      "\n",
      "Epoch 01946: loss did not improve from 0.14936\n",
      "Epoch 1947/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1695 - accuracy: 0.9258\n",
      "\n",
      "Epoch 01947: loss did not improve from 0.14936\n",
      "Epoch 1948/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1520 - accuracy: 0.9328\n",
      "\n",
      "Epoch 01948: loss did not improve from 0.14936\n",
      "Epoch 1949/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1474 - accuracy: 0.9333\n",
      "\n",
      "Epoch 01949: loss did not improve from 0.14936\n",
      "Epoch 1950/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1419 - accuracy: 0.9353\n",
      "\n",
      "Epoch 01950: loss did not improve from 0.14936\n",
      "Epoch 1951/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1533 - accuracy: 0.9294\n",
      "\n",
      "Epoch 01951: loss did not improve from 0.14936\n",
      "Epoch 1952/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1393 - accuracy: 0.9360\n",
      "\n",
      "Epoch 01952: loss did not improve from 0.14936\n",
      "Epoch 1953/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1487 - accuracy: 0.9296\n",
      "\n",
      "Epoch 01953: loss did not improve from 0.14936\n",
      "Epoch 1954/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1421 - accuracy: 0.9342\n",
      "\n",
      "Epoch 01954: loss did not improve from 0.14936\n",
      "Epoch 1955/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1481 - accuracy: 0.9312\n",
      "\n",
      "Epoch 01955: loss did not improve from 0.14936\n",
      "Epoch 1956/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1483 - accuracy: 0.9305\n",
      "\n",
      "Epoch 01956: loss improved from 0.14936 to 0.14810, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1957/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1528 - accuracy: 0.9225\n",
      "\n",
      "Epoch 01957: loss did not improve from 0.14810\n",
      "Epoch 1958/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1445 - accuracy: 0.9264\n",
      "\n",
      "Epoch 01958: loss did not improve from 0.14810\n",
      "Epoch 1959/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1406 - accuracy: 0.9291\n",
      "\n",
      "Epoch 01959: loss did not improve from 0.14810\n",
      "Epoch 1960/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1366 - accuracy: 0.9351\n",
      "\n",
      "Epoch 01960: loss did not improve from 0.14810\n",
      "Epoch 1961/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1438 - accuracy: 0.9276\n",
      "\n",
      "Epoch 01961: loss did not improve from 0.14810\n",
      "Epoch 1962/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1558 - accuracy: 0.9251\n",
      "\n",
      "Epoch 01962: loss did not improve from 0.14810\n",
      "Epoch 1963/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1522 - accuracy: 0.9271\n",
      "\n",
      "Epoch 01963: loss did not improve from 0.14810\n",
      "Epoch 1964/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1392 - accuracy: 0.9316\n",
      "\n",
      "Epoch 01964: loss did not improve from 0.14810\n",
      "Epoch 1965/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1528 - accuracy: 0.9261\n",
      "\n",
      "Epoch 01965: loss improved from 0.14810 to 0.14808, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1966/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1397 - accuracy: 0.9315\n",
      "\n",
      "Epoch 01966: loss did not improve from 0.14808\n",
      "Epoch 1967/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1548 - accuracy: 0.9252\n",
      "\n",
      "Epoch 01967: loss did not improve from 0.14808\n",
      "Epoch 1968/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1434 - accuracy: 0.9307\n",
      "\n",
      "Epoch 01968: loss did not improve from 0.14808\n",
      "Epoch 1969/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1371 - accuracy: 0.9356\n",
      "\n",
      "Epoch 01969: loss did not improve from 0.14808\n",
      "Epoch 1970/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1435 - accuracy: 0.9315\n",
      "\n",
      "Epoch 01970: loss did not improve from 0.14808\n",
      "Epoch 1971/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1429 - accuracy: 0.9303\n",
      "\n",
      "Epoch 01971: loss improved from 0.14808 to 0.14704, saving model to ./Results\\10LetterWordM.hdf5\n",
      "Epoch 1972/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1567 - accuracy: 0.9248\n",
      "\n",
      "Epoch 01972: loss did not improve from 0.14704\n",
      "Epoch 1973/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1479 - accuracy: 0.9335\n",
      "\n",
      "Epoch 01973: loss did not improve from 0.14704\n",
      "Epoch 1974/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1489 - accuracy: 0.9298\n",
      "\n",
      "Epoch 01974: loss did not improve from 0.14704\n",
      "Epoch 1975/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1497 - accuracy: 0.9291\n",
      "\n",
      "Epoch 01975: loss did not improve from 0.14704\n",
      "Epoch 1976/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1475 - accuracy: 0.9304\n",
      "\n",
      "Epoch 01976: loss did not improve from 0.14704\n",
      "Epoch 1977/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1494 - accuracy: 0.9322\n",
      "\n",
      "Epoch 01977: loss did not improve from 0.14704\n",
      "Epoch 1978/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1471 - accuracy: 0.9344\n",
      "\n",
      "Epoch 01978: loss did not improve from 0.14704\n",
      "Epoch 1979/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1325 - accuracy: 0.9383\n",
      "\n",
      "Epoch 01979: loss did not improve from 0.14704\n",
      "Epoch 1980/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1381 - accuracy: 0.9360\n",
      "\n",
      "Epoch 01980: loss did not improve from 0.14704\n",
      "Epoch 1981/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1408 - accuracy: 0.9355\n",
      "\n",
      "Epoch 01981: loss did not improve from 0.14704\n",
      "Epoch 1982/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1506 - accuracy: 0.9269\n",
      "\n",
      "Epoch 01982: loss did not improve from 0.14704\n",
      "Epoch 1983/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1505 - accuracy: 0.9292\n",
      "\n",
      "Epoch 01983: loss did not improve from 0.14704\n",
      "Epoch 1984/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1514 - accuracy: 0.9259\n",
      "\n",
      "Epoch 01984: loss did not improve from 0.14704\n",
      "Epoch 1985/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1522 - accuracy: 0.9310\n",
      "\n",
      "Epoch 01985: loss did not improve from 0.14704\n",
      "Epoch 1986/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1508 - accuracy: 0.9299\n",
      "\n",
      "Epoch 01986: loss did not improve from 0.14704\n",
      "Epoch 1987/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1557 - accuracy: 0.9303\n",
      "\n",
      "Epoch 01987: loss did not improve from 0.14704\n",
      "Epoch 1988/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1476 - accuracy: 0.9312\n",
      "\n",
      "Epoch 01988: loss did not improve from 0.14704\n",
      "Epoch 1989/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1611 - accuracy: 0.9266\n",
      "\n",
      "Epoch 01989: loss did not improve from 0.14704\n",
      "Epoch 1990/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1671 - accuracy: 0.9276\n",
      "\n",
      "Epoch 01990: loss did not improve from 0.14704\n",
      "Epoch 1991/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1750 - accuracy: 0.9223\n",
      "\n",
      "Epoch 01991: loss did not improve from 0.14704\n",
      "Epoch 1992/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1856 - accuracy: 0.9161\n",
      "\n",
      "Epoch 01992: loss did not improve from 0.14704\n",
      "Epoch 1993/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1917 - accuracy: 0.9252\n",
      "\n",
      "Epoch 01993: loss did not improve from 0.14704\n",
      "Epoch 1994/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1864 - accuracy: 0.9173\n",
      "\n",
      "Epoch 01994: loss did not improve from 0.14704\n",
      "Epoch 1995/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1697 - accuracy: 0.9264\n",
      "\n",
      "Epoch 01995: loss did not improve from 0.14704\n",
      "Epoch 1996/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1901 - accuracy: 0.9230\n",
      "\n",
      "Epoch 01996: loss did not improve from 0.14704\n",
      "Epoch 1997/2000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 0.1876 - accuracy: 0.9211\n",
      "\n",
      "Epoch 01997: loss did not improve from 0.14704\n",
      "Epoch 1998/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1929 - accuracy: 0.9171\n",
      "\n",
      "Epoch 01998: loss did not improve from 0.14704\n",
      "Epoch 1999/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.1663 - accuracy: 0.9316\n",
      "\n",
      "Epoch 01999: loss did not improve from 0.14704\n",
      "Epoch 2000/2000\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 0.2013 - accuracy: 0.9232\n",
      "\n",
      "Epoch 02000: loss did not improve from 0.14704\n"
     ]
    }
   ],
   "source": [
    "### Model checkpoint\n",
    "ModelSaveSameName = save_path+'10LetterWordM.hdf5'\n",
    "ModelSave = ModelCheckpoint(filepath=ModelSaveSameName, monitor='loss', verbose=1, save_best_only=True)\n",
    "\n",
    "### Model Early stop\n",
    "EarlyStop = EarlyStopping(monitor='loss', patience=100)\n",
    "\n",
    "WordsM = WordsModel()\n",
    "WordsM.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam', metrics =['accuracy'])\n",
    "WordsMHist = WordsM.fit(InpData, TargetData,  epochs=2000, batch_size=100,  verbose=1, callbacks=[ModelSave, EarlyStop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed39eb0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "589abdf6",
   "metadata": {},
   "source": [
    "### Model weight load and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50900133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loading weights\n",
      "9/9 [==============================] - 1s 4ms/step - loss: 3.2579 - accuracy: 0.0319\n",
      "[3.2579758167266846, 0.029877889901399612]\n",
      "\n",
      "After loading weights\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.1392 - accuracy: 0.9361\n",
      "[0.1391867846250534, 0.9360873103141785]\n"
     ]
    }
   ],
   "source": [
    "WordsM = WordsModel()\n",
    "WordsM.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam', metrics =['accuracy'])\n",
    "print('Before loading weights')\n",
    "print(WordsM.evaluate(InpData, TargetData, batch_size=300 ))\n",
    "print()\n",
    "\n",
    "WordsM.load_weights(ModelSaveSameName)\n",
    "print('After loading weights')\n",
    "print(WordsM.evaluate(InpData, TargetData, batch_size=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d23e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91ff0d80",
   "metadata": {},
   "source": [
    "### Plot loss graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dd9bee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(WordsMHist.history['loss'], label='Tr', marker='.')\n",
    "plt.legend() \n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010cb5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c957692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abbreviate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abdication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aberration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abjuration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abnegation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0  abbreviate\n",
       "1  abdication\n",
       "2  aberration\n",
       "3  abjuration\n",
       "4  abnegation"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa14661",
   "metadata": {},
   "source": [
    "### Prediction example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cb9c90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " abbrevi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AlphaInp = input().upper()\n",
    "AlphaInpEncod = [AlphOneHot.values[char_to_int[i]][None] for i in AlphaInp ]\n",
    "AlphaInpEncod = np.concatenate(AlphaInpEncod, axis=0)\n",
    "AlphaPred = WordsM.predict(AlphaInpEncod[None])\n",
    "\n",
    "plt.figure(figsize=(18,4))    \n",
    "sns.heatmap(AlphaPred[0], xticklabels=[i for i in char_to_int], annot=np.round(AlphaPred[0], 2),linewidths=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1a07d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
