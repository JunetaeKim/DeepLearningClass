{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Input\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import umap #pip install umap-learn\n",
    "import wget #pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the pre-trained model from a GitHub repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [..........................................................................] 6622952 / 6622952"
     ]
    }
   ],
   "source": [
    "save_path = './Results/'\n",
    "ModelSaveSameName = save_path+'ReutersM.hdf5'\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "    \n",
    "url = 'https://github.com/JunetaeKim/DeepLearningClass/raw/main/Week14/Results/ReutersM.hdf5'\n",
    "\n",
    "if not os.path.exists(ModelSaveSameName):\n",
    "    wget.download(url, ModelSaveSameName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "max_length = 100\n",
    "embedding_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and Index load\n",
    "#### Data are already be encoded as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "c:\\users\\user\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "### Load the data. The data returned will already be encoded as integers.\n",
    "#### Words are ranked by how often they occur (in the training set) and only the num_words most frequent words are kept.\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=vocab_size, test_split=0.2)\n",
    "\n",
    "### Get the index mapping integers to words from the Reuters dataset.\n",
    "word_index = reuters.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence length setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(train_data, maxlen=max_length)\n",
    "X_test = sequence.pad_sequences(test_data, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = to_categorical(train_labels)\n",
    "Y_test = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "InpL = Input(shape=(max_length, ))\n",
    "EmbedL = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length, name='Embedd')(InpL)\n",
    "LSTML = LSTM(100)(EmbedL)\n",
    "DenseL = Dense(50, activation='relu')(LSTML)\n",
    "Outcome = Dense(len(np.unique(train_labels)), activation='softmax')(LSTML)\n",
    "\n",
    "model = Model(InpL, Outcome)\n",
    "model.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(ModelSaveSameName)\n",
    "model.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 12ms/step - loss: 3.3537 - accuracy: 0.5616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.3465194702148438, 0.5596616268157959]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Embeddings for Words Occurring More Than a Specified Cutoff Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4567, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CutoffCount = 20\n",
    "UsedWord_idx, UsedWordCount = np.unique(np.concatenate(train_data), return_counts=True)\n",
    "UsedWord_idx = UsedWord_idx[UsedWordCount> CutoffCount]\n",
    "train_labels\n",
    "\n",
    "EmbeddedRes = model.get_layer('Embedd').weights[0].numpy()\n",
    "UsedWords = EmbeddedRes[UsedWord_idx]\n",
    "UsedWords.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing UMAP to reduce the dimensionality of word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality using UMAP\n",
    "Reducer = umap.UMAP(n_components=2, n_neighbors=10, min_dist=0.2 )\n",
    "UsedWords_2d = Reducer.fit_transform(UsedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot embeddings in 2D\n",
    "plt.scatter(UsedWords_2d[:, 0], UsedWords_2d[:, 1], s=5)\n",
    "plt.title('UMAP projection of Reuters data in 2D')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
