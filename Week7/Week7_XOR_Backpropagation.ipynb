{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "random.seed(777)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and Target value\n",
    "data = [\n",
    "    [[0, 0], [0]],\n",
    "    [[0, 1], [1]],\n",
    "    [[1, 0], [1]],\n",
    "    [[1, 1], [0]]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environmental settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations=5000  # Iterations\n",
    "lr=0.1  # learning rate \n",
    "mo=0.4  # momentum coefficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions and weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid\n",
    "def sigmoid(x, derivative=False):\n",
    "    if (derivative == True): # Each return value when differentiating or not\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Hyperbolic tangent\n",
    "def tanh(x, derivative=False): \n",
    "    if (derivative == True): # Each return value when differentiating or not\n",
    "        return 1 - x ** 2\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Function to initialize weight matrices\n",
    "def makeMatrix(i, j, fill=0.0):\n",
    "    mat = []\n",
    "    for i in range(i):\n",
    "        mat.append([fill] * j)\n",
    "    return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run neural network\n",
    "class NeuralNetwork:\n",
    "\n",
    "    # Initialize models\n",
    "    def __init__(self, num_x, num_yh, num_yo, bias=1):\n",
    "\n",
    "        # Input(num_x), Hidden(num_yh), output(num_yo), bias\n",
    "        self.num_x = num_x + bias  \n",
    "        self.num_yh = num_yh\n",
    "        self.num_yo = num_yo\n",
    "\n",
    "        # Initialize activation function\n",
    "        self.activation_input = [1.0] * self.num_x\n",
    "        self.activation_hidden = [1.0] * self.num_yh\n",
    "        self.activation_out = [1.0] * self.num_yo\n",
    "\n",
    "        # weight matrices input\n",
    "        self.weight_in = makeMatrix(self.num_x, self.num_yh)\n",
    "        for i in range(self.num_x):\n",
    "            for j in range(self.num_yh):\n",
    "                self.weight_in[i][j] = random.random()\n",
    "\n",
    "        # weight matrices output\n",
    "        self.weight_out = makeMatrix(self.num_yh, self.num_yo)\n",
    "        for j in range(self.num_yh):\n",
    "            for k in range(self.num_yo):\n",
    "                self.weight_out[j][k] = random.random()\n",
    "\n",
    "        # Weighted Initial Values for Momentum SGD\n",
    "        self.gradient_in = makeMatrix(self.num_x, self.num_yh)\n",
    "        self.gradient_out = makeMatrix(self.num_yh, self.num_yo)\n",
    "\n",
    "        \n",
    "        \n",
    "    # forward propagation \n",
    "    def update(self, inputs):\n",
    "\n",
    "        # activation function for input layer\n",
    "        for i in range(self.num_x - 1):\n",
    "            self.activation_input[i] = inputs[i]\n",
    "\n",
    "        # activation function for hidden layer\n",
    "        for j in range(self.num_yh):\n",
    "            sum = 0.0\n",
    "            for i in range(self.num_x):\n",
    "                sum = sum + self.activation_input[i] * self.weight_in[i][j]\n",
    "           # selecting activation functions between sigmoid and tanh, differentiation\n",
    "            self.activation_hidden[j] = sigmoid(sum, False)\n",
    "\n",
    "        # activation function for output layer\n",
    "        for k in range(self.num_yo):\n",
    "            sum = 0.0\n",
    "            for j in range(self.num_yh):\n",
    "                sum = sum + self.activation_hidden[j] * self.weight_out[j][k]\n",
    "            \n",
    "            # selecting activation functions between sigmoid and tanh\n",
    "            self.activation_out[k] = sigmoid(sum, False)\n",
    "\n",
    "        return self.activation_out[:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # running backpropagation\n",
    "    def backPropagate(self, targets):\n",
    "\n",
    "        # output delta \n",
    "        output_deltas = [0.0] * self.num_yo\n",
    "        for k in range(self.num_yo):\n",
    "            error = targets[k] - self.activation_out[k]\n",
    "            # selecting activation functions between sigmoid and tanh, differentiation\n",
    "            output_deltas[k] = sigmoid(self.activation_out[k], True) * error\n",
    "\n",
    "        # hidden node delta\n",
    "        hidden_deltas = [0.0] * self.num_yh\n",
    "        for j in range(self.num_yh):\n",
    "            error = 0.0\n",
    "            for k in range(self.num_yo):\n",
    "                error = error + output_deltas[k] * self.weight_out[j][k]\n",
    "                # selecting activation functions between sigmoid and tanh, differentiation\n",
    "            hidden_deltas[j] = sigmoid(self.activation_hidden[j], True) * error\n",
    "\n",
    "        # Update output weights\n",
    "        for j in range(self.num_yh):\n",
    "            for k in range(self.num_yo):\n",
    "                gradient = output_deltas[k] * self.activation_hidden[j]\n",
    "                v = mo * self.gradient_out[j][k] - lr * gradient\n",
    "                self.weight_out[j][k] += v\n",
    "                self.gradient_out[j][k] = gradient\n",
    "\n",
    "        # Input weight update\n",
    "        for i in range(self.num_x):\n",
    "            for j in range(self.num_yh):\n",
    "                gradient = hidden_deltas[j] * self.activation_input[i]\n",
    "                v = mo*self.gradient_in[i][j] - lr * gradient\n",
    "                self.weight_in[i][j] += v\n",
    "                self.gradient_in[i][j] = gradient\n",
    "\n",
    "        # Calculation of error (least squares method)\n",
    "        error = 0.0\n",
    "        for k in range(len(targets)):\n",
    "            error = error + 0.5 * (targets[k] - self.activation_out[k]) ** 2\n",
    "        return error\n",
    "\n",
    "    \n",
    "    \n",
    "    # Doing training\n",
    "    def train(self, patterns):\n",
    "        for i in range(iterations):\n",
    "            error = 0.0\n",
    "            for p in patterns:\n",
    "                inputs = p[0]\n",
    "                targets = p[1]\n",
    "                self.update(inputs)\n",
    "                error = error + self.backPropagate(targets)\n",
    "                \n",
    "            if i % 500 == 0:\n",
    "                print('error: %-.5f' % error)\n",
    "                \n",
    "    \n",
    "    # Printing results\n",
    "    def result(self, patterns):\n",
    "        for p in patterns:\n",
    "            print('Input: %s, Predict: %s' % (p[0], self.update(p[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3651267673345187\n",
      "0.14691586391989978\n",
      "0.10540076479377652\n",
      "0.021859748142623345\n",
      "0.009285721753224596\n",
      "0.005609337192869759\n",
      "0.003956390321656173\n",
      "0.0030351826165898707\n",
      "0.002453120635427478\n",
      "0.0020539851553678964\n",
      "Input: [0, 0], Predict: [0.017732820819339936]\n",
      "Input: [0, 1], Predict: [0.9510075898232156]\n",
      "Input: [1, 0], Predict: [0.9499517731360895]\n",
      "Input: [1, 1], Predict: [0.05958406581748622]\n"
     ]
    }
   ],
   "source": [
    "# model instantiation\n",
    "n = NeuralNetwork(2, 3, 1)\n",
    "\n",
    "\n",
    "# Doing training\n",
    "n.train(data)\n",
    "\n",
    "# Result\n",
    "n.result(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking down the code to understand it\n",
    "####  Initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_x = 2\n",
    "num_yh = 2\n",
    "num_yo = 1\n",
    "bias = 1\n",
    "\n",
    "# Size: input(num_x), Hidden(num_yh), output(num_yo), bias\n",
    "num_x = num_x + bias  \n",
    "num_yh = num_yh\n",
    "num_yo = num_yo\n",
    "\n",
    "# Initialize activation function\n",
    "activation_input = [1.0] * num_x\n",
    "activation_hidden = [1.0] * num_yh\n",
    "activation_out = [1.0] * num_yo\n",
    "\n",
    "# weight matrices input\n",
    "weight_in = makeMatrix(num_x, num_yh)\n",
    "for i in range(num_x):\n",
    "    for j in range(num_yh):\n",
    "        weight_in[i][j] = random.random()\n",
    "\n",
    "        \n",
    "# weight matrices output\n",
    "weight_out = makeMatrix(num_yh, num_yo)\n",
    "for j in range(num_yh):\n",
    "    for k in range(num_yo):\n",
    "        weight_out[j][k] = random.random()\n",
    "        \n",
    "        \n",
    "# Weighted Initial Values for Momentum SGD\n",
    "gradient_in = makeMatrix(num_x, num_yh)\n",
    "gradient_out = makeMatrix(num_yh, num_yo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    inputs = d[0]\n",
    "\n",
    "    # activation function for input layer\n",
    "    for i in range(num_x - 1):\n",
    "        activation_input[i] = inputs[i]\n",
    "        \n",
    "    # activation function for hidden layer\n",
    "    for j in range(num_yh):\n",
    "        sum = 0.0\n",
    "        for i in range(num_x):\n",
    "            sum = sum + activation_input[i] * weight_in[i][j]\n",
    "        # selecting activation functions between sigmoid and tanh, differentiation\n",
    "        activation_hidden[j] = sigmoid(sum, False)\n",
    "        \n",
    "        \n",
    "    # activation function for output layer\n",
    "    for k in range(num_yo):\n",
    "        sum = 0.0\n",
    "        for j in range(num_yh):\n",
    "            sum = sum + activation_hidden[j] * weight_out[j][k]\n",
    "\n",
    "        # selecting activation functions between sigmoid and tanh\n",
    "        activation_out[k] = sigmoid(sum, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40900896473032294\n",
      "0.004565528433366953\n",
      "0.004565528433366953\n",
      "0.40900896473032294\n"
     ]
    }
   ],
   "source": [
    "for d in data:\n",
    "    targets = d[1]\n",
    "    \n",
    "    # delta \n",
    "    output_deltas = [0.0] * num_yo    \n",
    "    for k in range(num_yo):\n",
    "        error = targets[k] - activation_out[k]\n",
    "        # selecting activation functions between sigmoid and tanh, differentiation\n",
    "        output_deltas[k] = sigmoid(activation_out[k], True) * error\n",
    "        \n",
    "        \n",
    "    # Error function of hidden node\n",
    "    hidden_deltas = [0.0] * num_yh\n",
    "    for j in range(num_yh):\n",
    "        error = 0.0\n",
    "        for k in range(num_yo):\n",
    "            error = error + output_deltas[k] * weight_out[j][k]\n",
    "            # selecting activation functions between sigmoid and tanh, differentiation\n",
    "        hidden_deltas[j] = sigmoid(activation_hidden[j], True) * error\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Update output weights\n",
    "    for j in range(num_yh):\n",
    "        for k in range(num_yo):\n",
    "            gradient = output_deltas[k] * activation_hidden[j]\n",
    "            v = mo * gradient_out[j][k] - lr * gradient\n",
    "            weight_out[j][k] += v\n",
    "            gradient_out[j][k] = gradient\n",
    "            \n",
    "            \n",
    "    # Input weight update\n",
    "    for i in range(num_x):\n",
    "        for j in range(num_yh):\n",
    "            gradient = hidden_deltas[j] * activation_input[i]\n",
    "            v = mo*gradient_in[i][j] - lr * gradient\n",
    "            weight_in[i][j] += v\n",
    "            gradient_in[i][j] = gradient\n",
    "            \n",
    "            \n",
    "    # Calculation of error (least squares method)\n",
    "    error = 0.0\n",
    "    for k in range(len(targets)):\n",
    "        error = error + 0.5 * (targets[k] - activation_out[k]) ** 2\n",
    "        \n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run neural network\n",
    "class NeuralNetwork:\n",
    "\n",
    "    # Initialize models\n",
    "    def __init__(self, num_x, num_yh, num_yo, bias=1):\n",
    "        pass\n",
    "       \n",
    "    # forward propagation \n",
    "    def update(self, inputs):\n",
    "        pass\n",
    "    \n",
    "    # running backpropagation\n",
    "    def backPropagate(self, targets):\n",
    "        pass\n",
    "    \n",
    "    # Doing training\n",
    "    def train(self, patterns):\n",
    "        for i in range(iterations):\n",
    "            error = 0.0\n",
    "            for p in patterns:\n",
    "                inputs = p[0]\n",
    "                targets = p[1]\n",
    "                self.update(inputs)\n",
    "                error = error + self.backPropagate(targets)\n",
    "            if i % 500 == 0:\n",
    "                print('error: %-.5f' % error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
